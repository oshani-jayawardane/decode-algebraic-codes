{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DFTSNN - DFT Encoded with updated weights - parellel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (r + 1) | (q - 1)\n",
    "# (r + 1) | n\n",
    "# q is a prime number\n",
    "# n = 2^t\n",
    "# r < n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't know how to reset  #, please run `%reset?` for details\n",
      "Don't know how to reset  clears, please run `%reset?` for details\n",
      "Don't know how to reset  all, please run `%reset?` for details\n",
      "Don't know how to reset  variables,, please run `%reset?` for details\n",
      "Don't know how to reset  history,, please run `%reset?` for details\n",
      "Don't know how to reset  and, please run `%reset?` for details\n",
      "Don't know how to reset  caches, please run `%reset?` for details\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset -f  # clears all variables, history, and caches\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original n: 16\n",
      "Padded n: 16\n",
      "Generated dataset shape: (1000, 16)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n = 16\n",
    "q = 5\n",
    "num_samples = 1000\n",
    "\n",
    "def next_power_of_two(x):\n",
    "    return 1 if x == 0 else 2**(x - 1).bit_length()\n",
    "\n",
    "n_padded = next_power_of_two(n)\n",
    "\n",
    "dataset = np.random.randint(0, q, size=(num_samples, n))\n",
    "\n",
    "if n_padded > n:\n",
    "    pad_width = n_padded - n\n",
    "    dataset = np.pad(dataset, ((0, 0), (0, pad_width)), mode='constant', constant_values=0)\n",
    "\n",
    "print(\"Original n:\", n)\n",
    "print(\"Padded n:\", n_padded)\n",
    "print(\"Generated dataset shape:\", dataset.shape)\n",
    "\n",
    "n = n_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 4 2 ... 1 3 1]\n",
      " [3 4 0 ... 3 2 3]\n",
      " [3 0 2 ... 0 1 4]\n",
      " ...\n",
      " [4 4 4 ... 0 2 2]\n",
      " [4 2 4 ... 4 0 0]\n",
      " [2 2 2 ... 0 4 2]]\n",
      "(1000, 16)\n",
      "(16,)\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(dataset.shape)\n",
    "print(dataset[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dft(u, n):\n",
    "    n1 = n // 2\n",
    "    if n == 2:  \n",
    "        return np.array([u[0] + u[1], u[0] - u[1]])\n",
    "    \n",
    "    zeta = np.exp(-2j * np.pi / n)\n",
    "    Dn = np.diag([zeta**k for k in range(n1)])\n",
    "    Hn = np.block([[np.eye(n1), np.eye(n1)],\n",
    "                   [Dn, -Dn]])\n",
    "    \n",
    "    p = np.dot(Hn, u)\n",
    "    \n",
    "    s1 = dft(p[:n1], n1)\n",
    "    s2 = dft(p[n1:], n1)\n",
    "    \n",
    "    interleaved = np.empty((len(s1) + len(s2)), dtype=complex)\n",
    "    interleaved[0::2] = s1\n",
    "    interleaved[1::2] = s2\n",
    "    \n",
    "    y = interleaved\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42.         +0.j          0.84148733 -0.81622894j\n",
      "  -0.87867966 -6.36396103j ... -1.2304425  -1.25570089j\n",
      "  -0.87867966 +6.36396103j  0.84148733 +0.81622894j]\n",
      " [34.         +0.j          5.88076075 +0.31815162j\n",
      "  -2.41421356 +3.j         ...  6.11085797 -0.77903661j\n",
      "  -2.41421356 -3.j          5.88076075 -0.31815162j]\n",
      " [29.         +0.j          2.38895517 -4.90489287j\n",
      "   0.17157288 -1.17157288j ...  2.07192983 -7.86463619j\n",
      "   0.17157288 +1.17157288j  2.38895517 +4.90489287j]\n",
      " ...\n",
      " [37.         +0.j          6.60006572-11.28512148j\n",
      "   2.70710678 -0.70710678j ... -0.68037683 +4.15172419j\n",
      "   2.70710678 +0.70710678j  6.60006572+11.28512148j]\n",
      " [40.         +0.j         -4.28191411 +3.31849689j\n",
      "   0.70710678 -3.70710678j ...  1.90134022+10.29671013j\n",
      "   0.70710678 +3.70710678j -4.28191411 -3.31849689j]\n",
      " [35.         +0.j         -2.11652017 +0.96427923j\n",
      "   2.41421356 +0.34314575j ...  0.28130457 -2.5159377j\n",
      "   2.41421356 -0.34314575j -2.11652017 -0.96427923j]]\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset = np.array([dft(message, n) for message in dataset])\n",
    "encoded_dataset[np.abs(encoded_dataset) < 1e-10] = 0\n",
    "encoded_dataset = np.round(encoded_dataset, decimals=10)\n",
    "print(encoded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 16)\n"
     ]
    }
   ],
   "source": [
    "print(encoded_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 2 4 4 1 2 2 2 4 3 2 4 1 3 1]\n",
      "[42.        +0.j          0.84148733-0.81622894j -0.87867966-6.36396103j\n",
      " -1.2304425 +1.25570089j  3.        -1.j          3.2304425 -1.57272623j\n",
      " -5.12132034-6.36396103j  1.15851267-3.64465606j  4.        +0.j\n",
      "  1.15851267+3.64465606j -5.12132034+6.36396103j  3.2304425 +1.57272623j\n",
      "  3.        +1.j         -1.2304425 -1.25570089j -0.87867966+6.36396103j\n",
      "  0.84148733+0.81622894j]\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])\n",
    "print(encoded_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to zero mean and unit variance - output results of normalizing made unseen data converge to q-1\n",
    "# encoded_dataset = (encoded_dataset - np.mean(encoded_dataset, axis=0)) / np.std(encoded_dataset, axis=0)\n",
    "# print(encoded_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train_real : (1000, 16)\n",
      "[42.          0.84148735 -0.87867963 -1.2304425   3.          3.2304425\n",
      " -5.1213202   1.1585127   4.          1.1585127  -5.1213202   3.2304425\n",
      "  3.         -1.2304425  -0.87867963  0.84148735]\n",
      "\n",
      "X_train_imag : (1000, 16)\n",
      "[ 0.        -0.8162289 -6.363961   1.255701  -1.        -1.5727262\n",
      " -6.363961  -3.644656   0.         3.644656   6.363961   1.5727262\n",
      "  1.        -1.255701   6.363961   0.8162289]\n"
     ]
    }
   ],
   "source": [
    "X_real = np.real(encoded_dataset).astype(np.float32)\n",
    "X_imag = np.imag(encoded_dataset).astype(np.float32)\n",
    "\n",
    "print(\"\\nX_train_real :\", X_real.shape)\n",
    "print(X_real[0])\n",
    "print(\"\\nX_train_imag :\", X_imag.shape)\n",
    "print(X_imag[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: \n",
      "[3. 4. 2. 4. 4. 1. 2. 2. 2. 4. 3. 2. 4. 1. 3. 1.]\n",
      "\n",
      "y_normalized: \n",
      "[0.75 1.   0.5  1.   1.   0.25 0.5  0.5  0.5  1.   0.75 0.5  1.   0.25\n",
      " 0.75 0.25]\n"
     ]
    }
   ],
   "source": [
    "# normalize target data (integers 0-q to [0, 1])\n",
    "y_normalized = dataset.astype(np.float32) / (q - 1)  # Scale to [0, 1]\n",
    "print(\"y: \")\n",
    "print(dataset.astype(np.float32)[0])\n",
    "print(\"\\ny_normalized: \")\n",
    "print(y_normalized[0])\n",
    "\n",
    "labels = y_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shapes: X_real: (800, 16) X_imag: (800, 16) y: (800, 16)\n",
      "Testing data shapes: X_real: (200, 16) X_imag: (200, 16) y: (200, 16)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_real_train, X_real_test, y_train, y_test = train_test_split(\n",
    "    X_real, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_imag_train, X_imag_test, _, _ = train_test_split(\n",
    "    X_imag, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training data shapes: X_real:\", X_real_train.shape, \"X_imag:\", X_imag_train.shape, \"y:\", y_train.shape)\n",
    "print(\"Testing data shapes: X_real:\", X_real_test.shape, \"X_imag:\", X_imag_test.shape, \"y:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "print(X_real_train.shape[1])\n",
    "print(X_real_test.shape[1])\n",
    "print(X_imag_train.shape[1])\n",
    "print(X_imag_test.shape[1])\n",
    "print(y_train.shape[1])\n",
    "print(y_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_real_train.shape[1]\n",
    "output_dim = y_train.shape[1]\n",
    "print(input_dim)\n",
    "print(output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDFT - Structure Imposed Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, use_bias=True, **kwargs):\n",
    "        super(FirstLayer, self).__init__(**kwargs)\n",
    "        self.units = units  # Features/neurons (16)\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n = self.units # --> 32\n",
    "        n1 = n // 2 # --> 16\n",
    "        num_blocks = n1 // 2 # --> 8\n",
    "        \n",
    "        self.b_1 = self.add_weight(name=\"kernel_b1\",\n",
    "                                   shape=(num_blocks, 2, 2),\n",
    "                                   initializer=tf.keras.initializers.HeNormal(), #GlorotNormal\n",
    "                                #    regularizer=tf.keras.regularizers.l2(1e-5),\n",
    "                                   trainable=True)\n",
    "        self.b_2 = self.add_weight(name=\"kernel_b2\",\n",
    "                                   shape=(num_blocks, 2, 2),\n",
    "                                   initializer=tf.keras.initializers.HeNormal(),\n",
    "                                #    regularizer=tf.keras.regularizers.l2(1e-5),\n",
    "                                   trainable=True)\n",
    "        self.d_1 = self.add_weight(name=\"kernel_d1\", \n",
    "                                  shape=(n1-2,), \n",
    "                                  initializer=tf.keras.initializers.HeNormal(),\n",
    "                                #   regularizer=tf.keras.regularizers.l2(1e-5),\n",
    "                                  trainable=True)\n",
    "        self.d_2 = self.add_weight(name=\"kernel_d2\", \n",
    "                                  shape=(n1-2,), \n",
    "                                  initializer=tf.keras.initializers.HeNormal(),\n",
    "                                #   regularizer=tf.keras.regularizers.l2(1e-5),\n",
    "                                  trainable=True)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name=\"bias\", \n",
    "                                        shape=(self.units,), \n",
    "                                        initializer=tf.keras.initializers.Zeros(),\n",
    "                                        trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # print(\"Enters first layer\")\n",
    "\n",
    "        def recursiveIDFT(inputVector, B, d, level):\n",
    "            n = inputVector.shape[1]\n",
    "            n1 = n // 2\n",
    "\n",
    "            if n == 2:\n",
    "                out = tf.matmul(inputVector, B[level])\n",
    "                return out\n",
    "\n",
    "            else:\n",
    "                q = tf.concat([inputVector[:, ::2], inputVector[:, 1::2]], axis=1)\n",
    "\n",
    "                B1 = recursiveIDFT(q[:, :n1], B, d[n1:], level + 1)\n",
    "                B2 = recursiveIDFT(q[:, n1:], B, d[n1:], level + 1)\n",
    "\n",
    "                d_n = tf.reshape(d[:n1], (1, -1))\n",
    "                z1 = tf.concat([(B1 + tf.multiply(B2, d_n)), (B1 - tf.multiply(B2, d_n))], axis=1)\n",
    "                \n",
    "                return z1 / tf.sqrt(tf.constant(2.0, dtype=tf.float32))\n",
    "\n",
    "\n",
    "        n = self.units \n",
    "        n1 = n // 2\n",
    "\n",
    "        q = tf.concat([inputs[:, ::2], inputs[:, 1::2]], axis=1)\n",
    "\n",
    "        B1 = recursiveIDFT(q[:, :n1], self.b_1, self.d_1, level=0)\n",
    "        B2 = recursiveIDFT(q[:, n1:], self.b_2, self.d_2, level=0)\n",
    "\n",
    "        out = tf.concat([B1, B2], axis=1)\n",
    "\n",
    "        if self.use_bias:\n",
    "            out += self.bias\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, use_bias=True, **kwargs):\n",
    "        super(SecondLayer, self).__init__(**kwargs)\n",
    "        self.units = units # features/neurons (16)\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n = self.units\n",
    "        n1 = n//2\n",
    "        \n",
    "        self.d_1 = self.add_weight(name =\"kernel_d1\",\n",
    "                                      shape=(n1,),\n",
    "                                      initializer=tf.keras.initializers.HeNormal(),\n",
    "                                      trainable=True)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name =\"bias\",\n",
    "                                       shape=(n,),\n",
    "                                       initializer=tf.keras.initializers.Zeros(),\n",
    "                                       trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        out1 = inputs[:, :int(inputs.shape[1] / 2)]\n",
    "        out2 = inputs[:, int(inputs.shape[1] / 2):]\n",
    "        \n",
    "        z1 = tf.concat([(out1 + tf.multiply(out2, self.d_1)), (out1 - tf.multiply(out2, self.d_1))], axis=1)\n",
    "        \n",
    "        out = z1 / tf.sqrt(tf.constant(2.0, dtype=tf.float32))\n",
    "        \n",
    "        if self.use_bias:\n",
    "            out += self.bias\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, use_bias=True, **kwargs):\n",
    "        super(CustomLayer, self).__init__(**kwargs)\n",
    "        self.units = units # features/neurons\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n = self.units\n",
    "        \n",
    "        self.m = self.add_weight(name =\"kernel_m\",\n",
    "                                      shape=(n,),\n",
    "                                      initializer=tf.keras.initializers.HeNormal(),\n",
    "                                      trainable=True)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name =\"bias\",\n",
    "                                       shape=(self.units,),\n",
    "                                       initializer=tf.keras.initializers.Zeros(),\n",
    "                                       trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # print(\"Enters custom layer\")\n",
    "        out = tf.multiply(inputs, self.m)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            out += self.bias\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, use_bias=True, **kwargs):\n",
    "        super(LinearLayer, self).__init__(**kwargs)\n",
    "        self.units = units # features/neurons\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n = self.units\n",
    "        \n",
    "        self.m = self.add_weight(name =\"kernel_m_1\",\n",
    "                                      shape=(n,),\n",
    "                                      initializer=tf.keras.initializers.Ones(),\n",
    "                                      trainable=True)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name =\"bias\",\n",
    "                                       shape=(self.units,),\n",
    "                                       initializer=tf.keras.initializers.Zeros(),\n",
    "                                       trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        print(\"linear layer input:\", inputs.shape)\n",
    "        \n",
    "        inputs1 = inputs[:, :int(inputs.shape[1] / 2)]\n",
    "        inputs2 = inputs[:, int(inputs.shape[1] / 2):]\n",
    "        \n",
    "        out = tf.math.sqrt(inputs1**2 + inputs2**2)\n",
    "        \n",
    "        out = tf.multiply(out, self.m)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            out += self.bias\n",
    "            \n",
    "        print(\"linear layer output:\", out.shape)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "linear layer input: (None, 32)\n",
      "linear layer output: (None, 16)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " real_input (InputLayer)     [(None, 16)]                 0         []                            \n",
      "                                                                                                  \n",
      " imag_input (InputLayer)     [(None, 16)]                 0         []                            \n",
      "                                                                                                  \n",
      " real_layer1 (FirstLayer)    (None, 16)                   60        ['real_input[0][0]']          \n",
      "                                                                                                  \n",
      " imag_layer1 (FirstLayer)    (None, 16)                   60        ['imag_input[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)     (None, 16)                   0         ['real_layer1[0][0]']         \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 16)                   0         ['imag_layer1[0][0]']         \n",
      "                                                                                                  \n",
      " real_support_layer_1 (Cust  (None, 16)                   32        ['leaky_re_lu[0][0]']         \n",
      " omLayer)                                                                                         \n",
      "                                                                                                  \n",
      " imag_support_layer_1 (Cust  (None, 16)                   32        ['leaky_re_lu_3[0][0]']       \n",
      " omLayer)                                                                                         \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 16)                   0         ['real_support_layer_1[0][0]']\n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 16)                   0         ['imag_support_layer_1[0][0]']\n",
      "                                                                                                  \n",
      " real_layer2 (SecondLayer)   (None, 16)                   24        ['leaky_re_lu_1[0][0]']       \n",
      "                                                                                                  \n",
      " imag_layer2 (SecondLayer)   (None, 16)                   24        ['leaky_re_lu_4[0][0]']       \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 16)                   0         ['real_layer2[0][0]']         \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 16)                   0         ['imag_layer2[0][0]']         \n",
      "                                                                                                  \n",
      " merge_real_imag (Concatena  (None, 32)                   0         ['leaky_re_lu_2[0][0]',       \n",
      " te)                                                                 'leaky_re_lu_5[0][0]']       \n",
      "                                                                                                  \n",
      " output_layer (LinearLayer)  (None, 16)                   32        ['merge_real_imag[0][0]']     \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 16)                   0         ['output_layer[0][0]']        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 264 (1.03 KB)\n",
      "Trainable params: 264 (1.03 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Concatenate, LeakyReLU, Activation, BatchNormalization\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def cosine_loss(y_true, y_pred):\n",
    "    y_true_norm = tf.nn.l2_normalize(y_true, axis=-1)\n",
    "    y_pred_norm = tf.nn.l2_normalize(y_pred, axis=-1)\n",
    "\n",
    "    return 1 - tf.reduce_mean(tf.reduce_sum(y_true_norm * y_pred_norm, axis=-1))  # 1 - Cosine similarity\n",
    "\n",
    "\n",
    "def hybrid_loss(y_true, y_pred):\n",
    "    mse_part = tf.reduce_mean(tf.square(y_true - y_pred))  # Regular MSE\n",
    "    cos_part = cosine_loss(y_true, y_pred)  # Vector-based comparison\n",
    "    return mse_part + cos_part\n",
    "\n",
    "\n",
    "def structured_NN(input_dim, output_dim):\n",
    "    real_input = Input(shape=(input_dim,), name=\"real_input\")\n",
    "    imag_input = Input(shape=(input_dim,), name=\"imag_input\")\n",
    "\n",
    "    # real part\n",
    "    real_x = FirstLayer(units=input_dim, name=\"real_layer1\")(real_input)\n",
    "    real_x = LeakyReLU(alpha=0.1)(real_x)\n",
    "    real_x = CustomLayer(units=input_dim, name=\"real_support_layer_1\")(real_x)\n",
    "    real_x = LeakyReLU(alpha=0.1)(real_x)\n",
    "    real_x = SecondLayer(units=input_dim, name=\"real_layer2\")(real_x)\n",
    "    real_x = LeakyReLU(alpha=0.1)(real_x)\n",
    "\n",
    "    # imaginary part\n",
    "    imag_x = FirstLayer(units=input_dim, name=\"imag_layer1\")(imag_input)\n",
    "    imag_x = LeakyReLU(alpha=0.1)(imag_x)\n",
    "    imag_x = CustomLayer(units=input_dim, name=\"imag_support_layer_1\")(imag_x)\n",
    "    imag_x = LeakyReLU(alpha=0.1)(imag_x)\n",
    "    imag_x = SecondLayer(units=input_dim, name=\"imag_layer2\")(imag_x)\n",
    "    imag_x = LeakyReLU(alpha=0.1)(imag_x)\n",
    "    \n",
    "\n",
    "    merged = Concatenate(name=\"merge_real_imag\")([real_x, imag_x])\n",
    "\n",
    "    output = LinearLayer(units=output_dim, name=\"output_layer\")(merged)\n",
    "    output = Activation('sigmoid')(output)  # linear - Keep outputs unrestricted\n",
    "\n",
    "    model = Model(inputs=[real_input, imag_input], outputs=output)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=hybrid_loss, #'mean_squared_error',\n",
    "        metrics=['mse', 'mae']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "input_dim = X_real_train.shape[1]\n",
    "output_dim = y_train.shape[1]\n",
    "model = structured_NN(input_dim, output_dim)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real_input [(None, 16)]\n",
      "imag_input [(None, 16)]\n",
      "real_layer1 (None, 16)\n",
      "imag_layer1 (None, 16)\n",
      "leaky_re_lu (None, 16)\n",
      "leaky_re_lu_3 (None, 16)\n",
      "real_support_layer_1 (None, 16)\n",
      "imag_support_layer_1 (None, 16)\n",
      "leaky_re_lu_1 (None, 16)\n",
      "leaky_re_lu_4 (None, 16)\n",
      "real_layer2 (None, 16)\n",
      "imag_layer2 (None, 16)\n",
      "leaky_re_lu_2 (None, 16)\n",
      "leaky_re_lu_5 (None, 16)\n",
      "merge_real_imag (None, 32)\n",
      "output_layer (None, 16)\n",
      "activation (None, 16)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name, layer.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "adjust_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "linear layer input: (16, 32)\n",
      "linear layer output: (16, 16)\n",
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "linear layer input: (16, 32)\n",
      "linear layer output: (16, 16)\n",
      "45/50 [==========================>...] - ETA: 0s - loss: 0.3999 - mse: 0.1855 - mae: 0.3543linear layer input: (None, 32)\n",
      "linear layer output: (None, 16)\n",
      "50/50 [==============================] - 6s 26ms/step - loss: 0.3981 - mse: 0.1845 - mae: 0.3539 - val_loss: 0.3849 - val_mse: 0.1745 - val_mae: 0.3460 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3621 - mse: 0.1623 - mae: 0.3379 - val_loss: 0.3466 - val_mse: 0.1504 - val_mae: 0.3267 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3275 - mse: 0.1407 - mae: 0.3203 - val_loss: 0.3193 - val_mse: 0.1331 - val_mae: 0.3124 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.3076 - mse: 0.1285 - mae: 0.3103 - val_loss: 0.3058 - val_mse: 0.1252 - val_mae: 0.3043 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2972 - mse: 0.1230 - mae: 0.3036 - val_loss: 0.2968 - val_mse: 0.1210 - val_mae: 0.2983 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2896 - mse: 0.1198 - mae: 0.2984 - val_loss: 0.2899 - val_mse: 0.1183 - val_mae: 0.2941 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2836 - mse: 0.1175 - mae: 0.2949 - val_loss: 0.2844 - val_mse: 0.1164 - val_mae: 0.2915 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2781 - mse: 0.1156 - mae: 0.2921 - val_loss: 0.2789 - val_mse: 0.1145 - val_mae: 0.2890 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2724 - mse: 0.1136 - mae: 0.2891 - val_loss: 0.2734 - val_mse: 0.1127 - val_mae: 0.2864 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2667 - mse: 0.1116 - mae: 0.2860 - val_loss: 0.2677 - val_mse: 0.1107 - val_mae: 0.2836 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2608 - mse: 0.1095 - mae: 0.2829 - val_loss: 0.2620 - val_mse: 0.1089 - val_mae: 0.2810 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2553 - mse: 0.1076 - mae: 0.2799 - val_loss: 0.2566 - val_mse: 0.1070 - val_mae: 0.2783 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2502 - mse: 0.1057 - mae: 0.2770 - val_loss: 0.2518 - val_mse: 0.1055 - val_mae: 0.2757 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2454 - mse: 0.1041 - mae: 0.2743 - val_loss: 0.2472 - val_mse: 0.1039 - val_mae: 0.2733 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2410 - mse: 0.1025 - mae: 0.2719 - val_loss: 0.2431 - val_mse: 0.1025 - val_mae: 0.2713 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2359 - mse: 0.1007 - mae: 0.2690 - val_loss: 0.2370 - val_mse: 0.1005 - val_mae: 0.2680 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2294 - mse: 0.0983 - mae: 0.2651 - val_loss: 0.2312 - val_mse: 0.0986 - val_mae: 0.2652 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2238 - mse: 0.0963 - mae: 0.2619 - val_loss: 0.2258 - val_mse: 0.0964 - val_mae: 0.2623 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2186 - mse: 0.0942 - mae: 0.2586 - val_loss: 0.2207 - val_mse: 0.0945 - val_mae: 0.2595 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2135 - mse: 0.0922 - mae: 0.2555 - val_loss: 0.2158 - val_mse: 0.0924 - val_mae: 0.2564 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2086 - mse: 0.0900 - mae: 0.2523 - val_loss: 0.2113 - val_mse: 0.0904 - val_mae: 0.2535 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2040 - mse: 0.0881 - mae: 0.2493 - val_loss: 0.2068 - val_mse: 0.0883 - val_mae: 0.2503 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1998 - mse: 0.0862 - mae: 0.2464 - val_loss: 0.2030 - val_mse: 0.0866 - val_mae: 0.2476 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1960 - mse: 0.0844 - mae: 0.2437 - val_loss: 0.1993 - val_mse: 0.0849 - val_mae: 0.2450 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1926 - mse: 0.0829 - mae: 0.2413 - val_loss: 0.1963 - val_mse: 0.0835 - val_mae: 0.2427 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1897 - mse: 0.0816 - mae: 0.2392 - val_loss: 0.1933 - val_mse: 0.0822 - val_mae: 0.2405 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1865 - mse: 0.0801 - mae: 0.2368 - val_loss: 0.1898 - val_mse: 0.0806 - val_mae: 0.2379 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1829 - mse: 0.0786 - mae: 0.2344 - val_loss: 0.1863 - val_mse: 0.0793 - val_mae: 0.2357 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1797 - mse: 0.0772 - mae: 0.2322 - val_loss: 0.1835 - val_mse: 0.0779 - val_mae: 0.2333 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1772 - mse: 0.0760 - mae: 0.2302 - val_loss: 0.1810 - val_mse: 0.0768 - val_mae: 0.2314 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1749 - mse: 0.0750 - mae: 0.2285 - val_loss: 0.1788 - val_mse: 0.0757 - val_mae: 0.2295 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1729 - mse: 0.0741 - mae: 0.2267 - val_loss: 0.1768 - val_mse: 0.0747 - val_mae: 0.2278 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1711 - mse: 0.0731 - mae: 0.2251 - val_loss: 0.1751 - val_mse: 0.0739 - val_mae: 0.2263 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1694 - mse: 0.0724 - mae: 0.2236 - val_loss: 0.1733 - val_mse: 0.0730 - val_mae: 0.2247 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1677 - mse: 0.0716 - mae: 0.2221 - val_loss: 0.1717 - val_mse: 0.0723 - val_mae: 0.2232 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1663 - mse: 0.0709 - mae: 0.2207 - val_loss: 0.1703 - val_mse: 0.0716 - val_mae: 0.2219 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1647 - mse: 0.0702 - mae: 0.2193 - val_loss: 0.1690 - val_mse: 0.0710 - val_mae: 0.2209 - lr: 0.0010\n",
      "Epoch 38/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1634 - mse: 0.0695 - mae: 0.2180 - val_loss: 0.1676 - val_mse: 0.0703 - val_mae: 0.2193 - lr: 0.0010\n",
      "Epoch 39/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1621 - mse: 0.0689 - mae: 0.2168 - val_loss: 0.1663 - val_mse: 0.0697 - val_mae: 0.2180 - lr: 0.0010\n",
      "Epoch 40/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1608 - mse: 0.0683 - mae: 0.2156 - val_loss: 0.1651 - val_mse: 0.0692 - val_mae: 0.2174 - lr: 0.0010\n",
      "Epoch 41/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1596 - mse: 0.0678 - mae: 0.2146 - val_loss: 0.1640 - val_mse: 0.0686 - val_mae: 0.2163 - lr: 0.0010\n",
      "Epoch 42/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1584 - mse: 0.0672 - mae: 0.2136 - val_loss: 0.1627 - val_mse: 0.0679 - val_mae: 0.2151 - lr: 0.0010\n",
      "Epoch 43/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1571 - mse: 0.0666 - mae: 0.2125 - val_loss: 0.1615 - val_mse: 0.0674 - val_mae: 0.2140 - lr: 0.0010\n",
      "Epoch 44/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1560 - mse: 0.0661 - mae: 0.2114 - val_loss: 0.1602 - val_mse: 0.0668 - val_mae: 0.2128 - lr: 0.0010\n",
      "Epoch 45/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1547 - mse: 0.0655 - mae: 0.2103 - val_loss: 0.1592 - val_mse: 0.0664 - val_mae: 0.2119 - lr: 0.0010\n",
      "Epoch 46/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1536 - mse: 0.0650 - mae: 0.2093 - val_loss: 0.1581 - val_mse: 0.0659 - val_mae: 0.2110 - lr: 0.0010\n",
      "Epoch 47/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1524 - mse: 0.0644 - mae: 0.2082 - val_loss: 0.1569 - val_mse: 0.0653 - val_mae: 0.2098 - lr: 0.0010\n",
      "Epoch 48/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1512 - mse: 0.0639 - mae: 0.2071 - val_loss: 0.1556 - val_mse: 0.0647 - val_mae: 0.2088 - lr: 0.0010\n",
      "Epoch 49/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1500 - mse: 0.0634 - mae: 0.2062 - val_loss: 0.1546 - val_mse: 0.0643 - val_mae: 0.2077 - lr: 0.0010\n",
      "Epoch 50/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1489 - mse: 0.0628 - mae: 0.2050 - val_loss: 0.1532 - val_mse: 0.0636 - val_mae: 0.2066 - lr: 0.0010\n",
      "Epoch 51/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1477 - mse: 0.0623 - mae: 0.2040 - val_loss: 0.1523 - val_mse: 0.0632 - val_mae: 0.2056 - lr: 0.0010\n",
      "Epoch 52/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1467 - mse: 0.0619 - mae: 0.2031 - val_loss: 0.1510 - val_mse: 0.0626 - val_mae: 0.2043 - lr: 0.0010\n",
      "Epoch 53/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1457 - mse: 0.0614 - mae: 0.2022 - val_loss: 0.1499 - val_mse: 0.0622 - val_mae: 0.2033 - lr: 0.0010\n",
      "Epoch 54/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1447 - mse: 0.0610 - mae: 0.2012 - val_loss: 0.1493 - val_mse: 0.0619 - val_mae: 0.2025 - lr: 0.0010\n",
      "Epoch 55/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1437 - mse: 0.0606 - mae: 0.2004 - val_loss: 0.1480 - val_mse: 0.0613 - val_mae: 0.2016 - lr: 0.0010\n",
      "Epoch 56/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1429 - mse: 0.0602 - mae: 0.1997 - val_loss: 0.1469 - val_mse: 0.0608 - val_mae: 0.2005 - lr: 0.0010\n",
      "Epoch 57/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1421 - mse: 0.0599 - mae: 0.1989 - val_loss: 0.1462 - val_mse: 0.0606 - val_mae: 0.2000 - lr: 0.0010\n",
      "Epoch 58/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1412 - mse: 0.0595 - mae: 0.1980 - val_loss: 0.1452 - val_mse: 0.0601 - val_mae: 0.1990 - lr: 0.0010\n",
      "Epoch 59/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1403 - mse: 0.0591 - mae: 0.1974 - val_loss: 0.1442 - val_mse: 0.0597 - val_mae: 0.1982 - lr: 0.0010\n",
      "Epoch 60/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1394 - mse: 0.0587 - mae: 0.1966 - val_loss: 0.1433 - val_mse: 0.0593 - val_mae: 0.1974 - lr: 0.0010\n",
      "Epoch 61/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1385 - mse: 0.0583 - mae: 0.1957 - val_loss: 0.1419 - val_mse: 0.0587 - val_mae: 0.1963 - lr: 0.0010\n",
      "Epoch 62/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1370 - mse: 0.0577 - mae: 0.1946 - val_loss: 0.1397 - val_mse: 0.0578 - val_mae: 0.1947 - lr: 0.0010\n",
      "Epoch 63/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1349 - mse: 0.0569 - mae: 0.1931 - val_loss: 0.1375 - val_mse: 0.0570 - val_mae: 0.1932 - lr: 0.0010\n",
      "Epoch 64/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1327 - mse: 0.0560 - mae: 0.1914 - val_loss: 0.1350 - val_mse: 0.0560 - val_mae: 0.1915 - lr: 0.0010\n",
      "Epoch 65/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1306 - mse: 0.0552 - mae: 0.1897 - val_loss: 0.1330 - val_mse: 0.0552 - val_mae: 0.1897 - lr: 0.0010\n",
      "Epoch 66/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1289 - mse: 0.0545 - mae: 0.1885 - val_loss: 0.1318 - val_mse: 0.0547 - val_mae: 0.1889 - lr: 0.0010\n",
      "Epoch 67/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1278 - mse: 0.0541 - mae: 0.1875 - val_loss: 0.1309 - val_mse: 0.0544 - val_mae: 0.1883 - lr: 0.0010\n",
      "Epoch 68/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1269 - mse: 0.0537 - mae: 0.1868 - val_loss: 0.1298 - val_mse: 0.0539 - val_mae: 0.1873 - lr: 0.0010\n",
      "Epoch 69/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1262 - mse: 0.0534 - mae: 0.1863 - val_loss: 0.1290 - val_mse: 0.0536 - val_mae: 0.1867 - lr: 0.0010\n",
      "Epoch 70/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1255 - mse: 0.0531 - mae: 0.1857 - val_loss: 0.1287 - val_mse: 0.0535 - val_mae: 0.1865 - lr: 0.0010\n",
      "Epoch 71/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1249 - mse: 0.0529 - mae: 0.1852 - val_loss: 0.1281 - val_mse: 0.0532 - val_mae: 0.1861 - lr: 0.0010\n",
      "Epoch 72/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1244 - mse: 0.0527 - mae: 0.1848 - val_loss: 0.1275 - val_mse: 0.0530 - val_mae: 0.1855 - lr: 0.0010\n",
      "Epoch 73/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1238 - mse: 0.0524 - mae: 0.1843 - val_loss: 0.1269 - val_mse: 0.0527 - val_mae: 0.1849 - lr: 0.0010\n",
      "Epoch 74/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1233 - mse: 0.0522 - mae: 0.1838 - val_loss: 0.1264 - val_mse: 0.0525 - val_mae: 0.1846 - lr: 0.0010\n",
      "Epoch 75/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1228 - mse: 0.0520 - mae: 0.1835 - val_loss: 0.1258 - val_mse: 0.0522 - val_mae: 0.1841 - lr: 0.0010\n",
      "Epoch 76/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1223 - mse: 0.0518 - mae: 0.1830 - val_loss: 0.1253 - val_mse: 0.0520 - val_mae: 0.1835 - lr: 0.0010\n",
      "Epoch 77/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1218 - mse: 0.0516 - mae: 0.1827 - val_loss: 0.1247 - val_mse: 0.0518 - val_mae: 0.1832 - lr: 0.0010\n",
      "Epoch 78/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1214 - mse: 0.0514 - mae: 0.1822 - val_loss: 0.1243 - val_mse: 0.0516 - val_mae: 0.1826 - lr: 0.0010\n",
      "Epoch 79/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1209 - mse: 0.0512 - mae: 0.1818 - val_loss: 0.1237 - val_mse: 0.0514 - val_mae: 0.1823 - lr: 0.0010\n",
      "Epoch 80/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1205 - mse: 0.0510 - mae: 0.1815 - val_loss: 0.1231 - val_mse: 0.0511 - val_mae: 0.1818 - lr: 0.0010\n",
      "Epoch 81/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1200 - mse: 0.0508 - mae: 0.1811 - val_loss: 0.1228 - val_mse: 0.0510 - val_mae: 0.1815 - lr: 0.0010\n",
      "Epoch 82/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1196 - mse: 0.0507 - mae: 0.1808 - val_loss: 0.1223 - val_mse: 0.0507 - val_mae: 0.1809 - lr: 0.0010\n",
      "Epoch 83/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1192 - mse: 0.0505 - mae: 0.1804 - val_loss: 0.1218 - val_mse: 0.0506 - val_mae: 0.1806 - lr: 0.0010\n",
      "Epoch 84/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1188 - mse: 0.0503 - mae: 0.1801 - val_loss: 0.1214 - val_mse: 0.0504 - val_mae: 0.1803 - lr: 0.0010\n",
      "Epoch 85/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1184 - mse: 0.0502 - mae: 0.1797 - val_loss: 0.1209 - val_mse: 0.0502 - val_mae: 0.1798 - lr: 0.0010\n",
      "Epoch 86/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1180 - mse: 0.0500 - mae: 0.1794 - val_loss: 0.1204 - val_mse: 0.0500 - val_mae: 0.1796 - lr: 0.0010\n",
      "Epoch 87/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1177 - mse: 0.0499 - mae: 0.1791 - val_loss: 0.1201 - val_mse: 0.0499 - val_mae: 0.1795 - lr: 0.0010\n",
      "Epoch 88/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1173 - mse: 0.0497 - mae: 0.1787 - val_loss: 0.1198 - val_mse: 0.0497 - val_mae: 0.1792 - lr: 0.0010\n",
      "Epoch 89/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1169 - mse: 0.0495 - mae: 0.1783 - val_loss: 0.1196 - val_mse: 0.0496 - val_mae: 0.1787 - lr: 0.0010\n",
      "Epoch 90/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1165 - mse: 0.0494 - mae: 0.1781 - val_loss: 0.1190 - val_mse: 0.0494 - val_mae: 0.1782 - lr: 0.0010\n",
      "Epoch 91/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1162 - mse: 0.0492 - mae: 0.1777 - val_loss: 0.1187 - val_mse: 0.0493 - val_mae: 0.1781 - lr: 0.0010\n",
      "Epoch 92/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1158 - mse: 0.0491 - mae: 0.1775 - val_loss: 0.1182 - val_mse: 0.0490 - val_mae: 0.1774 - lr: 0.0010\n",
      "Epoch 93/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1155 - mse: 0.0490 - mae: 0.1772 - val_loss: 0.1179 - val_mse: 0.0489 - val_mae: 0.1773 - lr: 0.0010\n",
      "Epoch 94/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1152 - mse: 0.0488 - mae: 0.1767 - val_loss: 0.1174 - val_mse: 0.0487 - val_mae: 0.1769 - lr: 0.0010\n",
      "Epoch 95/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1148 - mse: 0.0486 - mae: 0.1765 - val_loss: 0.1170 - val_mse: 0.0486 - val_mae: 0.1768 - lr: 0.0010\n",
      "Epoch 96/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1144 - mse: 0.0485 - mae: 0.1762 - val_loss: 0.1167 - val_mse: 0.0484 - val_mae: 0.1765 - lr: 0.0010\n",
      "Epoch 97/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1140 - mse: 0.0483 - mae: 0.1759 - val_loss: 0.1164 - val_mse: 0.0483 - val_mae: 0.1761 - lr: 0.0010\n",
      "Epoch 98/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1138 - mse: 0.0482 - mae: 0.1756 - val_loss: 0.1161 - val_mse: 0.0482 - val_mae: 0.1759 - lr: 0.0010\n",
      "Epoch 99/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1134 - mse: 0.0481 - mae: 0.1753 - val_loss: 0.1158 - val_mse: 0.0480 - val_mae: 0.1754 - lr: 0.0010\n",
      "Epoch 100/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1131 - mse: 0.0479 - mae: 0.1750 - val_loss: 0.1155 - val_mse: 0.0479 - val_mae: 0.1753 - lr: 0.0010\n",
      "Epoch 101/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1127 - mse: 0.0478 - mae: 0.1746 - val_loss: 0.1149 - val_mse: 0.0477 - val_mae: 0.1747 - lr: 0.0010\n",
      "Epoch 102/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1124 - mse: 0.0476 - mae: 0.1743 - val_loss: 0.1147 - val_mse: 0.0475 - val_mae: 0.1743 - lr: 0.0010\n",
      "Epoch 103/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1121 - mse: 0.0475 - mae: 0.1740 - val_loss: 0.1144 - val_mse: 0.0475 - val_mae: 0.1744 - lr: 0.0010\n",
      "Epoch 104/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1119 - mse: 0.0474 - mae: 0.1738 - val_loss: 0.1139 - val_mse: 0.0473 - val_mae: 0.1740 - lr: 0.0010\n",
      "Epoch 105/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1115 - mse: 0.0473 - mae: 0.1736 - val_loss: 0.1138 - val_mse: 0.0472 - val_mae: 0.1736 - lr: 0.0010\n",
      "Epoch 106/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1111 - mse: 0.0471 - mae: 0.1731 - val_loss: 0.1134 - val_mse: 0.0470 - val_mae: 0.1733 - lr: 0.0010\n",
      "Epoch 107/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1107 - mse: 0.0469 - mae: 0.1729 - val_loss: 0.1130 - val_mse: 0.0468 - val_mae: 0.1728 - lr: 0.0010\n",
      "Epoch 108/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1104 - mse: 0.0468 - mae: 0.1725 - val_loss: 0.1128 - val_mse: 0.0468 - val_mae: 0.1728 - lr: 0.0010\n",
      "Epoch 109/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1101 - mse: 0.0467 - mae: 0.1723 - val_loss: 0.1124 - val_mse: 0.0466 - val_mae: 0.1725 - lr: 0.0010\n",
      "Epoch 110/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1097 - mse: 0.0465 - mae: 0.1718 - val_loss: 0.1119 - val_mse: 0.0464 - val_mae: 0.1724 - lr: 0.0010\n",
      "Epoch 111/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1094 - mse: 0.0464 - mae: 0.1717 - val_loss: 0.1115 - val_mse: 0.0462 - val_mae: 0.1719 - lr: 0.0010\n",
      "Epoch 112/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1092 - mse: 0.0463 - mae: 0.1716 - val_loss: 0.1112 - val_mse: 0.0461 - val_mae: 0.1716 - lr: 0.0010\n",
      "Epoch 113/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1088 - mse: 0.0461 - mae: 0.1711 - val_loss: 0.1111 - val_mse: 0.0461 - val_mae: 0.1718 - lr: 0.0010\n",
      "Epoch 114/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1085 - mse: 0.0460 - mae: 0.1707 - val_loss: 0.1108 - val_mse: 0.0460 - val_mae: 0.1711 - lr: 0.0010\n",
      "Epoch 115/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1082 - mse: 0.0459 - mae: 0.1706 - val_loss: 0.1106 - val_mse: 0.0458 - val_mae: 0.1705 - lr: 0.0010\n",
      "Epoch 116/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1079 - mse: 0.0457 - mae: 0.1702 - val_loss: 0.1100 - val_mse: 0.0456 - val_mae: 0.1705 - lr: 0.0010\n",
      "Epoch 117/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1076 - mse: 0.0456 - mae: 0.1700 - val_loss: 0.1098 - val_mse: 0.0455 - val_mae: 0.1702 - lr: 0.0010\n",
      "Epoch 118/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1073 - mse: 0.0455 - mae: 0.1697 - val_loss: 0.1091 - val_mse: 0.0452 - val_mae: 0.1696 - lr: 0.0010\n",
      "Epoch 119/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1071 - mse: 0.0454 - mae: 0.1695 - val_loss: 0.1091 - val_mse: 0.0452 - val_mae: 0.1694 - lr: 0.0010\n",
      "Epoch 120/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1067 - mse: 0.0452 - mae: 0.1691 - val_loss: 0.1089 - val_mse: 0.0451 - val_mae: 0.1693 - lr: 0.0010\n",
      "Epoch 121/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1065 - mse: 0.0451 - mae: 0.1689 - val_loss: 0.1084 - val_mse: 0.0449 - val_mae: 0.1689 - lr: 0.0010\n",
      "Epoch 122/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1062 - mse: 0.0450 - mae: 0.1686 - val_loss: 0.1082 - val_mse: 0.0449 - val_mae: 0.1688 - lr: 0.0010\n",
      "Epoch 123/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1060 - mse: 0.0449 - mae: 0.1684 - val_loss: 0.1076 - val_mse: 0.0446 - val_mae: 0.1680 - lr: 0.0010\n",
      "Epoch 124/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1056 - mse: 0.0447 - mae: 0.1681 - val_loss: 0.1074 - val_mse: 0.0445 - val_mae: 0.1678 - lr: 0.0010\n",
      "Epoch 125/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1054 - mse: 0.0447 - mae: 0.1679 - val_loss: 0.1071 - val_mse: 0.0444 - val_mae: 0.1674 - lr: 0.0010\n",
      "Epoch 126/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1052 - mse: 0.0446 - mae: 0.1676 - val_loss: 0.1071 - val_mse: 0.0443 - val_mae: 0.1671 - lr: 0.0010\n",
      "Epoch 127/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1049 - mse: 0.0444 - mae: 0.1674 - val_loss: 0.1069 - val_mse: 0.0443 - val_mae: 0.1674 - lr: 0.0010\n",
      "Epoch 128/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1046 - mse: 0.0443 - mae: 0.1671 - val_loss: 0.1065 - val_mse: 0.0441 - val_mae: 0.1669 - lr: 0.0010\n",
      "Epoch 129/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1043 - mse: 0.0442 - mae: 0.1669 - val_loss: 0.1064 - val_mse: 0.0441 - val_mae: 0.1668 - lr: 0.0010\n",
      "Epoch 130/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1041 - mse: 0.0441 - mae: 0.1666 - val_loss: 0.1059 - val_mse: 0.0439 - val_mae: 0.1661 - lr: 0.0010\n",
      "Epoch 131/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1038 - mse: 0.0440 - mae: 0.1663 - val_loss: 0.1059 - val_mse: 0.0439 - val_mae: 0.1663 - lr: 0.0010\n",
      "Epoch 132/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1037 - mse: 0.0439 - mae: 0.1662 - val_loss: 0.1054 - val_mse: 0.0437 - val_mae: 0.1658 - lr: 0.0010\n",
      "Epoch 133/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1034 - mse: 0.0438 - mae: 0.1659 - val_loss: 0.1054 - val_mse: 0.0436 - val_mae: 0.1655 - lr: 0.0010\n",
      "Epoch 134/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1032 - mse: 0.0437 - mae: 0.1658 - val_loss: 0.1055 - val_mse: 0.0437 - val_mae: 0.1654 - lr: 0.0010\n",
      "Epoch 135/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1030 - mse: 0.0437 - mae: 0.1656 - val_loss: 0.1048 - val_mse: 0.0434 - val_mae: 0.1651 - lr: 0.0010\n",
      "Epoch 136/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1027 - mse: 0.0435 - mae: 0.1651 - val_loss: 0.1045 - val_mse: 0.0433 - val_mae: 0.1652 - lr: 0.0010\n",
      "Epoch 137/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1025 - mse: 0.0435 - mae: 0.1651 - val_loss: 0.1044 - val_mse: 0.0433 - val_mae: 0.1649 - lr: 0.0010\n",
      "Epoch 138/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1023 - mse: 0.0433 - mae: 0.1649 - val_loss: 0.1041 - val_mse: 0.0431 - val_mae: 0.1644 - lr: 0.0010\n",
      "Epoch 139/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1021 - mse: 0.0432 - mae: 0.1646 - val_loss: 0.1039 - val_mse: 0.0430 - val_mae: 0.1641 - lr: 0.0010\n",
      "Epoch 140/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1019 - mse: 0.0432 - mae: 0.1644 - val_loss: 0.1037 - val_mse: 0.0430 - val_mae: 0.1641 - lr: 0.0010\n",
      "Epoch 141/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1017 - mse: 0.0431 - mae: 0.1642 - val_loss: 0.1035 - val_mse: 0.0428 - val_mae: 0.1636 - lr: 0.0010\n",
      "Epoch 142/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1014 - mse: 0.0430 - mae: 0.1639 - val_loss: 0.1030 - val_mse: 0.0427 - val_mae: 0.1633 - lr: 0.0010\n",
      "Epoch 143/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1013 - mse: 0.0429 - mae: 0.1638 - val_loss: 0.1029 - val_mse: 0.0426 - val_mae: 0.1633 - lr: 0.0010\n",
      "Epoch 144/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1011 - mse: 0.0428 - mae: 0.1636 - val_loss: 0.1033 - val_mse: 0.0428 - val_mae: 0.1639 - lr: 0.0010\n",
      "Epoch 145/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1009 - mse: 0.0428 - mae: 0.1634 - val_loss: 0.1024 - val_mse: 0.0424 - val_mae: 0.1627 - lr: 0.0010\n",
      "Epoch 146/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1007 - mse: 0.0427 - mae: 0.1631 - val_loss: 0.1025 - val_mse: 0.0425 - val_mae: 0.1626 - lr: 0.0010\n",
      "Epoch 147/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1005 - mse: 0.0426 - mae: 0.1630 - val_loss: 0.1024 - val_mse: 0.0424 - val_mae: 0.1625 - lr: 0.0010\n",
      "Epoch 148/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1003 - mse: 0.0425 - mae: 0.1628 - val_loss: 0.1021 - val_mse: 0.0423 - val_mae: 0.1623 - lr: 0.0010\n",
      "Epoch 149/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1001 - mse: 0.0424 - mae: 0.1625 - val_loss: 0.1020 - val_mse: 0.0423 - val_mae: 0.1626 - lr: 0.0010\n",
      "Epoch 150/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1000 - mse: 0.0424 - mae: 0.1624 - val_loss: 0.1017 - val_mse: 0.0422 - val_mae: 0.1618 - lr: 0.0010\n",
      "Epoch 151/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0997 - mse: 0.0423 - mae: 0.1621 - val_loss: 0.1013 - val_mse: 0.0420 - val_mae: 0.1617 - lr: 0.0010\n",
      "Epoch 152/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0995 - mse: 0.0422 - mae: 0.1619 - val_loss: 0.1015 - val_mse: 0.0421 - val_mae: 0.1616 - lr: 0.0010\n",
      "Epoch 153/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0994 - mse: 0.0421 - mae: 0.1617 - val_loss: 0.1011 - val_mse: 0.0419 - val_mae: 0.1615 - lr: 0.0010\n",
      "Epoch 154/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0992 - mse: 0.0420 - mae: 0.1615 - val_loss: 0.1009 - val_mse: 0.0418 - val_mae: 0.1614 - lr: 0.0010\n",
      "Epoch 155/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0991 - mse: 0.0420 - mae: 0.1614 - val_loss: 0.1009 - val_mse: 0.0418 - val_mae: 0.1611 - lr: 0.0010\n",
      "Epoch 156/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0988 - mse: 0.0419 - mae: 0.1612 - val_loss: 0.1005 - val_mse: 0.0416 - val_mae: 0.1606 - lr: 0.0010\n",
      "Epoch 157/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0987 - mse: 0.0419 - mae: 0.1611 - val_loss: 0.1004 - val_mse: 0.0416 - val_mae: 0.1607 - lr: 0.0010\n",
      "Epoch 158/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0985 - mse: 0.0417 - mae: 0.1608 - val_loss: 0.1003 - val_mse: 0.0415 - val_mae: 0.1603 - lr: 0.0010\n",
      "Epoch 159/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0983 - mse: 0.0417 - mae: 0.1606 - val_loss: 0.0999 - val_mse: 0.0414 - val_mae: 0.1599 - lr: 0.0010\n",
      "Epoch 160/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0982 - mse: 0.0416 - mae: 0.1605 - val_loss: 0.0998 - val_mse: 0.0414 - val_mae: 0.1600 - lr: 0.0010\n",
      "Epoch 161/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0981 - mse: 0.0416 - mae: 0.1602 - val_loss: 0.0998 - val_mse: 0.0414 - val_mae: 0.1603 - lr: 0.0010\n",
      "Epoch 162/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0980 - mse: 0.0415 - mae: 0.1602 - val_loss: 0.0995 - val_mse: 0.0412 - val_mae: 0.1596 - lr: 0.0010\n",
      "Epoch 163/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0978 - mse: 0.0414 - mae: 0.1599 - val_loss: 0.0995 - val_mse: 0.0412 - val_mae: 0.1593 - lr: 0.0010\n",
      "Epoch 164/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0977 - mse: 0.0414 - mae: 0.1598 - val_loss: 0.0993 - val_mse: 0.0411 - val_mae: 0.1593 - lr: 0.0010\n",
      "Epoch 165/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0975 - mse: 0.0413 - mae: 0.1596 - val_loss: 0.0992 - val_mse: 0.0411 - val_mae: 0.1595 - lr: 0.0010\n",
      "Epoch 166/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0973 - mse: 0.0413 - mae: 0.1595 - val_loss: 0.0991 - val_mse: 0.0411 - val_mae: 0.1593 - lr: 0.0010\n",
      "Epoch 167/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0972 - mse: 0.0412 - mae: 0.1592 - val_loss: 0.0989 - val_mse: 0.0410 - val_mae: 0.1590 - lr: 0.0010\n",
      "Epoch 168/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0970 - mse: 0.0412 - mae: 0.1591 - val_loss: 0.0985 - val_mse: 0.0408 - val_mae: 0.1589 - lr: 0.0010\n",
      "Epoch 169/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0970 - mse: 0.0411 - mae: 0.1590 - val_loss: 0.0985 - val_mse: 0.0408 - val_mae: 0.1585 - lr: 0.0010\n",
      "Epoch 170/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0968 - mse: 0.0411 - mae: 0.1588 - val_loss: 0.0985 - val_mse: 0.0408 - val_mae: 0.1582 - lr: 0.0010\n",
      "Epoch 171/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0967 - mse: 0.0410 - mae: 0.1586 - val_loss: 0.0985 - val_mse: 0.0408 - val_mae: 0.1585 - lr: 0.0010\n",
      "Epoch 172/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0965 - mse: 0.0409 - mae: 0.1584 - val_loss: 0.0981 - val_mse: 0.0406 - val_mae: 0.1579 - lr: 0.0010\n",
      "Epoch 173/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0964 - mse: 0.0409 - mae: 0.1584 - val_loss: 0.0980 - val_mse: 0.0406 - val_mae: 0.1581 - lr: 0.0010\n",
      "Epoch 174/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0963 - mse: 0.0408 - mae: 0.1581 - val_loss: 0.0978 - val_mse: 0.0405 - val_mae: 0.1577 - lr: 0.0010\n",
      "Epoch 175/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0956 - mse: 0.0406 - mae: 0.1577 - val_loss: 0.0959 - val_mse: 0.0397 - val_mae: 0.1563 - lr: 0.0010\n",
      "Epoch 176/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0918 - mse: 0.0390 - mae: 0.1552 - val_loss: 0.0912 - val_mse: 0.0380 - val_mae: 0.1541 - lr: 0.0010\n",
      "Epoch 177/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0869 - mse: 0.0371 - mae: 0.1516 - val_loss: 0.0872 - val_mse: 0.0364 - val_mae: 0.1506 - lr: 0.0010\n",
      "Epoch 178/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0844 - mse: 0.0361 - mae: 0.1493 - val_loss: 0.0857 - val_mse: 0.0358 - val_mae: 0.1492 - lr: 0.0010\n",
      "Epoch 179/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0833 - mse: 0.0357 - mae: 0.1485 - val_loss: 0.0850 - val_mse: 0.0355 - val_mae: 0.1482 - lr: 0.0010\n",
      "Epoch 180/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0828 - mse: 0.0355 - mae: 0.1478 - val_loss: 0.0843 - val_mse: 0.0352 - val_mae: 0.1478 - lr: 0.0010\n",
      "Epoch 181/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0823 - mse: 0.0352 - mae: 0.1472 - val_loss: 0.0835 - val_mse: 0.0349 - val_mae: 0.1469 - lr: 0.0010\n",
      "Epoch 182/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0819 - mse: 0.0351 - mae: 0.1471 - val_loss: 0.0832 - val_mse: 0.0348 - val_mae: 0.1468 - lr: 0.0010\n",
      "Epoch 183/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0816 - mse: 0.0350 - mae: 0.1467 - val_loss: 0.0830 - val_mse: 0.0347 - val_mae: 0.1463 - lr: 0.0010\n",
      "Epoch 184/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0813 - mse: 0.0349 - mae: 0.1462 - val_loss: 0.0827 - val_mse: 0.0346 - val_mae: 0.1460 - lr: 0.0010\n",
      "Epoch 185/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0810 - mse: 0.0348 - mae: 0.1461 - val_loss: 0.0824 - val_mse: 0.0345 - val_mae: 0.1459 - lr: 0.0010\n",
      "Epoch 186/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0808 - mse: 0.0346 - mae: 0.1457 - val_loss: 0.0820 - val_mse: 0.0343 - val_mae: 0.1454 - lr: 0.0010\n",
      "Epoch 187/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0806 - mse: 0.0346 - mae: 0.1456 - val_loss: 0.0817 - val_mse: 0.0342 - val_mae: 0.1453 - lr: 0.0010\n",
      "Epoch 188/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0804 - mse: 0.0345 - mae: 0.1454 - val_loss: 0.0814 - val_mse: 0.0340 - val_mae: 0.1446 - lr: 0.0010\n",
      "Epoch 189/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0801 - mse: 0.0344 - mae: 0.1451 - val_loss: 0.0815 - val_mse: 0.0341 - val_mae: 0.1447 - lr: 0.0010\n",
      "Epoch 190/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0799 - mse: 0.0343 - mae: 0.1449 - val_loss: 0.0812 - val_mse: 0.0340 - val_mae: 0.1447 - lr: 0.0010\n",
      "Epoch 191/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0798 - mse: 0.0343 - mae: 0.1447 - val_loss: 0.0809 - val_mse: 0.0339 - val_mae: 0.1443 - lr: 0.0010\n",
      "Epoch 192/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0796 - mse: 0.0342 - mae: 0.1445 - val_loss: 0.0805 - val_mse: 0.0337 - val_mae: 0.1439 - lr: 0.0010\n",
      "Epoch 193/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0794 - mse: 0.0341 - mae: 0.1443 - val_loss: 0.0803 - val_mse: 0.0336 - val_mae: 0.1436 - lr: 0.0010\n",
      "Epoch 194/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0793 - mse: 0.0340 - mae: 0.1441 - val_loss: 0.0802 - val_mse: 0.0336 - val_mae: 0.1438 - lr: 0.0010\n",
      "Epoch 195/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0791 - mse: 0.0340 - mae: 0.1440 - val_loss: 0.0800 - val_mse: 0.0335 - val_mae: 0.1433 - lr: 0.0010\n",
      "Epoch 196/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0790 - mse: 0.0339 - mae: 0.1438 - val_loss: 0.0801 - val_mse: 0.0335 - val_mae: 0.1430 - lr: 0.0010\n",
      "Epoch 197/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0788 - mse: 0.0338 - mae: 0.1435 - val_loss: 0.0797 - val_mse: 0.0334 - val_mae: 0.1429 - lr: 0.0010\n",
      "Epoch 198/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0788 - mse: 0.0339 - mae: 0.1436 - val_loss: 0.0798 - val_mse: 0.0334 - val_mae: 0.1427 - lr: 0.0010\n",
      "Epoch 199/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0785 - mse: 0.0337 - mae: 0.1432 - val_loss: 0.0795 - val_mse: 0.0332 - val_mae: 0.1424 - lr: 0.0010\n",
      "Epoch 200/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0783 - mse: 0.0336 - mae: 0.1429 - val_loss: 0.0792 - val_mse: 0.0331 - val_mae: 0.1422 - lr: 0.0010\n",
      "Epoch 201/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0782 - mse: 0.0336 - mae: 0.1429 - val_loss: 0.0792 - val_mse: 0.0332 - val_mae: 0.1421 - lr: 0.0010\n",
      "Epoch 202/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0781 - mse: 0.0335 - mae: 0.1426 - val_loss: 0.0789 - val_mse: 0.0330 - val_mae: 0.1422 - lr: 0.0010\n",
      "Epoch 203/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0779 - mse: 0.0335 - mae: 0.1425 - val_loss: 0.0786 - val_mse: 0.0329 - val_mae: 0.1417 - lr: 0.0010\n",
      "Epoch 204/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0778 - mse: 0.0334 - mae: 0.1423 - val_loss: 0.0786 - val_mse: 0.0329 - val_mae: 0.1419 - lr: 0.0010\n",
      "Epoch 205/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0776 - mse: 0.0334 - mae: 0.1421 - val_loss: 0.0784 - val_mse: 0.0328 - val_mae: 0.1410 - lr: 0.0010\n",
      "Epoch 206/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0775 - mse: 0.0333 - mae: 0.1419 - val_loss: 0.0783 - val_mse: 0.0327 - val_mae: 0.1408 - lr: 0.0010\n",
      "Epoch 207/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0773 - mse: 0.0332 - mae: 0.1418 - val_loss: 0.0781 - val_mse: 0.0326 - val_mae: 0.1406 - lr: 0.0010\n",
      "Epoch 208/500\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0772 - mse: 0.0332 - mae: 0.1416 - val_loss: 0.0781 - val_mse: 0.0327 - val_mae: 0.1406 - lr: 0.0010\n",
      "Epoch 209/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0770 - mse: 0.0331 - mae: 0.1413 - val_loss: 0.0778 - val_mse: 0.0325 - val_mae: 0.1405 - lr: 0.0010\n",
      "Epoch 210/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0769 - mse: 0.0330 - mae: 0.1411 - val_loss: 0.0777 - val_mse: 0.0325 - val_mae: 0.1405 - lr: 0.0010\n",
      "Epoch 211/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0768 - mse: 0.0330 - mae: 0.1411 - val_loss: 0.0774 - val_mse: 0.0324 - val_mae: 0.1402 - lr: 0.0010\n",
      "Epoch 212/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0766 - mse: 0.0329 - mae: 0.1409 - val_loss: 0.0772 - val_mse: 0.0323 - val_mae: 0.1398 - lr: 0.0010\n",
      "Epoch 213/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0764 - mse: 0.0328 - mae: 0.1407 - val_loss: 0.0770 - val_mse: 0.0323 - val_mae: 0.1400 - lr: 0.0010\n",
      "Epoch 214/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0763 - mse: 0.0328 - mae: 0.1406 - val_loss: 0.0769 - val_mse: 0.0322 - val_mae: 0.1396 - lr: 0.0010\n",
      "Epoch 215/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0761 - mse: 0.0327 - mae: 0.1403 - val_loss: 0.0768 - val_mse: 0.0321 - val_mae: 0.1392 - lr: 0.0010\n",
      "Epoch 216/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0760 - mse: 0.0326 - mae: 0.1402 - val_loss: 0.0764 - val_mse: 0.0320 - val_mae: 0.1390 - lr: 0.0010\n",
      "Epoch 217/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0758 - mse: 0.0326 - mae: 0.1401 - val_loss: 0.0763 - val_mse: 0.0319 - val_mae: 0.1388 - lr: 0.0010\n",
      "Epoch 218/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0757 - mse: 0.0326 - mae: 0.1399 - val_loss: 0.0759 - val_mse: 0.0318 - val_mae: 0.1384 - lr: 0.0010\n",
      "Epoch 219/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0755 - mse: 0.0324 - mae: 0.1396 - val_loss: 0.0760 - val_mse: 0.0318 - val_mae: 0.1385 - lr: 0.0010\n",
      "Epoch 220/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0753 - mse: 0.0324 - mae: 0.1395 - val_loss: 0.0759 - val_mse: 0.0318 - val_mae: 0.1387 - lr: 0.0010\n",
      "Epoch 221/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0752 - mse: 0.0323 - mae: 0.1394 - val_loss: 0.0755 - val_mse: 0.0316 - val_mae: 0.1380 - lr: 0.0010\n",
      "Epoch 222/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0750 - mse: 0.0322 - mae: 0.1390 - val_loss: 0.0755 - val_mse: 0.0316 - val_mae: 0.1380 - lr: 0.0010\n",
      "Epoch 223/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0748 - mse: 0.0322 - mae: 0.1390 - val_loss: 0.0753 - val_mse: 0.0316 - val_mae: 0.1381 - lr: 0.0010\n",
      "Epoch 224/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0747 - mse: 0.0321 - mae: 0.1387 - val_loss: 0.0752 - val_mse: 0.0315 - val_mae: 0.1378 - lr: 0.0010\n",
      "Epoch 225/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0745 - mse: 0.0320 - mae: 0.1385 - val_loss: 0.0748 - val_mse: 0.0313 - val_mae: 0.1374 - lr: 0.0010\n",
      "Epoch 226/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0743 - mse: 0.0320 - mae: 0.1384 - val_loss: 0.0746 - val_mse: 0.0312 - val_mae: 0.1369 - lr: 0.0010\n",
      "Epoch 227/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0741 - mse: 0.0319 - mae: 0.1381 - val_loss: 0.0744 - val_mse: 0.0312 - val_mae: 0.1368 - lr: 0.0010\n",
      "Epoch 228/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0739 - mse: 0.0318 - mae: 0.1381 - val_loss: 0.0745 - val_mse: 0.0312 - val_mae: 0.1370 - lr: 0.0010\n",
      "Epoch 229/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0738 - mse: 0.0317 - mae: 0.1378 - val_loss: 0.0743 - val_mse: 0.0311 - val_mae: 0.1365 - lr: 0.0010\n",
      "Epoch 230/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0736 - mse: 0.0317 - mae: 0.1376 - val_loss: 0.0739 - val_mse: 0.0310 - val_mae: 0.1364 - lr: 0.0010\n",
      "Epoch 231/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0734 - mse: 0.0316 - mae: 0.1374 - val_loss: 0.0738 - val_mse: 0.0309 - val_mae: 0.1363 - lr: 0.0010\n",
      "Epoch 232/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0733 - mse: 0.0315 - mae: 0.1372 - val_loss: 0.0735 - val_mse: 0.0309 - val_mae: 0.1360 - lr: 0.0010\n",
      "Epoch 233/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0731 - mse: 0.0314 - mae: 0.1370 - val_loss: 0.0732 - val_mse: 0.0307 - val_mae: 0.1359 - lr: 0.0010\n",
      "Epoch 234/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0729 - mse: 0.0314 - mae: 0.1370 - val_loss: 0.0734 - val_mse: 0.0307 - val_mae: 0.1355 - lr: 0.0010\n",
      "Epoch 235/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0727 - mse: 0.0313 - mae: 0.1367 - val_loss: 0.0733 - val_mse: 0.0307 - val_mae: 0.1357 - lr: 0.0010\n",
      "Epoch 236/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0726 - mse: 0.0312 - mae: 0.1365 - val_loss: 0.0733 - val_mse: 0.0307 - val_mae: 0.1357 - lr: 0.0010\n",
      "Epoch 237/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0723 - mse: 0.0311 - mae: 0.1361 - val_loss: 0.0725 - val_mse: 0.0304 - val_mae: 0.1353 - lr: 0.0010\n",
      "Epoch 238/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0721 - mse: 0.0310 - mae: 0.1361 - val_loss: 0.0726 - val_mse: 0.0304 - val_mae: 0.1351 - lr: 0.0010\n",
      "Epoch 239/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0718 - mse: 0.0309 - mae: 0.1358 - val_loss: 0.0723 - val_mse: 0.0303 - val_mae: 0.1343 - lr: 0.0010\n",
      "Epoch 240/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0717 - mse: 0.0308 - mae: 0.1355 - val_loss: 0.0720 - val_mse: 0.0302 - val_mae: 0.1342 - lr: 0.0010\n",
      "Epoch 241/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0715 - mse: 0.0308 - mae: 0.1354 - val_loss: 0.0718 - val_mse: 0.0301 - val_mae: 0.1340 - lr: 0.0010\n",
      "Epoch 242/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0712 - mse: 0.0306 - mae: 0.1350 - val_loss: 0.0714 - val_mse: 0.0299 - val_mae: 0.1337 - lr: 0.0010\n",
      "Epoch 243/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0711 - mse: 0.0306 - mae: 0.1350 - val_loss: 0.0711 - val_mse: 0.0298 - val_mae: 0.1333 - lr: 0.0010\n",
      "Epoch 244/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0708 - mse: 0.0305 - mae: 0.1349 - val_loss: 0.0713 - val_mse: 0.0299 - val_mae: 0.1339 - lr: 0.0010\n",
      "Epoch 245/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0706 - mse: 0.0304 - mae: 0.1345 - val_loss: 0.0707 - val_mse: 0.0296 - val_mae: 0.1332 - lr: 0.0010\n",
      "Epoch 246/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0704 - mse: 0.0303 - mae: 0.1344 - val_loss: 0.0708 - val_mse: 0.0297 - val_mae: 0.1333 - lr: 0.0010\n",
      "Epoch 247/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0701 - mse: 0.0302 - mae: 0.1340 - val_loss: 0.0703 - val_mse: 0.0295 - val_mae: 0.1330 - lr: 0.0010\n",
      "Epoch 248/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0700 - mse: 0.0301 - mae: 0.1339 - val_loss: 0.0700 - val_mse: 0.0293 - val_mae: 0.1322 - lr: 0.0010\n",
      "Epoch 249/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0698 - mse: 0.0301 - mae: 0.1337 - val_loss: 0.0698 - val_mse: 0.0292 - val_mae: 0.1320 - lr: 0.0010\n",
      "Epoch 250/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0695 - mse: 0.0299 - mae: 0.1335 - val_loss: 0.0695 - val_mse: 0.0291 - val_mae: 0.1317 - lr: 0.0010\n",
      "Epoch 251/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0693 - mse: 0.0299 - mae: 0.1332 - val_loss: 0.0694 - val_mse: 0.0291 - val_mae: 0.1319 - lr: 0.0010\n",
      "Epoch 252/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0690 - mse: 0.0297 - mae: 0.1329 - val_loss: 0.0690 - val_mse: 0.0289 - val_mae: 0.1312 - lr: 0.0010\n",
      "Epoch 253/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0688 - mse: 0.0296 - mae: 0.1327 - val_loss: 0.0689 - val_mse: 0.0288 - val_mae: 0.1311 - lr: 0.0010\n",
      "Epoch 254/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0685 - mse: 0.0295 - mae: 0.1324 - val_loss: 0.0685 - val_mse: 0.0287 - val_mae: 0.1310 - lr: 0.0010\n",
      "Epoch 255/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0683 - mse: 0.0294 - mae: 0.1322 - val_loss: 0.0683 - val_mse: 0.0286 - val_mae: 0.1307 - lr: 0.0010\n",
      "Epoch 256/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0681 - mse: 0.0293 - mae: 0.1321 - val_loss: 0.0683 - val_mse: 0.0286 - val_mae: 0.1309 - lr: 0.0010\n",
      "Epoch 257/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0678 - mse: 0.0292 - mae: 0.1318 - val_loss: 0.0680 - val_mse: 0.0285 - val_mae: 0.1305 - lr: 0.0010\n",
      "Epoch 258/500\n",
      "50/50 [==============================] - 1s 19ms/step - loss: 0.0676 - mse: 0.0291 - mae: 0.1314 - val_loss: 0.0676 - val_mse: 0.0284 - val_mae: 0.1304 - lr: 0.0010\n",
      "Epoch 259/500\n",
      "50/50 [==============================] - 1s 14ms/step - loss: 0.0674 - mse: 0.0290 - mae: 0.1315 - val_loss: 0.0676 - val_mse: 0.0283 - val_mae: 0.1298 - lr: 0.0010\n",
      "Epoch 260/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0671 - mse: 0.0288 - mae: 0.1308 - val_loss: 0.0670 - val_mse: 0.0280 - val_mae: 0.1292 - lr: 0.0010\n",
      "Epoch 261/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0668 - mse: 0.0288 - mae: 0.1309 - val_loss: 0.0668 - val_mse: 0.0280 - val_mae: 0.1293 - lr: 0.0010\n",
      "Epoch 262/500\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.0666 - mse: 0.0287 - mae: 0.1305 - val_loss: 0.0664 - val_mse: 0.0278 - val_mae: 0.1289 - lr: 0.0010\n",
      "Epoch 263/500\n",
      "50/50 [==============================] - 1s 18ms/step - loss: 0.0663 - mse: 0.0286 - mae: 0.1304 - val_loss: 0.0664 - val_mse: 0.0278 - val_mae: 0.1287 - lr: 0.0010\n",
      "Epoch 264/500\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0662 - mse: 0.0285 - mae: 0.1300 - val_loss: 0.0660 - val_mse: 0.0276 - val_mae: 0.1282 - lr: 0.0010\n",
      "Epoch 265/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0659 - mse: 0.0284 - mae: 0.1298 - val_loss: 0.0656 - val_mse: 0.0275 - val_mae: 0.1283 - lr: 0.0010\n",
      "Epoch 266/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0657 - mse: 0.0283 - mae: 0.1296 - val_loss: 0.0658 - val_mse: 0.0276 - val_mae: 0.1283 - lr: 0.0010\n",
      "Epoch 267/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0654 - mse: 0.0281 - mae: 0.1292 - val_loss: 0.0651 - val_mse: 0.0273 - val_mae: 0.1277 - lr: 0.0010\n",
      "Epoch 268/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0652 - mse: 0.0281 - mae: 0.1292 - val_loss: 0.0651 - val_mse: 0.0273 - val_mae: 0.1276 - lr: 0.0010\n",
      "Epoch 269/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0650 - mse: 0.0280 - mae: 0.1289 - val_loss: 0.0651 - val_mse: 0.0273 - val_mae: 0.1272 - lr: 0.0010\n",
      "Epoch 270/500\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.0647 - mse: 0.0279 - mae: 0.1286 - val_loss: 0.0648 - val_mse: 0.0272 - val_mae: 0.1272 - lr: 0.0010\n",
      "Epoch 271/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0646 - mse: 0.0278 - mae: 0.1284 - val_loss: 0.0644 - val_mse: 0.0269 - val_mae: 0.1265 - lr: 0.0010\n",
      "Epoch 272/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0643 - mse: 0.0277 - mae: 0.1281 - val_loss: 0.0643 - val_mse: 0.0270 - val_mae: 0.1268 - lr: 0.0010\n",
      "Epoch 273/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0641 - mse: 0.0275 - mae: 0.1279 - val_loss: 0.0637 - val_mse: 0.0267 - val_mae: 0.1261 - lr: 0.0010\n",
      "Epoch 274/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0639 - mse: 0.0275 - mae: 0.1276 - val_loss: 0.0635 - val_mse: 0.0266 - val_mae: 0.1258 - lr: 0.0010\n",
      "Epoch 275/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0637 - mse: 0.0274 - mae: 0.1274 - val_loss: 0.0637 - val_mse: 0.0267 - val_mae: 0.1259 - lr: 0.0010\n",
      "Epoch 276/500\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.0634 - mse: 0.0273 - mae: 0.1271 - val_loss: 0.0634 - val_mse: 0.0265 - val_mae: 0.1251 - lr: 0.0010\n",
      "Epoch 277/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0632 - mse: 0.0271 - mae: 0.1266 - val_loss: 0.0629 - val_mse: 0.0264 - val_mae: 0.1252 - lr: 0.0010\n",
      "Epoch 278/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0631 - mse: 0.0272 - mae: 0.1267 - val_loss: 0.0630 - val_mse: 0.0264 - val_mae: 0.1247 - lr: 0.0010\n",
      "Epoch 279/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0628 - mse: 0.0270 - mae: 0.1264 - val_loss: 0.0627 - val_mse: 0.0263 - val_mae: 0.1248 - lr: 0.0010\n",
      "Epoch 280/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0626 - mse: 0.0269 - mae: 0.1261 - val_loss: 0.0624 - val_mse: 0.0261 - val_mae: 0.1240 - lr: 0.0010\n",
      "Epoch 281/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0624 - mse: 0.0268 - mae: 0.1258 - val_loss: 0.0622 - val_mse: 0.0261 - val_mae: 0.1246 - lr: 0.0010\n",
      "Epoch 282/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0622 - mse: 0.0268 - mae: 0.1258 - val_loss: 0.0620 - val_mse: 0.0260 - val_mae: 0.1240 - lr: 0.0010\n",
      "Epoch 283/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0620 - mse: 0.0266 - mae: 0.1253 - val_loss: 0.0616 - val_mse: 0.0258 - val_mae: 0.1237 - lr: 0.0010\n",
      "Epoch 284/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0618 - mse: 0.0266 - mae: 0.1253 - val_loss: 0.0618 - val_mse: 0.0259 - val_mae: 0.1237 - lr: 0.0010\n",
      "Epoch 285/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0616 - mse: 0.0265 - mae: 0.1249 - val_loss: 0.0615 - val_mse: 0.0258 - val_mae: 0.1235 - lr: 0.0010\n",
      "Epoch 286/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0615 - mse: 0.0264 - mae: 0.1247 - val_loss: 0.0611 - val_mse: 0.0257 - val_mae: 0.1232 - lr: 0.0010\n",
      "Epoch 287/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0612 - mse: 0.0263 - mae: 0.1246 - val_loss: 0.0613 - val_mse: 0.0256 - val_mae: 0.1229 - lr: 0.0010\n",
      "Epoch 288/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0612 - mse: 0.0263 - mae: 0.1245 - val_loss: 0.0613 - val_mse: 0.0256 - val_mae: 0.1229 - lr: 0.0010\n",
      "Epoch 289/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0609 - mse: 0.0262 - mae: 0.1241 - val_loss: 0.0607 - val_mse: 0.0254 - val_mae: 0.1221 - lr: 0.0010\n",
      "Epoch 290/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0607 - mse: 0.0261 - mae: 0.1238 - val_loss: 0.0603 - val_mse: 0.0253 - val_mae: 0.1219 - lr: 0.0010\n",
      "Epoch 291/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0605 - mse: 0.0260 - mae: 0.1236 - val_loss: 0.0603 - val_mse: 0.0252 - val_mae: 0.1219 - lr: 0.0010\n",
      "Epoch 292/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0603 - mse: 0.0260 - mae: 0.1235 - val_loss: 0.0599 - val_mse: 0.0251 - val_mae: 0.1216 - lr: 0.0010\n",
      "Epoch 293/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0602 - mse: 0.0259 - mae: 0.1232 - val_loss: 0.0599 - val_mse: 0.0251 - val_mae: 0.1214 - lr: 0.0010\n",
      "Epoch 294/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0600 - mse: 0.0258 - mae: 0.1231 - val_loss: 0.0597 - val_mse: 0.0250 - val_mae: 0.1212 - lr: 0.0010\n",
      "Epoch 295/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0598 - mse: 0.0257 - mae: 0.1227 - val_loss: 0.0597 - val_mse: 0.0250 - val_mae: 0.1216 - lr: 0.0010\n",
      "Epoch 296/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0597 - mse: 0.0257 - mae: 0.1227 - val_loss: 0.0597 - val_mse: 0.0250 - val_mae: 0.1208 - lr: 0.0010\n",
      "Epoch 297/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0595 - mse: 0.0255 - mae: 0.1222 - val_loss: 0.0592 - val_mse: 0.0248 - val_mae: 0.1208 - lr: 0.0010\n",
      "Epoch 298/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0593 - mse: 0.0255 - mae: 0.1222 - val_loss: 0.0593 - val_mse: 0.0248 - val_mae: 0.1209 - lr: 0.0010\n",
      "Epoch 299/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0592 - mse: 0.0255 - mae: 0.1220 - val_loss: 0.0589 - val_mse: 0.0247 - val_mae: 0.1202 - lr: 0.0010\n",
      "Epoch 300/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0590 - mse: 0.0254 - mae: 0.1217 - val_loss: 0.0590 - val_mse: 0.0247 - val_mae: 0.1205 - lr: 0.0010\n",
      "Epoch 301/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0589 - mse: 0.0253 - mae: 0.1217 - val_loss: 0.0588 - val_mse: 0.0246 - val_mae: 0.1200 - lr: 0.0010\n",
      "Epoch 302/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0587 - mse: 0.0253 - mae: 0.1214 - val_loss: 0.0586 - val_mse: 0.0246 - val_mae: 0.1203 - lr: 0.0010\n",
      "Epoch 303/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0585 - mse: 0.0251 - mae: 0.1211 - val_loss: 0.0582 - val_mse: 0.0244 - val_mae: 0.1194 - lr: 0.0010\n",
      "Epoch 304/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0584 - mse: 0.0251 - mae: 0.1210 - val_loss: 0.0579 - val_mse: 0.0243 - val_mae: 0.1193 - lr: 0.0010\n",
      "Epoch 305/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0582 - mse: 0.0251 - mae: 0.1209 - val_loss: 0.0579 - val_mse: 0.0243 - val_mae: 0.1190 - lr: 0.0010\n",
      "Epoch 306/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0580 - mse: 0.0250 - mae: 0.1207 - val_loss: 0.0580 - val_mse: 0.0244 - val_mae: 0.1192 - lr: 0.0010\n",
      "Epoch 307/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0579 - mse: 0.0249 - mae: 0.1203 - val_loss: 0.0575 - val_mse: 0.0241 - val_mae: 0.1186 - lr: 0.0010\n",
      "Epoch 308/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0578 - mse: 0.0249 - mae: 0.1203 - val_loss: 0.0579 - val_mse: 0.0243 - val_mae: 0.1189 - lr: 0.0010\n",
      "Epoch 309/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0576 - mse: 0.0248 - mae: 0.1200 - val_loss: 0.0572 - val_mse: 0.0240 - val_mae: 0.1186 - lr: 0.0010\n",
      "Epoch 310/500\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0574 - mse: 0.0248 - mae: 0.1199 - val_loss: 0.0571 - val_mse: 0.0239 - val_mae: 0.1182 - lr: 0.0010\n",
      "Epoch 311/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0573 - mse: 0.0247 - mae: 0.1198 - val_loss: 0.0572 - val_mse: 0.0240 - val_mae: 0.1183 - lr: 0.0010\n",
      "Epoch 312/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0572 - mse: 0.0246 - mae: 0.1195 - val_loss: 0.0569 - val_mse: 0.0239 - val_mae: 0.1179 - lr: 0.0010\n",
      "Epoch 313/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0570 - mse: 0.0246 - mae: 0.1194 - val_loss: 0.0569 - val_mse: 0.0239 - val_mae: 0.1178 - lr: 0.0010\n",
      "Epoch 314/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0569 - mse: 0.0245 - mae: 0.1190 - val_loss: 0.0563 - val_mse: 0.0236 - val_mae: 0.1170 - lr: 0.0010\n",
      "Epoch 315/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0568 - mse: 0.0245 - mae: 0.1191 - val_loss: 0.0564 - val_mse: 0.0237 - val_mae: 0.1176 - lr: 0.0010\n",
      "Epoch 316/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0566 - mse: 0.0244 - mae: 0.1187 - val_loss: 0.0563 - val_mse: 0.0236 - val_mae: 0.1169 - lr: 0.0010\n",
      "Epoch 317/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0564 - mse: 0.0243 - mae: 0.1186 - val_loss: 0.0567 - val_mse: 0.0238 - val_mae: 0.1173 - lr: 0.0010\n",
      "Epoch 318/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0563 - mse: 0.0243 - mae: 0.1184 - val_loss: 0.0561 - val_mse: 0.0235 - val_mae: 0.1169 - lr: 0.0010\n",
      "Epoch 319/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0562 - mse: 0.0242 - mae: 0.1182 - val_loss: 0.0559 - val_mse: 0.0234 - val_mae: 0.1166 - lr: 0.0010\n",
      "Epoch 320/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0560 - mse: 0.0241 - mae: 0.1180 - val_loss: 0.0559 - val_mse: 0.0235 - val_mae: 0.1166 - lr: 0.0010\n",
      "Epoch 321/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0559 - mse: 0.0242 - mae: 0.1182 - val_loss: 0.0557 - val_mse: 0.0234 - val_mae: 0.1163 - lr: 0.0010\n",
      "Epoch 322/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0558 - mse: 0.0241 - mae: 0.1178 - val_loss: 0.0555 - val_mse: 0.0233 - val_mae: 0.1163 - lr: 0.0010\n",
      "Epoch 323/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0556 - mse: 0.0240 - mae: 0.1176 - val_loss: 0.0553 - val_mse: 0.0232 - val_mae: 0.1156 - lr: 0.0010\n",
      "Epoch 324/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0556 - mse: 0.0239 - mae: 0.1174 - val_loss: 0.0552 - val_mse: 0.0232 - val_mae: 0.1158 - lr: 0.0010\n",
      "Epoch 325/500\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0554 - mse: 0.0239 - mae: 0.1173 - val_loss: 0.0549 - val_mse: 0.0230 - val_mae: 0.1153 - lr: 0.0010\n",
      "Epoch 326/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0553 - mse: 0.0239 - mae: 0.1171 - val_loss: 0.0550 - val_mse: 0.0231 - val_mae: 0.1158 - lr: 0.0010\n",
      "Epoch 327/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0551 - mse: 0.0238 - mae: 0.1170 - val_loss: 0.0548 - val_mse: 0.0231 - val_mae: 0.1155 - lr: 0.0010\n",
      "Epoch 328/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0550 - mse: 0.0237 - mae: 0.1168 - val_loss: 0.0547 - val_mse: 0.0230 - val_mae: 0.1148 - lr: 0.0010\n",
      "Epoch 329/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0549 - mse: 0.0237 - mae: 0.1165 - val_loss: 0.0545 - val_mse: 0.0229 - val_mae: 0.1151 - lr: 0.0010\n",
      "Epoch 330/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0547 - mse: 0.0236 - mae: 0.1165 - val_loss: 0.0544 - val_mse: 0.0228 - val_mae: 0.1147 - lr: 0.0010\n",
      "Epoch 331/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0546 - mse: 0.0236 - mae: 0.1163 - val_loss: 0.0542 - val_mse: 0.0227 - val_mae: 0.1143 - lr: 0.0010\n",
      "Epoch 332/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0545 - mse: 0.0235 - mae: 0.1160 - val_loss: 0.0540 - val_mse: 0.0227 - val_mae: 0.1142 - lr: 0.0010\n",
      "Epoch 333/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0544 - mse: 0.0235 - mae: 0.1162 - val_loss: 0.0543 - val_mse: 0.0227 - val_mae: 0.1142 - lr: 0.0010\n",
      "Epoch 334/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0543 - mse: 0.0234 - mae: 0.1158 - val_loss: 0.0539 - val_mse: 0.0226 - val_mae: 0.1141 - lr: 0.0010\n",
      "Epoch 335/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0542 - mse: 0.0234 - mae: 0.1157 - val_loss: 0.0537 - val_mse: 0.0226 - val_mae: 0.1137 - lr: 0.0010\n",
      "Epoch 336/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0540 - mse: 0.0233 - mae: 0.1157 - val_loss: 0.0536 - val_mse: 0.0225 - val_mae: 0.1136 - lr: 0.0010\n",
      "Epoch 337/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0539 - mse: 0.0233 - mae: 0.1153 - val_loss: 0.0537 - val_mse: 0.0226 - val_mae: 0.1140 - lr: 0.0010\n",
      "Epoch 338/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0538 - mse: 0.0232 - mae: 0.1153 - val_loss: 0.0534 - val_mse: 0.0224 - val_mae: 0.1135 - lr: 0.0010\n",
      "Epoch 339/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0536 - mse: 0.0231 - mae: 0.1149 - val_loss: 0.0534 - val_mse: 0.0224 - val_mae: 0.1131 - lr: 0.0010\n",
      "Epoch 340/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0536 - mse: 0.0232 - mae: 0.1151 - val_loss: 0.0532 - val_mse: 0.0224 - val_mae: 0.1132 - lr: 0.0010\n",
      "Epoch 341/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0535 - mse: 0.0231 - mae: 0.1148 - val_loss: 0.0530 - val_mse: 0.0222 - val_mae: 0.1127 - lr: 0.0010\n",
      "Epoch 342/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0534 - mse: 0.0231 - mae: 0.1148 - val_loss: 0.0529 - val_mse: 0.0222 - val_mae: 0.1125 - lr: 0.0010\n",
      "Epoch 343/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0532 - mse: 0.0230 - mae: 0.1146 - val_loss: 0.0532 - val_mse: 0.0223 - val_mae: 0.1128 - lr: 0.0010\n",
      "Epoch 344/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0531 - mse: 0.0229 - mae: 0.1142 - val_loss: 0.0526 - val_mse: 0.0222 - val_mae: 0.1126 - lr: 0.0010\n",
      "Epoch 345/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0530 - mse: 0.0229 - mae: 0.1142 - val_loss: 0.0528 - val_mse: 0.0222 - val_mae: 0.1125 - lr: 0.0010\n",
      "Epoch 346/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0528 - mse: 0.0228 - mae: 0.1140 - val_loss: 0.0527 - val_mse: 0.0221 - val_mae: 0.1122 - lr: 0.0010\n",
      "Epoch 347/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0527 - mse: 0.0228 - mae: 0.1138 - val_loss: 0.0523 - val_mse: 0.0220 - val_mae: 0.1120 - lr: 0.0010\n",
      "Epoch 348/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0527 - mse: 0.0228 - mae: 0.1138 - val_loss: 0.0523 - val_mse: 0.0220 - val_mae: 0.1119 - lr: 0.0010\n",
      "Epoch 349/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0525 - mse: 0.0227 - mae: 0.1135 - val_loss: 0.0522 - val_mse: 0.0220 - val_mae: 0.1119 - lr: 0.0010\n",
      "Epoch 350/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0524 - mse: 0.0227 - mae: 0.1135 - val_loss: 0.0521 - val_mse: 0.0219 - val_mae: 0.1117 - lr: 0.0010\n",
      "Epoch 351/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0523 - mse: 0.0226 - mae: 0.1133 - val_loss: 0.0518 - val_mse: 0.0218 - val_mae: 0.1115 - lr: 0.0010\n",
      "Epoch 352/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0522 - mse: 0.0226 - mae: 0.1132 - val_loss: 0.0521 - val_mse: 0.0219 - val_mae: 0.1117 - lr: 0.0010\n",
      "Epoch 353/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0521 - mse: 0.0225 - mae: 0.1130 - val_loss: 0.0516 - val_mse: 0.0217 - val_mae: 0.1112 - lr: 0.0010\n",
      "Epoch 354/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0521 - mse: 0.0225 - mae: 0.1129 - val_loss: 0.0514 - val_mse: 0.0217 - val_mae: 0.1110 - lr: 0.0010\n",
      "Epoch 355/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0519 - mse: 0.0225 - mae: 0.1129 - val_loss: 0.0513 - val_mse: 0.0216 - val_mae: 0.1108 - lr: 0.0010\n",
      "Epoch 356/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0518 - mse: 0.0224 - mae: 0.1127 - val_loss: 0.0515 - val_mse: 0.0217 - val_mae: 0.1109 - lr: 0.0010\n",
      "Epoch 357/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0517 - mse: 0.0224 - mae: 0.1125 - val_loss: 0.0516 - val_mse: 0.0217 - val_mae: 0.1108 - lr: 0.0010\n",
      "Epoch 358/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0516 - mse: 0.0223 - mae: 0.1122 - val_loss: 0.0512 - val_mse: 0.0215 - val_mae: 0.1105 - lr: 0.0010\n",
      "Epoch 359/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0515 - mse: 0.0223 - mae: 0.1123 - val_loss: 0.0513 - val_mse: 0.0216 - val_mae: 0.1105 - lr: 0.0010\n",
      "Epoch 360/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0514 - mse: 0.0222 - mae: 0.1121 - val_loss: 0.0510 - val_mse: 0.0215 - val_mae: 0.1103 - lr: 0.0010\n",
      "Epoch 361/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0513 - mse: 0.0222 - mae: 0.1119 - val_loss: 0.0510 - val_mse: 0.0215 - val_mae: 0.1103 - lr: 0.0010\n",
      "Epoch 362/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0512 - mse: 0.0222 - mae: 0.1119 - val_loss: 0.0509 - val_mse: 0.0215 - val_mae: 0.1104 - lr: 0.0010\n",
      "Epoch 363/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0511 - mse: 0.0221 - mae: 0.1117 - val_loss: 0.0507 - val_mse: 0.0214 - val_mae: 0.1104 - lr: 0.0010\n",
      "Epoch 364/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0509 - mse: 0.0221 - mae: 0.1116 - val_loss: 0.0504 - val_mse: 0.0212 - val_mae: 0.1096 - lr: 0.0010\n",
      "Epoch 365/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0509 - mse: 0.0220 - mae: 0.1115 - val_loss: 0.0505 - val_mse: 0.0213 - val_mae: 0.1100 - lr: 0.0010\n",
      "Epoch 366/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0508 - mse: 0.0220 - mae: 0.1112 - val_loss: 0.0503 - val_mse: 0.0212 - val_mae: 0.1096 - lr: 0.0010\n",
      "Epoch 367/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0506 - mse: 0.0220 - mae: 0.1114 - val_loss: 0.0504 - val_mse: 0.0212 - val_mae: 0.1091 - lr: 0.0010\n",
      "Epoch 368/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0505 - mse: 0.0219 - mae: 0.1110 - val_loss: 0.0502 - val_mse: 0.0212 - val_mae: 0.1097 - lr: 0.0010\n",
      "Epoch 369/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0505 - mse: 0.0218 - mae: 0.1109 - val_loss: 0.0501 - val_mse: 0.0210 - val_mae: 0.1088 - lr: 0.0010\n",
      "Epoch 370/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0504 - mse: 0.0218 - mae: 0.1109 - val_loss: 0.0498 - val_mse: 0.0210 - val_mae: 0.1089 - lr: 0.0010\n",
      "Epoch 371/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0502 - mse: 0.0218 - mae: 0.1108 - val_loss: 0.0501 - val_mse: 0.0210 - val_mae: 0.1088 - lr: 0.0010\n",
      "Epoch 372/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0502 - mse: 0.0217 - mae: 0.1106 - val_loss: 0.0501 - val_mse: 0.0211 - val_mae: 0.1090 - lr: 0.0010\n",
      "Epoch 373/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0501 - mse: 0.0217 - mae: 0.1105 - val_loss: 0.0499 - val_mse: 0.0210 - val_mae: 0.1090 - lr: 0.0010\n",
      "Epoch 374/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0500 - mse: 0.0216 - mae: 0.1102 - val_loss: 0.0496 - val_mse: 0.0209 - val_mae: 0.1089 - lr: 0.0010\n",
      "Epoch 375/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0499 - mse: 0.0216 - mae: 0.1103 - val_loss: 0.0496 - val_mse: 0.0209 - val_mae: 0.1084 - lr: 0.0010\n",
      "Epoch 376/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0498 - mse: 0.0215 - mae: 0.1099 - val_loss: 0.0491 - val_mse: 0.0208 - val_mae: 0.1085 - lr: 0.0010\n",
      "Epoch 377/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0497 - mse: 0.0216 - mae: 0.1101 - val_loss: 0.0492 - val_mse: 0.0208 - val_mae: 0.1084 - lr: 0.0010\n",
      "Epoch 378/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0496 - mse: 0.0215 - mae: 0.1099 - val_loss: 0.0492 - val_mse: 0.0207 - val_mae: 0.1081 - lr: 0.0010\n",
      "Epoch 379/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0495 - mse: 0.0214 - mae: 0.1097 - val_loss: 0.0488 - val_mse: 0.0206 - val_mae: 0.1080 - lr: 0.0010\n",
      "Epoch 380/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0494 - mse: 0.0215 - mae: 0.1098 - val_loss: 0.0489 - val_mse: 0.0207 - val_mae: 0.1082 - lr: 0.0010\n",
      "Epoch 381/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0493 - mse: 0.0214 - mae: 0.1095 - val_loss: 0.0490 - val_mse: 0.0206 - val_mae: 0.1078 - lr: 0.0010\n",
      "Epoch 382/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0492 - mse: 0.0213 - mae: 0.1094 - val_loss: 0.0488 - val_mse: 0.0206 - val_mae: 0.1078 - lr: 0.0010\n",
      "Epoch 383/500\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0490 - mse: 0.0213 - mae: 0.1092 - val_loss: 0.0486 - val_mse: 0.0205 - val_mae: 0.1075 - lr: 0.0010\n",
      "Epoch 384/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0490 - mse: 0.0213 - mae: 0.1093 - val_loss: 0.0486 - val_mse: 0.0205 - val_mae: 0.1074 - lr: 0.0010\n",
      "Epoch 385/500\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0489 - mse: 0.0212 - mae: 0.1089 - val_loss: 0.0482 - val_mse: 0.0204 - val_mae: 0.1072 - lr: 0.0010\n",
      "Epoch 386/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0488 - mse: 0.0212 - mae: 0.1091 - val_loss: 0.0484 - val_mse: 0.0205 - val_mae: 0.1076 - lr: 0.0010\n",
      "Epoch 387/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0487 - mse: 0.0211 - mae: 0.1089 - val_loss: 0.0484 - val_mse: 0.0204 - val_mae: 0.1070 - lr: 0.0010\n",
      "Epoch 388/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0486 - mse: 0.0211 - mae: 0.1086 - val_loss: 0.0481 - val_mse: 0.0203 - val_mae: 0.1074 - lr: 0.0010\n",
      "Epoch 389/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0486 - mse: 0.0211 - mae: 0.1087 - val_loss: 0.0480 - val_mse: 0.0202 - val_mae: 0.1068 - lr: 0.0010\n",
      "Epoch 390/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0484 - mse: 0.0210 - mae: 0.1085 - val_loss: 0.0479 - val_mse: 0.0202 - val_mae: 0.1068 - lr: 0.0010\n",
      "Epoch 391/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0482 - mse: 0.0209 - mae: 0.1083 - val_loss: 0.0477 - val_mse: 0.0202 - val_mae: 0.1068 - lr: 0.0010\n",
      "Epoch 392/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0482 - mse: 0.0209 - mae: 0.1083 - val_loss: 0.0478 - val_mse: 0.0201 - val_mae: 0.1065 - lr: 0.0010\n",
      "Epoch 393/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0481 - mse: 0.0209 - mae: 0.1081 - val_loss: 0.0475 - val_mse: 0.0200 - val_mae: 0.1062 - lr: 0.0010\n",
      "Epoch 394/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0480 - mse: 0.0208 - mae: 0.1081 - val_loss: 0.0475 - val_mse: 0.0201 - val_mae: 0.1064 - lr: 0.0010\n",
      "Epoch 395/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0479 - mse: 0.0208 - mae: 0.1080 - val_loss: 0.0473 - val_mse: 0.0200 - val_mae: 0.1061 - lr: 0.0010\n",
      "Epoch 396/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0477 - mse: 0.0208 - mae: 0.1079 - val_loss: 0.0480 - val_mse: 0.0202 - val_mae: 0.1070 - lr: 0.0010\n",
      "Epoch 397/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0477 - mse: 0.0207 - mae: 0.1077 - val_loss: 0.0472 - val_mse: 0.0199 - val_mae: 0.1061 - lr: 0.0010\n",
      "Epoch 398/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0476 - mse: 0.0207 - mae: 0.1077 - val_loss: 0.0474 - val_mse: 0.0199 - val_mae: 0.1060 - lr: 0.0010\n",
      "Epoch 399/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0475 - mse: 0.0206 - mae: 0.1074 - val_loss: 0.0470 - val_mse: 0.0199 - val_mae: 0.1059 - lr: 0.0010\n",
      "Epoch 400/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0473 - mse: 0.0206 - mae: 0.1073 - val_loss: 0.0471 - val_mse: 0.0199 - val_mae: 0.1059 - lr: 0.0010\n",
      "Epoch 401/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0474 - mse: 0.0206 - mae: 0.1075 - val_loss: 0.0468 - val_mse: 0.0198 - val_mae: 0.1059 - lr: 0.0010\n",
      "Epoch 402/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0472 - mse: 0.0205 - mae: 0.1072 - val_loss: 0.0468 - val_mse: 0.0197 - val_mae: 0.1054 - lr: 0.0010\n",
      "Epoch 403/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0470 - mse: 0.0204 - mae: 0.1070 - val_loss: 0.0464 - val_mse: 0.0197 - val_mae: 0.1055 - lr: 0.0010\n",
      "Epoch 404/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0469 - mse: 0.0204 - mae: 0.1071 - val_loss: 0.0467 - val_mse: 0.0197 - val_mae: 0.1056 - lr: 0.0010\n",
      "Epoch 405/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0469 - mse: 0.0203 - mae: 0.1067 - val_loss: 0.0461 - val_mse: 0.0195 - val_mae: 0.1052 - lr: 0.0010\n",
      "Epoch 406/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0467 - mse: 0.0203 - mae: 0.1069 - val_loss: 0.0463 - val_mse: 0.0196 - val_mae: 0.1053 - lr: 0.0010\n",
      "Epoch 407/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0466 - mse: 0.0203 - mae: 0.1066 - val_loss: 0.0462 - val_mse: 0.0195 - val_mae: 0.1051 - lr: 0.0010\n",
      "Epoch 408/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0465 - mse: 0.0203 - mae: 0.1067 - val_loss: 0.0463 - val_mse: 0.0195 - val_mae: 0.1049 - lr: 0.0010\n",
      "Epoch 409/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0464 - mse: 0.0202 - mae: 0.1064 - val_loss: 0.0461 - val_mse: 0.0195 - val_mae: 0.1050 - lr: 0.0010\n",
      "Epoch 410/500\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.0463 - mse: 0.0201 - mae: 0.1062\n",
      "Epoch 410: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0463 - mse: 0.0202 - mae: 0.1063 - val_loss: 0.0461 - val_mse: 0.0194 - val_mae: 0.1048 - lr: 0.0010\n",
      "Epoch 411/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0461 - mse: 0.0201 - mae: 0.1061 - val_loss: 0.0457 - val_mse: 0.0193 - val_mae: 0.1047 - lr: 5.0000e-04\n",
      "Epoch 412/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0461 - mse: 0.0200 - mae: 0.1061 - val_loss: 0.0456 - val_mse: 0.0193 - val_mae: 0.1044 - lr: 5.0000e-04\n",
      "Epoch 413/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0460 - mse: 0.0200 - mae: 0.1061 - val_loss: 0.0457 - val_mse: 0.0193 - val_mae: 0.1045 - lr: 5.0000e-04\n",
      "Epoch 414/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0459 - mse: 0.0200 - mae: 0.1059 - val_loss: 0.0457 - val_mse: 0.0193 - val_mae: 0.1046 - lr: 5.0000e-04\n",
      "Epoch 415/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0459 - mse: 0.0200 - mae: 0.1060 - val_loss: 0.0456 - val_mse: 0.0193 - val_mae: 0.1046 - lr: 5.0000e-04\n",
      "Epoch 416/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0459 - mse: 0.0200 - mae: 0.1058 - val_loss: 0.0455 - val_mse: 0.0193 - val_mae: 0.1046 - lr: 5.0000e-04\n",
      "Epoch 417/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0458 - mse: 0.0199 - mae: 0.1059 - val_loss: 0.0455 - val_mse: 0.0192 - val_mae: 0.1044 - lr: 5.0000e-04\n",
      "Epoch 418/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0457 - mse: 0.0199 - mae: 0.1058 - val_loss: 0.0455 - val_mse: 0.0192 - val_mae: 0.1044 - lr: 5.0000e-04\n",
      "Epoch 419/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0457 - mse: 0.0199 - mae: 0.1057 - val_loss: 0.0455 - val_mse: 0.0193 - val_mae: 0.1045 - lr: 5.0000e-04\n",
      "Epoch 420/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0457 - mse: 0.0199 - mae: 0.1057 - val_loss: 0.0453 - val_mse: 0.0191 - val_mae: 0.1040 - lr: 5.0000e-04\n",
      "Epoch 421/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0456 - mse: 0.0199 - mae: 0.1056 - val_loss: 0.0452 - val_mse: 0.0191 - val_mae: 0.1042 - lr: 5.0000e-04\n",
      "Epoch 422/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0456 - mse: 0.0199 - mae: 0.1056 - val_loss: 0.0453 - val_mse: 0.0192 - val_mae: 0.1045 - lr: 5.0000e-04\n",
      "Epoch 423/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0455 - mse: 0.0198 - mae: 0.1055 - val_loss: 0.0451 - val_mse: 0.0191 - val_mae: 0.1040 - lr: 5.0000e-04\n",
      "Epoch 424/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0454 - mse: 0.0197 - mae: 0.1053 - val_loss: 0.0451 - val_mse: 0.0190 - val_mae: 0.1039 - lr: 5.0000e-04\n",
      "Epoch 425/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0454 - mse: 0.0198 - mae: 0.1055 - val_loss: 0.0450 - val_mse: 0.0190 - val_mae: 0.1037 - lr: 5.0000e-04\n",
      "Epoch 426/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0453 - mse: 0.0197 - mae: 0.1054 - val_loss: 0.0449 - val_mse: 0.0190 - val_mae: 0.1036 - lr: 5.0000e-04\n",
      "Epoch 427/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0453 - mse: 0.0197 - mae: 0.1053 - val_loss: 0.0449 - val_mse: 0.0190 - val_mae: 0.1039 - lr: 5.0000e-04\n",
      "Epoch 428/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0452 - mse: 0.0197 - mae: 0.1052 - val_loss: 0.0449 - val_mse: 0.0190 - val_mae: 0.1038 - lr: 5.0000e-04\n",
      "Epoch 429/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0452 - mse: 0.0197 - mae: 0.1052 - val_loss: 0.0449 - val_mse: 0.0190 - val_mae: 0.1039 - lr: 5.0000e-04\n",
      "Epoch 430/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0451 - mse: 0.0196 - mae: 0.1051 - val_loss: 0.0447 - val_mse: 0.0189 - val_mae: 0.1038 - lr: 5.0000e-04\n",
      "Epoch 431/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0450 - mse: 0.0196 - mae: 0.1050 - val_loss: 0.0447 - val_mse: 0.0189 - val_mae: 0.1036 - lr: 5.0000e-04\n",
      "Epoch 432/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0450 - mse: 0.0196 - mae: 0.1051 - val_loss: 0.0446 - val_mse: 0.0188 - val_mae: 0.1033 - lr: 5.0000e-04\n",
      "Epoch 433/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0449 - mse: 0.0196 - mae: 0.1049 - val_loss: 0.0447 - val_mse: 0.0189 - val_mae: 0.1037 - lr: 5.0000e-04\n",
      "Epoch 434/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0449 - mse: 0.0195 - mae: 0.1049 - val_loss: 0.0446 - val_mse: 0.0189 - val_mae: 0.1036 - lr: 5.0000e-04\n",
      "Epoch 435/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0448 - mse: 0.0195 - mae: 0.1048 - val_loss: 0.0444 - val_mse: 0.0188 - val_mae: 0.1032 - lr: 5.0000e-04\n",
      "Epoch 436/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0447 - mse: 0.0195 - mae: 0.1047 - val_loss: 0.0444 - val_mse: 0.0188 - val_mae: 0.1033 - lr: 5.0000e-04\n",
      "Epoch 437/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0447 - mse: 0.0195 - mae: 0.1048 - val_loss: 0.0444 - val_mse: 0.0188 - val_mae: 0.1034 - lr: 5.0000e-04\n",
      "Epoch 438/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0446 - mse: 0.0195 - mae: 0.1047 - val_loss: 0.0444 - val_mse: 0.0188 - val_mae: 0.1033 - lr: 5.0000e-04\n",
      "Epoch 439/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0446 - mse: 0.0194 - mae: 0.1045 - val_loss: 0.0442 - val_mse: 0.0187 - val_mae: 0.1033 - lr: 5.0000e-04\n",
      "Epoch 440/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0445 - mse: 0.0194 - mae: 0.1046 - val_loss: 0.0442 - val_mse: 0.0187 - val_mae: 0.1030 - lr: 5.0000e-04\n",
      "Epoch 441/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0445 - mse: 0.0194 - mae: 0.1044 - val_loss: 0.0441 - val_mse: 0.0187 - val_mae: 0.1029 - lr: 5.0000e-04\n",
      "Epoch 442/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0444 - mse: 0.0194 - mae: 0.1044 - val_loss: 0.0441 - val_mse: 0.0187 - val_mae: 0.1033 - lr: 5.0000e-04\n",
      "Epoch 443/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0443 - mse: 0.0193 - mae: 0.1044 - val_loss: 0.0439 - val_mse: 0.0186 - val_mae: 0.1031 - lr: 5.0000e-04\n",
      "Epoch 444/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0443 - mse: 0.0193 - mae: 0.1043 - val_loss: 0.0441 - val_mse: 0.0187 - val_mae: 0.1032 - lr: 5.0000e-04\n",
      "Epoch 445/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0442 - mse: 0.0193 - mae: 0.1044 - val_loss: 0.0440 - val_mse: 0.0186 - val_mae: 0.1031 - lr: 5.0000e-04\n",
      "Epoch 446/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0442 - mse: 0.0193 - mae: 0.1042 - val_loss: 0.0438 - val_mse: 0.0185 - val_mae: 0.1026 - lr: 5.0000e-04\n",
      "Epoch 447/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0441 - mse: 0.0192 - mae: 0.1043 - val_loss: 0.0438 - val_mse: 0.0185 - val_mae: 0.1027 - lr: 5.0000e-04\n",
      "Epoch 448/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0441 - mse: 0.0192 - mae: 0.1040 - val_loss: 0.0437 - val_mse: 0.0185 - val_mae: 0.1027 - lr: 5.0000e-04\n",
      "Epoch 449/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0440 - mse: 0.0192 - mae: 0.1041 - val_loss: 0.0437 - val_mse: 0.0185 - val_mae: 0.1027 - lr: 5.0000e-04\n",
      "Epoch 450/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0439 - mse: 0.0192 - mae: 0.1040 - val_loss: 0.0435 - val_mse: 0.0184 - val_mae: 0.1023 - lr: 5.0000e-04\n",
      "Epoch 451/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0439 - mse: 0.0192 - mae: 0.1040 - val_loss: 0.0435 - val_mse: 0.0184 - val_mae: 0.1025 - lr: 5.0000e-04\n",
      "Epoch 452/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0438 - mse: 0.0191 - mae: 0.1039 - val_loss: 0.0436 - val_mse: 0.0185 - val_mae: 0.1026 - lr: 5.0000e-04\n",
      "Epoch 453/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0438 - mse: 0.0191 - mae: 0.1038 - val_loss: 0.0434 - val_mse: 0.0184 - val_mae: 0.1025 - lr: 5.0000e-04\n",
      "Epoch 454/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0437 - mse: 0.0191 - mae: 0.1038 - val_loss: 0.0434 - val_mse: 0.0184 - val_mae: 0.1024 - lr: 5.0000e-04\n",
      "Epoch 455/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0436 - mse: 0.0190 - mae: 0.1038 - val_loss: 0.0433 - val_mse: 0.0183 - val_mae: 0.1023 - lr: 5.0000e-04\n",
      "Epoch 456/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0436 - mse: 0.0190 - mae: 0.1036 - val_loss: 0.0433 - val_mse: 0.0184 - val_mae: 0.1026 - lr: 5.0000e-04\n",
      "Epoch 457/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0435 - mse: 0.0190 - mae: 0.1036 - val_loss: 0.0433 - val_mse: 0.0183 - val_mae: 0.1023 - lr: 5.0000e-04\n",
      "Epoch 458/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0435 - mse: 0.0190 - mae: 0.1036 - val_loss: 0.0432 - val_mse: 0.0183 - val_mae: 0.1020 - lr: 5.0000e-04\n",
      "Epoch 459/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0434 - mse: 0.0189 - mae: 0.1034 - val_loss: 0.0431 - val_mse: 0.0182 - val_mae: 0.1021 - lr: 5.0000e-04\n",
      "Epoch 460/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0433 - mse: 0.0189 - mae: 0.1036 - val_loss: 0.0431 - val_mse: 0.0182 - val_mae: 0.1020 - lr: 5.0000e-04\n",
      "Epoch 461/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0433 - mse: 0.0189 - mae: 0.1033 - val_loss: 0.0432 - val_mse: 0.0183 - val_mae: 0.1023 - lr: 5.0000e-04\n",
      "Epoch 462/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0432 - mse: 0.0189 - mae: 0.1033 - val_loss: 0.0429 - val_mse: 0.0182 - val_mae: 0.1018 - lr: 5.0000e-04\n",
      "Epoch 463/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0432 - mse: 0.0188 - mae: 0.1033 - val_loss: 0.0428 - val_mse: 0.0181 - val_mae: 0.1016 - lr: 5.0000e-04\n",
      "Epoch 464/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0431 - mse: 0.0188 - mae: 0.1032 - val_loss: 0.0429 - val_mse: 0.0181 - val_mae: 0.1017 - lr: 5.0000e-04\n",
      "Epoch 465/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0430 - mse: 0.0188 - mae: 0.1031 - val_loss: 0.0427 - val_mse: 0.0181 - val_mae: 0.1016 - lr: 5.0000e-04\n",
      "Epoch 466/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0430 - mse: 0.0187 - mae: 0.1031 - val_loss: 0.0425 - val_mse: 0.0180 - val_mae: 0.1015 - lr: 5.0000e-04\n",
      "Epoch 467/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0429 - mse: 0.0187 - mae: 0.1031 - val_loss: 0.0427 - val_mse: 0.0181 - val_mae: 0.1016 - lr: 5.0000e-04\n",
      "Epoch 468/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0428 - mse: 0.0187 - mae: 0.1030 - val_loss: 0.0426 - val_mse: 0.0180 - val_mae: 0.1017 - lr: 5.0000e-04\n",
      "Epoch 469/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0428 - mse: 0.0187 - mae: 0.1029 - val_loss: 0.0425 - val_mse: 0.0180 - val_mae: 0.1018 - lr: 5.0000e-04\n",
      "Epoch 470/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0427 - mse: 0.0186 - mae: 0.1029 - val_loss: 0.0424 - val_mse: 0.0180 - val_mae: 0.1014 - lr: 5.0000e-04\n",
      "Epoch 471/500\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0423 - mse: 0.0185 - mae: 0.1026\n",
      "Epoch 471: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0426 - mse: 0.0186 - mae: 0.1028 - val_loss: 0.0425 - val_mse: 0.0180 - val_mae: 0.1015 - lr: 5.0000e-04\n",
      "Epoch 472/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0425 - mse: 0.0186 - mae: 0.1027 - val_loss: 0.0423 - val_mse: 0.0179 - val_mae: 0.1014 - lr: 2.5000e-04\n",
      "Epoch 473/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0425 - mse: 0.0185 - mae: 0.1027 - val_loss: 0.0423 - val_mse: 0.0179 - val_mae: 0.1014 - lr: 2.5000e-04\n",
      "Epoch 474/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0425 - mse: 0.0186 - mae: 0.1027 - val_loss: 0.0422 - val_mse: 0.0179 - val_mae: 0.1014 - lr: 2.5000e-04\n",
      "Epoch 475/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0424 - mse: 0.0185 - mae: 0.1026 - val_loss: 0.0422 - val_mse: 0.0179 - val_mae: 0.1012 - lr: 2.5000e-04\n",
      "Epoch 476/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0424 - mse: 0.0185 - mae: 0.1025 - val_loss: 0.0421 - val_mse: 0.0179 - val_mae: 0.1012 - lr: 2.5000e-04\n",
      "Epoch 477/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0423 - mse: 0.0185 - mae: 0.1026 - val_loss: 0.0421 - val_mse: 0.0178 - val_mae: 0.1012 - lr: 2.5000e-04\n",
      "Epoch 478/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0423 - mse: 0.0184 - mae: 0.1024 - val_loss: 0.0421 - val_mse: 0.0178 - val_mae: 0.1012 - lr: 2.5000e-04\n",
      "Epoch 479/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0423 - mse: 0.0185 - mae: 0.1026 - val_loss: 0.0421 - val_mse: 0.0178 - val_mae: 0.1012 - lr: 2.5000e-04\n",
      "Epoch 480/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0423 - mse: 0.0185 - mae: 0.1025 - val_loss: 0.0421 - val_mse: 0.0178 - val_mae: 0.1012 - lr: 2.5000e-04\n",
      "Epoch 481/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0422 - mse: 0.0184 - mae: 0.1024 - val_loss: 0.0420 - val_mse: 0.0178 - val_mae: 0.1011 - lr: 2.5000e-04\n",
      "Epoch 482/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0422 - mse: 0.0184 - mae: 0.1024 - val_loss: 0.0420 - val_mse: 0.0178 - val_mae: 0.1010 - lr: 2.5000e-04\n",
      "Epoch 483/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0421 - mse: 0.0184 - mae: 0.1024 - val_loss: 0.0419 - val_mse: 0.0177 - val_mae: 0.1009 - lr: 2.5000e-04\n",
      "Epoch 484/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0421 - mse: 0.0184 - mae: 0.1023 - val_loss: 0.0419 - val_mse: 0.0178 - val_mae: 0.1010 - lr: 2.5000e-04\n",
      "Epoch 485/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0421 - mse: 0.0184 - mae: 0.1023 - val_loss: 0.0418 - val_mse: 0.0177 - val_mae: 0.1010 - lr: 2.5000e-04\n",
      "Epoch 486/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0420 - mse: 0.0184 - mae: 0.1023 - val_loss: 0.0418 - val_mse: 0.0177 - val_mae: 0.1010 - lr: 2.5000e-04\n",
      "Epoch 487/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0420 - mse: 0.0183 - mae: 0.1023 - val_loss: 0.0417 - val_mse: 0.0177 - val_mae: 0.1008 - lr: 2.5000e-04\n",
      "Epoch 488/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0419 - mse: 0.0183 - mae: 0.1022 - val_loss: 0.0418 - val_mse: 0.0177 - val_mae: 0.1009 - lr: 2.5000e-04\n",
      "Epoch 489/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0419 - mse: 0.0183 - mae: 0.1022 - val_loss: 0.0418 - val_mse: 0.0177 - val_mae: 0.1010 - lr: 2.5000e-04\n",
      "Epoch 490/500\n",
      "40/50 [=======================>......] - ETA: 0s - loss: 0.0416 - mse: 0.0182 - mae: 0.1023\n",
      "Epoch 490: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0419 - mse: 0.0183 - mae: 0.1022 - val_loss: 0.0417 - val_mse: 0.0177 - val_mae: 0.1009 - lr: 2.5000e-04\n",
      "Epoch 491/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0418 - mse: 0.0183 - mae: 0.1020 - val_loss: 0.0416 - val_mse: 0.0176 - val_mae: 0.1008 - lr: 1.2500e-04\n",
      "Epoch 492/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0418 - mse: 0.0183 - mae: 0.1021 - val_loss: 0.0416 - val_mse: 0.0176 - val_mae: 0.1008 - lr: 1.2500e-04\n",
      "Epoch 493/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0418 - mse: 0.0182 - mae: 0.1021 - val_loss: 0.0417 - val_mse: 0.0176 - val_mae: 0.1007 - lr: 1.2500e-04\n",
      "Epoch 494/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0417 - mse: 0.0182 - mae: 0.1020 - val_loss: 0.0416 - val_mse: 0.0176 - val_mae: 0.1007 - lr: 1.2500e-04\n",
      "Epoch 495/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0417 - mse: 0.0182 - mae: 0.1021 - val_loss: 0.0416 - val_mse: 0.0176 - val_mae: 0.1007 - lr: 1.2500e-04\n",
      "Epoch 496/500\n",
      "32/50 [==================>...........] - ETA: 0s - loss: 0.0413 - mse: 0.0181 - mae: 0.1017\n",
      "Epoch 496: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0417 - mse: 0.0182 - mae: 0.1020 - val_loss: 0.0416 - val_mse: 0.0176 - val_mae: 0.1007 - lr: 1.2500e-04\n",
      "Epoch 497/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0417 - mse: 0.0182 - mae: 0.1020 - val_loss: 0.0415 - val_mse: 0.0176 - val_mae: 0.1007 - lr: 6.2500e-05\n",
      "Epoch 498/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0417 - mse: 0.0182 - mae: 0.1020 - val_loss: 0.0415 - val_mse: 0.0176 - val_mae: 0.1007 - lr: 6.2500e-05\n",
      "Epoch 499/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0417 - mse: 0.0182 - mae: 0.1020 - val_loss: 0.0415 - val_mse: 0.0176 - val_mae: 0.1007 - lr: 6.2500e-05\n",
      "Epoch 500/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0416 - mse: 0.0182 - mae: 0.1020 - val_loss: 0.0415 - val_mse: 0.0176 - val_mae: 0.1007 - lr: 6.2500e-05\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [X_real_train, X_imag_train],\n",
    "    y_train,\n",
    "    epochs=500,\n",
    "    batch_size=16,\n",
    "    validation_data=([X_real_test, X_imag_test], y_test),\n",
    "    callbacks=[adjust_lr, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0415 - mse: 0.0176 - mae: 0.1007\n",
      "Test MSE: 0.0176, Test MAE: 0.1007\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_mse, test_mae = model.evaluate([X_real_test, X_imag_test], y_test)\n",
    "print(f\"Test MSE: {test_mse:.4f}, Test MAE: {test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAGwCAYAAABB4NqyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABgfUlEQVR4nO3deXwU9f3H8dfsbrK57xNJQrjvK1wBObw4qhbU/kBrQStWUbAitVXEA20t2npQq1DpIdpWwFtbUQj1AAQvIICCnIFwJIRw5E422Z3fHwurMYCQhEyO9/PxmAfZ78x89zMjmrff+c6MYZqmiYiIiEgLYrO6ABEREZGGpgAkIiIiLY4CkIiIiLQ4CkAiIiLS4igAiYiISIujACQiIiItjgKQiIiItDgOqwtojDweDwcPHiQ0NBTDMKwuR0RERM6CaZoUFRXRqlUrbLYzj/EoAJ3CwYMHSUpKsroMERERqYV9+/bRunXrM26jAHQKoaGhgPcEhoWFWVyNiIiInI3CwkKSkpJ8v8fPRAHoFE5e9goLC1MAEhERaWLOZvqKJkGLiIhIi6MAJCIiIi2OApCIiIi0OJoDJCIi54Xb7aaystLqMqSZ8ff3/8Fb3M+GApCIiNQr0zTJzc3l+PHjVpcizZDNZiM1NRV/f/869aMAJCIi9epk+ImLiyMoKEgPlJV6c/JBxTk5OSQnJ9fp75YCkIiI1Bu32+0LP9HR0VaXI81QbGwsBw8epKqqCj8/v1r3o0nQIiJSb07O+QkKCrK4EmmuTl76crvddepHAUhEROqdLnvJ+VJff7csD0Dz5s0jNTWVgIAA0tLSWLVq1Vnt98knn+BwOOjdu3eNda+//jpdu3bF6XTStWtX3nzzzXquWkRERJoySwPQkiVLmD59OrNmzWLDhg0MHTqUMWPGkJ2dfcb9CgoKmDRpEpdcckmNdWvXrmXChAlMnDiRjRs3MnHiRMaPH89nn312vg5DREREmhjDNE3Tqi8fOHAgffv2Zf78+b62Ll26MG7cOObMmXPa/a699lo6dOiA3W7nrbfeIjMz07duwoQJFBYW8t577/naRo8eTWRkJIsWLTplfxUVFVRUVPg+n3yZWkFBgd4FJiJyDsrLy8nKyvKN7LdkI0aMoHfv3sydO/estt+zZw+pqals2LDhlFc3xOtMf8cKCwsJDw8/q9/flo0AuVwu1q1bx8iRI6u1jxw5kjVr1px2vxdeeIFdu3bx0EMPnXL92rVra/Q5atSoM/Y5Z84cwsPDfUtSUtI5HMnZc1V5OHi8jAPHy85L/yIicu4MwzjjcuONN9aq3zfeeIPf/va3Z719UlISOTk5dO/evVbfd7b27NmDYRjVBg9aIstug8/Pz8ftdhMfH1+tPT4+ntzc3FPus2PHDu69915WrVqFw3Hq0nNzc8+pT4CZM2cyY8YM3+eTI0D1beP+4/zfX9aSGhPMh3ePqPf+RUTk3OXk5Ph+XrJkCQ8++CDbtm3ztQUGBlbbvrKy8qxuv46KijqnOux2OwkJCee0j9Se5ZOgvz+b2zTNU87wdrvd/PSnP+Xhhx+mY8eO9dLnSU6nk7CwsGrL+eBv955uV5XnvPQvItLYmKZJqavKkuVsZ3gkJCT4lvDwcAzD8H0uLy8nIiKCV155hREjRhAQEMC//vUvjhw5wnXXXUfr1q0JCgqiR48eNaZZjBgxgunTp/s+t2nTht///vfcdNNNhIaGkpyczIIFC3zrvz8y89FHH2EYBv/73//o168fQUFBDB48uFo4A/jd735HXFwcoaGh3Hzzzdx77711uoRWUVHBL3/5S+Li4ggICODCCy/kiy++8K0/duwY119/PbGxsQQGBtKhQwdeeOEFwHt1Z9q0aSQmJhIQEECbNm3OOKXFSpaNAMXExGC322uMzOTl5dUYwQEoKiriyy+/ZMOGDUybNg3wPhHSNE0cDgfLly/n4osvJiEh4az7bGj+Dm8AqlAAEpEWoqzSTdcHl1ny3VseGUWQf/38mrvnnnt48skneeGFF3A6nZSXl5OWlsY999xDWFgY7777LhMnTqRt27YMHDjwtP08+eST/Pa3v+W+++7jtdde47bbbmPYsGF07tz5tPvMmjWLJ598ktjYWKZMmcJNN93EJ598AsC///1vHn30UebNm8eQIUNYvHgxTz75JKmpqbU+1t/85je8/vrrvPjii6SkpPCHP/yBUaNGsXPnTqKionjggQfYsmUL7733HjExMezcuZOyMu/UjmeeeYZ33nmHV155heTkZPbt28e+fftqXcv5ZFkA8vf3Jy0tjYyMDK666ipfe0ZGBmPHjq2xfVhYGJs3b67WNm/ePD744ANee+013z/s9PR0MjIyuOuuu3zbLV++nMGDB5+nIzl7JwOQq6puD28SEZGGNX36dK6++upqbXfffbfv5zvuuIP333+fV1999YwB6Ec/+hG333474A1VTz/9NB999NEZA9Cjjz7K8OHDAbj33nu5/PLLKS8vJyAggD//+c9MnjyZn//85wA8+OCDLF++nOLi4lodZ0lJCfPnz2fhwoWMGTMGgL/+9a9kZGTw97//nV//+tdkZ2fTp08f+vXrB3hHtk7Kzs6mQ4cOXHjhhRiGQUpKSq3qaAiWvgpjxowZTJw4kX79+pGens6CBQvIzs5mypQpgHduzoEDB3jppZew2Ww1JoadHJ77bvudd97JsGHDePzxxxk7dixvv/02K1asYPXq1Q16bKfiuwTm1giQiLQMgX52tjwyyrLvri8nf9mf5Ha7eeyxx1iyZAkHDhzw3U0cHBx8xn569uzp+/nkpba8vLyz3icxMRHwXtlITk5m27ZtvkB10oABA/jggw/O6ri+b9euXVRWVjJkyBBfm5+fHwMGDGDr1q0A3HbbbVxzzTWsX7+ekSNHMm7cON8gw4033shll11Gp06dGD16NFdccUWNG5MaC0sD0IQJEzhy5AiPPPKIb+b70qVLfYkxJyfnB58J9H2DBw9m8eLF3H///TzwwAO0a9eOJUuWnDGRNxSnQ3OARKRlMQyj3i5DWen7webJJ5/k6aefZu7cufTo0YPg4GCmT5+Oy+U6Yz/fnzxtGAYez5l/J3x3n5PzWb+7z6nmvdbWyX3PNJd2zJgx7N27l3fffZcVK1ZwySWXMHXqVJ544gn69u1LVlYW7733HitWrGD8+PFceumlvPbaa7Wu6XyxfBL07bffzp49e6ioqGDdunUMGzbMt27hwoV89NFHp9139uzZp7yN7yc/+QnffPMNLpeLrVu31hi2tMrJS2AeE6o0CiQi0mStWrWKsWPH8rOf/YxevXrRtm1bduzY0eB1dOrUic8//7xa25dfflnr/tq3b4+/v3+1qyaVlZV8+eWXdOnSxdcWGxvLjTfeyL/+9S/mzp1bbTJ3WFgYEyZM4K9//StLlizh9ddf5+jRo7Wu6Xxp+rG8CTkZgMB7Gcxhtzx/iohILbRv357XX3+dNWvWEBkZyVNPPUVubm61kNAQ7rjjDn7xi1/Qr18/Bg8ezJIlS9i0aRNt27b9wX2/fzcZQNeuXbntttv49a9/TVRUFMnJyfzhD3+gtLSUyZMnA955RmlpaXTr1o2Kigr++9//+o776aefJjExkd69e2Oz2Xj11VdJSEggIiKiXo+7PigANSD/7wQeV5WHIH8LixERkVp74IEHyMrKYtSoUQQFBXHLLbcwbtw4CgoKGrSO66+/nt27d3P33XdTXl7O+PHjufHGG2uMCp3KtddeW6MtKyuLxx57DI/Hw8SJEykqKqJfv34sW7aMyMhIwHsT08yZM9mzZw+BgYEMHTqUxYsXAxASEsLjjz/Ojh07sNvt9O/fn6VLl2KzNb7/4bf0VRiN1bk8SvtctZ35Lh4TPr/vEuLCWvZj4kWk+dGrMKx32WWXkZCQwD//+U+rSzkv6utVGBoBamD+DhvllR49C0hEROqstLSUv/zlL4waNQq73c6iRYtYsWIFGRkZVpfW6CkANTB/uzcA6VZ4ERGpK8MwWLp0Kb/73e+oqKigU6dOvP7661x66aVWl9boKQA1pLxveNb4AwcdIbiqhlpdjYiINHGBgYGsWLHC6jKaJAWghlRRxDDzS/bZYjmqS2AiIiKWaXzTspszu/dhVn5GlS6BiYiIWEgBqCE5nAD4UaWnQYuIiFhIAagh2b0P/vFXABIREbGUAlBD+k4A0m3wIiIi1lEAakgnApDTqMRV5ba4GBERqU8jRoxg+vTpvs9t2rRh7ty5Z9zHMAzeeuutOn93ffXTkigANST7t2/0raw88xuDRUSkYVx55ZWnfW7O2rVrMQyD9evXn3O/X3zxBbfccktdy6tm9uzZ9O7du0Z7Tk4OY8aMqdfv+r6FCxc2ynd61ZYCUEM6MQkaoMpVYWEhIiJy0uTJk/nggw/Yu3dvjXX/+Mc/6N27N3379j3nfmNjYwkKCqqPEn9QQkICTqfzhzcUHwWghmT/9u2n7spyCwsREZGTrrjiCuLi4li4cGG19tLSUpYsWcLkyZM5cuQI1113Ha1btyYoKIgePXqwaNGiM/b7/UtgO3bsYNiwYQQEBNC1a9dTvq7innvuoWPHjgQFBdG2bVseeOABKisrAe8IzMMPP8zGjRsxDAPDMHw1f/8S2ObNm7n44osJDAwkOjqaW265heLiYt/6G2+8kXHjxvHEE0+QmJhIdHQ0U6dO9X1XbWRnZzN27FhCQkIICwtj/PjxHDp0yLd+48aNXHTRRYSGhhIWFkZaWhpffvklAHv37uXKK68kMjKS4OBgunXrxtKlS2tdy9nQgxAbks2BBwMbJu5KjQCJSAtgmlBZas13+wWBYfzgZg6Hg0mTJrFw4UIefPBBjBP7vPrqq7hcLq6//npKS0tJS0vjnnvuISwsjHfffZeJEyfStm1bBg4c+IPf4fF4uPrqq4mJieHTTz+lsLCw2nyhk0JDQ1m4cCGtWrVi8+bN/OIXvyA0NJTf/OY3TJgwga+++or333/f9/Tn8PDwGn2UlpYyevRoBg0axBdffEFeXh4333wz06ZNqxbyPvzwQxITE/nwww/ZuXMnEyZMoHfv3vziF7/4weP5PtM0GTduHMHBwXz88cdUVVVx++23M2HCBD766CPA++b6Pn36MH/+fOx2O5mZmfj5eaeGTJ06FZfLxcqVKwkODmbLli2EhISccx3nQgGoIRkGbsMPm+nCowAkIi1BZSn8vpU1333fQfAPPqtNb7rpJv74xz/y0UcfcdFFFwHey19XX301kZGRREZGcvfdd/u2v+OOO3j//fd59dVXzyoArVixgq1bt7Jnzx5at24NwO9///sa83buv/9+389t2rThV7/6FUuWLOE3v/kNgYGBhISE4HA4SEhIOO13/fvf/6asrIyXXnqJ4GDv8T/77LNceeWVPP7448THxwMQGRnJs88+i91up3Pnzlx++eX873//q1UAWrFiBZs2bSIrK4ukpCQA/vnPf9KtWze++OIL+vfvT3Z2Nr/+9a/p3LkzAB06dPDtn52dzTXXXEOPHj0AaNu27TnXcK50CayBuQ1v2tUIkIhI49G5c2cGDx7MP/7xDwB27drFqlWruOmmmwBwu908+uij9OzZk+joaEJCQli+fDnZ2dln1f/WrVtJTk72hR+A9PT0Gtu99tprXHjhhSQkJBASEsIDDzxw1t/x3e/q1auXL/wADBkyBI/Hw7Zt23xt3bp1w263+z4nJiaSl5d3Tt/13e9MSkryhR+Arl27EhERwdatWwGYMWMGN998M5deeimPPfYYu3bt8m37y1/+kt/97ncMGTKEhx56iE2bNtWqjnOhEaAG5rb5gQc8VbW/zioi0mT4BXlHYqz67nMwefJkpk2bxnPPPccLL7xASkoKl1xyCQBPPvkkTz/9NHPnzqVHjx4EBwczffp0XK6zu6PXNM0abcb3Ls99+umnXHvttTz88MOMGjWK8PBwFi9ezJNPPnlOx2GaZo2+T/WdJy8/fXedx1O7Z9Sd7ju/2z579mx++tOf8u677/Lee+/x0EMPsXjxYq666ipuvvlmRo0axbvvvsvy5cuZM2cOTz75JHfccUet6jkbGgFqYJ4TI0CeKo0AiUgLYBjey1BWLGcx/+e7xo8fj91u5+WXX+bFF1/k5z//ue+X96pVqxg7diw/+9nP6NWrF23btmXHjh1n3XfXrl3Jzs7m4MFvw+DatWurbfPJJ5+QkpLCrFmz6NevHx06dKhxZ5q/vz9u95mfI9e1a1cyMzMpKSmp1rfNZqNjx45nXfO5OHl8+/bt87Vt2bKFgoICunTp4mvr2LEjd911F8uXL+fqq6/mhRde8K1LSkpiypQpvPHGG/zqV7/ir3/963mp9SQFoAbmsZ0MQLoLTESkMQkJCWHChAncd999HDx4kBtvvNG3rn379mRkZLBmzRq2bt3KrbfeSm5u7ln3femll9KpUycmTZrExo0bWbVqFbNmzaq2Tfv27cnOzmbx4sXs2rWLZ555hjfffLPaNm3atCErK4vMzEzy8/OpqKj5P9PXX389AQEB3HDDDXz11Vd8+OGH3HHHHUycONE3/6e23G43mZmZ1ZYtW7Zw6aWX0rNnT66//nrWr1/P559/zqRJkxg+fDj9+vWjrKyMadOm8dFHH7F3714++eQTvvjiC184mj59OsuWLSMrK4v169fzwQcfVAtO54MCUAM7GYCo0oMQRUQam8mTJ3Ps2DEuvfRSkpOTfe0PPPAAffv2ZdSoUYwYMYKEhATGjRt31v3abDbefPNNKioqGDBgADfffDOPPvpotW3Gjh3LXXfdxbRp0+jduzdr1qzhgQceqLbNNddcw+jRo7nooouIjY095a34QUFBLFu2jKNHj9K/f39+8pOfcMkll/Dss8+e28k4heLiYvr06VNt+dGPfuS7DT8yMpJhw4Zx6aWX0rZtW5YsWQKA3W7nyJEjTJo0iY4dOzJ+/HjGjBnDww8/DHiD1dSpU+nSpQujR4+mU6dOzJs3r871nolhnurCZAtXWFhIeHg4BQUFhIWF1WvfR57oR3TxDuYlPcHtk899pr2ISGNWXl5OVlYWqampBAQEWF2ONENn+jt2Lr+/NQLUwEzbiYchag6QiIiIZRSAGph58mnQbl0CExERsYoCUAMzT7wQ1VQAEhERsYwCUEPzXQJTABIREbGKAlBDc3gDkOHRgxBFpPnS/TVyvtTX3y0FoIZ2Yg6Q4dYkaBFpfk4+Xbi01KIXoEqzd/Lp2999jUdt6FUYDcxwOL1/ujUCJCLNj91uJyIiwvdOqaCgoNO+lkHkXHk8Hg4fPkxQUBAOR90ijAJQAzN0CUxEmrmTbyqv7Ys1Rc7EZrORnJxc52CtANTADLt3BMjm0SRoEWmeDMMgMTGRuLg4Kiv1P3tSv/z9/bHZ6j6DRwGogZ0cAbJpBEhEmjm73V7neRoi54smQTcwm593BMiuESARERHLKAA1MNvJESCzyuJKREREWi4FoAb27QhQpZ6TISIiYhHLA9C8efN8b3RNS0tj1apVp9129erVDBkyhOjoaAIDA+ncuTNPP/10tW0WLlyIYRg1lvLy8vN9KGfF7ud9c60/lVS6FYBERESsYOkk6CVLljB9+nTmzZvHkCFDeP755xkzZgxbtmwhOTm5xvbBwcFMmzaNnj17EhwczOrVq7n11lsJDg7mlltu8W0XFhbGtm3bqu0bEBBw3o/nbNj9vJfA/IwqXG4P/g7LM6iIiEiLY2kAeuqpp5g8eTI333wzAHPnzmXZsmXMnz+fOXPm1Ni+T58+9OnTx/e5TZs2vPHGG6xatapaADIMw/ccirNRUVFBRcW3T2YuLCyszeGcFYdvBKgKV5UHnOftq0REROQ0LBt+cLlcrFu3jpEjR1ZrHzlyJGvWrDmrPjZs2MCaNWsYPnx4tfbi4mJSUlJo3bo1V1xxBRs2bDhjP3PmzCE8PNy3JCUlndvBnIOTk6D9TgYgERERaXCWBaD8/Hzcbjfx8fHV2uPj48nNzT3jvq1bt8bpdNKvXz+mTp3qG0EC6Ny5MwsXLuSdd95h0aJFBAQEMGTIEHbs2HHa/mbOnElBQYFv2bdvX90O7kxOvArDXwFIRETEMpY/CPH7j7I2TfMHH2+9atUqiouL+fTTT7n33ntp37491113HQCDBg1i0KBBvm2HDBlC3759+fOf/8wzzzxzyv6cTidOZwNdi7J7XxToTxUut7thvlNERESqsSwAxcTEYLfba4z25OXl1RgV+r7U1FQAevTowaFDh5g9e7YvAH2fzWajf//+ZxwBalAnXoXhb1TiqtJdYCIiIlaw7BKYv78/aWlpZGRkVGvPyMhg8ODBZ92PaZrVJjCfan1mZiaJiYm1rrVeObyToANw4XLrEpiIiIgVLL0ENmPGDCZOnEi/fv1IT09nwYIFZGdnM2XKFMA7N+fAgQO89NJLADz33HMkJyfTuXNnwPtcoCeeeII77rjD1+fDDz/MoEGD6NChA4WFhTzzzDNkZmby3HPPNfwBnorftwGoRHOARERELGFpAJowYQJHjhzhkUceIScnh+7du7N06VJSUlIAyMnJITs727e9x+Nh5syZZGVl4XA4aNeuHY899hi33nqrb5vjx49zyy23kJubS3h4OH369GHlypUMGDCgwY/vlByBADiNSk2CFhERsYhh6n0MNRQWFhIeHk5BQQFhYWH12/mhr2H+YPLNMDZd9yUXdz7zfCcRERE5O+fy+1uPIW5o350DpBEgERERSygANTQ/7yWwAFxUKACJiIhYQgGooZ0YAXIYHipdp797TURERM4fBaCGdmIECMDtKrOwEBERkZZLAaihOb59K71HAUhERMQSCkANzTCoNLwvRFUAEhERsYYCkAUqbd7XYZiVCkAiIiJWUACyQJUCkIiIiKUUgCzwbQAqt7gSERGRlkkByAJum3citAKQiIiINRSALOC2O0/8oEtgIiIiVlAAsoDb7h0BMjQCJCIiYgkFIAt4TgagKgUgERERKygAWcB0nAxAugQmIiJiBQUgCxh+J54GrREgERERSygAWcAXgPQcIBEREUsoAFnA5hcEaA6QiIiIVRSALGB3et8Ib3MrAImIiFhBAcgCDqd3BMjmrrC4EhERkZZJAcgCDn/vCJBdI0AiIiKWUACygF9AsPdP00WV22NxNSIiIi2PApAF/ANDAQimjBKX2+JqREREWh4FIAs4QqIACDdKKHVVWVyNiIhIy6MAZIXASAAiKKGkQgFIRESkoSkAWeFkADKKKanQJTAREZGGpgBkhcATl8AopqSi0uJiREREWh4FICucGAHyN9yUlxRaXIyIiEjLowBkBb9AXPgBUFVyxOJiREREWh4FICsYBqX2MADcxccsLkZERKTlUQCySJnd+ywgT+lRiysRERFpeRSALFLuF37iB40AiYiINDQFIIu4TgQgW/lxawsRERFpgRSALOJ2Rnh/KNMIkIiISENTALKIEeR9FpChACQiItLgFIAs4giNAyCg4rDFlYiIiLQ8CkAW8Y+6AIBQV77FlYiIiLQ8lgegefPmkZqaSkBAAGlpaaxateq0265evZohQ4YQHR1NYGAgnTt35umnn66x3euvv07Xrl1xOp107dqVN99883weQq0ExyQDEO3JxzRNi6sRERFpWSwNQEuWLGH69OnMmjWLDRs2MHToUMaMGUN2dvYptw8ODmbatGmsXLmSrVu3cv/993P//fezYMEC3zZr165lwoQJTJw4kY0bNzJx4kTGjx/PZ5991lCHdVZC47wBKJ6jFJS6LK5GRESkZTFMC4cfBg4cSN++fZk/f76vrUuXLowbN445c+acVR9XX301wcHB/POf/wRgwoQJFBYW8t577/m2GT16NJGRkSxatOiUfVRUVFBRUeH7XFhYSFJSEgUFBYSFhdXm0H5YZTk8Gg/A7ps20zY5+fx8j4iISAtRWFhIeHj4Wf3+tmwEyOVysW7dOkaOHFmtfeTIkaxZs+as+tiwYQNr1qxh+PDhvra1a9fW6HPUqFFn7HPOnDmEh4f7lqSkpHM4klryC6DA8D4NujDv1CNeIiIicn5YFoDy8/Nxu93Ex8dXa4+Pjyc3N/eM+7Zu3Rqn00m/fv2YOnUqN998s29dbm7uOfc5c+ZMCgoKfMu+fftqcUTn7pg9FoDyI/sb5PtERETEy2F1AYZhVPtsmmaNtu9btWoVxcXFfPrpp9x77720b9+e6667rtZ9Op1OnE5nLaqvm2JnLFTtpvKYApCIiEhDsiwAxcTEYLfba4zM5OXl1RjB+b7U1FQAevTowaFDh5g9e7YvACUkJNSqTytUBCVCCVDQMCNOIiIi4mXZJTB/f3/S0tLIyMio1p6RkcHgwYPPuh/TNKtNYE5PT6/R5/Lly8+pz4ZiRniDnLNIc4BEREQakqWXwGbMmMHEiRPp168f6enpLFiwgOzsbKZMmQJ45+YcOHCAl156CYDnnnuO5ORkOnfuDHifC/TEE09wxx13+Pq88847GTZsGI8//jhjx47l7bffZsWKFaxevbrhD/AH+Me1hR0QXqZLYCIiIg3J0gA0YcIEjhw5wiOPPEJOTg7du3dn6dKlpKSkAJCTk1PtmUAej4eZM2eSlZWFw+GgXbt2PPbYY9x6662+bQYPHszixYu5//77eeCBB2jXrh1Llixh4MCBDX58PyQssRMA8e6DFlciIiLSslj6HKDG6lyeI1AXJUXHCX7SG/YKp+8iLCLmvH2XiIhIc9ckngMkEBwaQT4RAOTv/cbaYkRERFoQBSCL5TlaAVB0cJvFlYiIiLQcCkAWOx7kvQRWlbvV4kpERERaDgUgi1VGdwHA/6gugYmIiDQUBSCLBbbuAUBMyU6LKxEREWk5FIAsFtuuDwDx7lzMimKLqxEREWkZFIAs1jophXwzDJthkrd7o9XliIiItAgKQBbzs9vI9vO+EuNYVqa1xYiIiLQQCkCNwLGQDgBUHvzK4kpERERaBgWgRsAd670TLOCY7gQTERFpCApAjUBQ654AxJbusrgSERGRlkEBqBGIb9cbj2kQYRZgFh2yuhwREZFmTwGoEUhJiGEv8QAc0URoERGR804BqBHwd9jYf+JOsONZGyyuRkREpPlTAGokCsM6AuDO0Z1gIiIi55sCUGMR3xWAoOPbLS5ERESk+VMAaiRCk3sBEFeeBR63xdWIiIg0bwpAjURSu26Umf44ceE+stvqckRERJo1BaBGIjkmlJ20BuDILk2EFhEROZ8UgBoJu80gN6AtAIXZeimqiIjI+aQA1IiURHTy/nDoa2sLERERaeYUgBoRR0J3AEILd1hciYiISPOmANSIRKT2BiC28gC4Sq0tRkREpBlTAGpE2rZJJd8Mw4aJK3eL1eWIiIg0WwpAjUhieAA7jWQAjuzWnWAiIiLniwJQI2IYBvlB7QAo3bfJ4mpERESaLwWgRqYiqgsAjvytFlciIiLSfCkANTIBF3jvBIss0p1gIiIi54sCUCMTk9obj2kQ5jkOxYetLkdERKRZUgBqZNq3jmOvGQdA+QHNAxIRETkfFIAamegQJ1n2NgAc2Z1paS0iIiLNlQJQI3Q8pD0AFQc3W1yJiIhI86QA1AhVxXrvBHMe+cbiSkRERJonBaBGKLh1TwBiynaDx2NxNSIiIs2PAlAjlNi2G+WmH06zAo5lWV2OiIhIs6MA1Ah1SAhnh3kBACV6IrSIiEi9szwAzZs3j9TUVAICAkhLS2PVqlWn3faNN97gsssuIzY2lrCwMNLT01m2bFm1bRYuXIhhGDWW8vLy830o9SY0wI99jlQAju3ZaHE1IiIizY+lAWjJkiVMnz6dWbNmsWHDBoYOHcqYMWPIzs4+5fYrV67ksssuY+nSpaxbt46LLrqIK6+8kg0bqr84NCwsjJycnGpLQEBAQxxSvSkK7wiAO0cjQCIiIvXNYeWXP/XUU0yePJmbb74ZgLlz57Js2TLmz5/PnDlzamw/d+7cap9///vf8/bbb/Of//yHPn36+NoNwyAhIeGs66ioqKCiosL3ubCw8ByPpP65E3rBMYg4qlvhRURE6ptlI0Aul4t169YxcuTIau0jR45kzZo1Z9WHx+OhqKiIqKioau3FxcWkpKTQunVrrrjiihojRN83Z84cwsPDfUtSUtK5Hcx5ENp2AG7TILwyDwoOWF2OiIhIs2JZAMrPz8ftdhMfH1+tPT4+ntzc3LPq48knn6SkpITx48f72jp37szChQt55513WLRoEQEBAQwZMoQdO07/ctGZM2dSUFDgW/bt21e7g6pHXVIS+cZMBsCd/bnF1YiIiDQvll4CA+/lqu8yTbNG26ksWrSI2bNn8/bbbxMXF+drHzRoEIMGDfJ9HjJkCH379uXPf/4zzzzzzCn7cjqdOJ3OWh7B+dE2JphXjI50Yy8FO9cS1eMqq0sSERFpNiwbAYqJicFut9cY7cnLy6sxKvR9S5YsYfLkybzyyitceumlZ9zWZrPRv3//M44ANUY2m0F+RC8APBoBEhERqVeWBSB/f3/S0tLIyMio1p6RkcHgwYNPu9+iRYu48cYbefnll7n88st/8HtM0yQzM5PExMQ619zQbEn9AQg//jVUuSyuRkREpPmw9BLYjBkzmDhxIv369SM9PZ0FCxaQnZ3NlClTAO/cnAMHDvDSSy8B3vAzadIk/vSnPzFo0CDf6FFgYCDh4eEAPPzwwwwaNIgOHTpQWFjIM888Q2ZmJs8995w1B1kHF7TrzrHNIURSDIc2wwVpVpckIiLSLFgagCZMmMCRI0d45JFHyMnJoXv37ixdupSUlBQAcnJyqj0T6Pnnn6eqqoqpU6cydepUX/sNN9zAwoULATh+/Di33HILubm5hIeH06dPH1auXMmAAQMa9NjqQ4/WEWzwtOdieybu7M+xKwCJiIjUC8M0TdPqIhqbwsJCwsPDKSgoICwszLI6PB6T5x++mduM1zje+Voirn3eslpEREQau3P5/W35qzDk9Gw2A1dUewCqDn1jcTUiIiLNhwJQIxfUqpv3z8JdoME6ERGReqEA1MgltuuB2zQIchdBcZ7V5YiIiDQLCkCNXPfkOPaZ3gc9VuXpMpiIiEh9UABq5FKig8iytQbg8O6NFlcjIiLSPCgANXKGYVAU0haA4v1bLK5GRESkeVAAagI80R0B8DvWtF7nISIi0lgpADUBfgldAIgs2W1xJSIiIs2DAlATEJ7svRU+3H0Uyo5bW4yIiEgzoADUBKQkJpBjRgHgydtmcTUiIiJNX60C0L59+9i/f7/v8+eff8706dNZsGBBvRUm32oVEchO03snWMHeTGuLERERaQZqFYB++tOf8uGHHwKQm5vLZZddxueff859993HI488Uq8FCthtBjkB7QAo3bfJ4mpERESavloFoK+++sr3dvVXXnmF7t27s2bNGl5++WXfW9mlfhVHdALAfvhriysRERFp+moVgCorK3E6nQCsWLGCH//4xwB07tyZnJyc+qtOfBytegEQXrhd7wQTERGpo1oFoG7duvGXv/yFVatWkZGRwejRowE4ePAg0dHR9VqgeMWldsdl2gn0lMDxbKvLERERadJqFYAef/xxnn/+eUaMGMF1111Hr17e0Yl33nnHd2lM6lenC6J9E6HduZstrkZERKRpc9RmpxEjRpCfn09hYSGRkZG+9ltuuYWgoKB6K06+lRwVxH9IoSt7KcjaQFSXK6wuSUREpMmq1QhQWVkZFRUVvvCzd+9e5s6dy7Zt24iLi6vXAsXLbjM4Eup9JYZrv16KKiIiUhe1CkBjx47lpZdeAuD48eMMHDiQJ598knHjxjF//vx6LVC+5Y71PhHaefQbiysRERFp2moVgNavX8/QoUMBeO2114iPj2fv3r289NJLPPPMM/VaoHwrOLk3AOHl+6Gi2NpiREREmrBaBaDS0lJCQ0MBWL58OVdffTU2m41Bgwaxd+/eei1QvpWanEyOGYUNEzQRWkREpNZqFYDat2/PW2+9xb59+1i2bBkjR44EIC8vj7CwsHotUL7VKSGUzZ5UACqy11lcjYiISNNVqwD04IMPcvfdd9OmTRsGDBhAeno64B0N6tOnT70WKN+KDnGy2887Ebo463OLqxEREWm6anUb/E9+8hMuvPBCcnJyfM8AArjkkku46qqr6q04qakkpgfkLcKRm2l1KSIiIk1WrQIQQEJCAgkJCezfvx/DMLjgggv0EMQGEJDcD/IgvHQvlBdAQLjVJYmIiDQ5tboE5vF4eOSRRwgPDyclJYXk5GQiIiL47W9/i8fjqe8a5TvatUlmnyfW+yFHzwMSERGpjVqNAM2aNYu///3vPPbYYwwZMgTTNPnkk0+YPXs25eXlPProo/Vdp5zQrVU4m8xUkjhM1b51OFKHWV2SiIhIk1OrAPTiiy/yt7/9zfcWeIBevXpxwQUXcPvttysAnUetIwN5zd6By/mckqwvCFf+EREROWe1ugR29OhROnfuXKO9c+fOHD16tM5FyekZhkFJdE8A7JoILSIiUiu1CkC9evXi2WefrdH+7LPP0rNnzzoXJWfmTOqLxzQIKTsAJflWlyMiItLk1OoS2B/+8Acuv/xyVqxYQXp6OoZhsGbNGvbt28fSpUvru0b5ng4pF7BrfSs6GAdg/5fQabTVJYmIiDQptRoBGj58ONu3b+eqq67i+PHjHD16lKuvvpqvv/6aF154ob5rlO/pfkEYmZ52AHj2fWFxNSIiIk1PrZ8D1KpVqxqTnTdu3MiLL77IP/7xjzoXJqfXNiaEl+0dgZWUZn1GiNUFiYiINDG1GgESa9lsBuXxaQA4c9eBu9LiikRERJoWBaAmKrpdH46ZIfi5S+FgptXliIiINCkKQE1Un5QoPvV08X7I+tjaYkRERJqYcwpAV1999RmXu+6665wLmDdvHqmpqQQEBJCWlsaqVatOu+0bb7zBZZddRmxsLGFhYaSnp7Ns2bIa273++ut07doVp9NJ165defPNN8+5rsaud1Ikaz1dAajctdLiakRERJqWcwpA4eHhZ1xSUlKYNGnSWfe3ZMkSpk+fzqxZs9iwYQNDhw5lzJgxZGdnn3L7lStXctlll7F06VLWrVvHRRddxJVXXsmGDRt826xdu5YJEyYwceJENm7cyMSJExk/fjyfffbZuRxqoxcV7E92mHcekG3/Z1BVYXFFIiIiTYdhmqZp1ZcPHDiQvn37Mn/+fF9bly5dGDduHHPmzDmrPrp168aECRN48MEHAZgwYQKFhYW89957vm1Gjx5NZGQkixYtOqs+CwsLCQ8Pp6CggLCwsHM4ooZ11+IN3Lf1x8QahXDjUmgzxOqSRERELHMuv78tmwPkcrlYt24dI0eOrNY+cuRI1qxZc1Z9eDweioqKiIqK8rWtXbu2Rp+jRo06Y58VFRUUFhZWW5qCPimRfHriMhhZugwmIiJytiwLQPn5+bjdbuLj46u1x8fHk5ube1Z9PPnkk5SUlDB+/HhfW25u7jn3OWfOnGqX8pKSks7hSKwzIDWKTzzdAfDsWG5xNSIiIk2H5XeBGYZR7bNpmjXaTmXRokXMnj2bJUuWEBcXV6c+Z86cSUFBgW/Zt2/fORyBdTrFh7IhYCAe08B2cD0UHLC6JBERkSbBsgAUExOD3W6vMTKTl5dXYwTn+5YsWcLkyZN55ZVXuPTSS6utS0hIOOc+nU4nYWFh1ZamwDAMOnfowDqzg7fhm3etLUhERKSJsCwA+fv7k5aWRkZGRrX2jIwMBg8efNr9Fi1axI033sjLL7/M5ZdfXmN9enp6jT6XL19+xj6bsiHtY1jm7u/9sPUda4sRERFpImr9LrD6MGPGDCZOnEi/fv1IT09nwYIFZGdnM2XKFMB7aerAgQO89NJLgDf8TJo0iT/96U8MGjTIN9ITGBhIeHg4AHfeeSfDhg3j8ccfZ+zYsbz99tusWLGC1atXW3OQ59mF7WP4s6cf9/NvzL1rMEqPQlDUD+8oIiLSglk6B2jChAnMnTuXRx55hN69e7Ny5UqWLl1KSkoKADk5OdWeCfT8889TVVXF1KlTSUxM9C133nmnb5vBgwezePFiXnjhBXr27MnChQtZsmQJAwcObPDjawitIgLxi2nLVk8yhumGbe/98E4iIiItnKXPAWqsmspzgE568O2viPnij/zS8RZ0uxr+7wWrSxIREWlwTeI5QFJ/Lu4cx0p3TwDM3R+Cx21xRSIiIo2bAlAzkN4ump3+nSkyAzHKjkFOptUliYiINGoKQM2A02FnaOdWrPF08zbs+sDagkRERBo5BaBmYmTXeFZ5eng/7PrQ2mJEREQaOQWgZmJEp1jW0hsAc99nUN403mcmIiJiBQWgZiI0wI/W7bqS5YnH8FTBzhVWlyQiItJoKQA1Ixd1imWp58Tzjja/am0xIiIijZgCUDMyvFMcb7ovBMDcsRxKj1pckYiISOOkANSMtIkOwhXZka89Kd7LYF+/aXVJIiIijZICUDNiGAbDO8b6RoHY9Iq1BYmIiDRSCkDNzPCOsbzjHowHA/Z9CnlbrS5JRESk0VEAambS20VzzB7Fcnc/b8PaZ60tSEREpBFSAGpmgp0OLuoUx4Kqy70Nm16BolxrixIREWlkFICaoZ8OTGa92ZH1dAK3Cz77i9UliYiINCoKQM3Q0A6xXBARyHzXiVGgz/8KRYesLUpERKQRUQBqhuw2g+sGJLHC05ftjo7gKoYPfmt1WSIiIo2GAlAzNb5fEg67nXtLfupt2PAvyNlobVEiIiKNhAJQMxUXFsD/9UtivdmRNYEXASa8PxNM0+rSRERELKcA1IzdNrwdDpvB3ceuwmMPgL2f6OnQIiIiKAA1a0lRQfwkrTUHieHNwGu8jUt/DcV51hYmIiJiMQWgZm7qRe1xOmzMzB9JYVhHKM2Hd+7QpTAREWnRFICauaSoIG4f0R4XfkwpnYJp94ft78P6F60uTURExDIKQC3ArcPbkhwVxJriBP7X6lZv4/v3Qf4OawsTERGxiAJQCxDgZ2f2j7sCcNuuQZS2GgyVJbBkIlQUW1ydiIhIw1MAaiEu7hzPZV3jqfQYTK24HTMkAQ5v1XwgERFpkRSAWpCHf9yNEKeDDw/Y+G+n34PNAV+/oXeFiYhIi6MA1IK0ighk1uVdAPj150HkD37Qu2L5/bB3jYWViYiINCwFoBbm2v5JXNg+hvJKDz/d1IuqrteApwpevRGKcq0uT0REpEEoALUwhmHw1IRexIU62Z5Xwiz3LzDjukLxIW8IcldaXaKIiMh5pwDUAsWFBvDsT/titxks2XiUtzs9Ds4wyF4Ly+6zujwREZHzTgGohRqQGsU9ozsB8JsPSsga+pR3xecL4LMFFlYmIiJy/ikAtWC/GNqWUd3icbk9/Gx1NKXDHvCueP8e2L7M2uJERETOIwWgFswwDP74f71oEx3EgeNl3Lr7Qjy9J4LpgVd/DrlfWV2iiIjIeaEA1MKFBfgx/2dpBPrZWbXzCL+33QKpw7xPil78Uyg9anWJIiIi9U4BSOiSGMaT43sB8Lc1+3iz/aMQkQzH98LrN4PHbXGFIiIi9UsBSAD4UY9Epl/aAYDfLN3PV0PngyMQdv0PPnzU4upERETql+UBaN68eaSmphIQEEBaWhqrVq067bY5OTn89Kc/pVOnTthsNqZPn15jm4ULF2IYRo2lvLz8PB5F8/DLiztweY9EKt0mk5aWceSSJ7wrVj0Ja+dZW5yIiEg9sjQALVmyhOnTpzNr1iw2bNjA0KFDGTNmDNnZ2afcvqKigtjYWGbNmkWvXr1O229YWBg5OTnVloCAgPN1GM2GzWbwxP/1ovsFYRwtcXH9Z8m4htztXblsJmxfbm2BIiIi9cTSAPTUU08xefJkbr75Zrp06cLcuXNJSkpi/vz5p9y+TZs2/OlPf2LSpEmEh4eftl/DMEhISKi2yNkJ9LezYGI/YkKcfJNbxNSDozH73exd+cYvIO8bawsUERGpB5YFIJfLxbp16xg5cmS19pEjR7JmTd1ezFlcXExKSgqtW7fmiiuuYMOGDWfcvqKigsLCwmpLS9YqIpAFk9Lwd9jI2JrH0/YboHV/KD8O/xipZwSJiEiTZ1kAys/Px+12Ex8fX609Pj6e3Nzav5Szc+fOLFy4kHfeeYdFixYREBDAkCFD2LFjx2n3mTNnDuHh4b4lKSmp1t/fXPRNjuSxq3sA8MzH+3ivx1y4IA3KC+CVG/SMIBERadIsnwRtGEa1z6Zp1mg7F4MGDeJnP/sZvXr1YujQobzyyit07NiRP//5z6fdZ+bMmRQUFPiWffv21fr7m5Or+7bm1uFtAbjzP/vIHLkE2l0CVWWw5Ho9I0hERJosywJQTEwMdru9xmhPXl5ejVGhurDZbPTv3/+MI0BOp5OwsLBqi3j9ZlRnLukch6vKw83/3EjWiD9BRAoc2wPPD4OcTVaXKCIics4sC0D+/v6kpaWRkZFRrT0jI4PBgwfX2/eYpklmZiaJiYn11mdLYrcZzL22N10Sw8gvruDaf27jwJgXvCGoYB+89GPI3Wx1mSIiIufE0ktgM2bM4G9/+xv/+Mc/2Lp1K3fddRfZ2dlMmTIF8F6amjRpUrV9MjMzyczMpLi4mMOHD5OZmcmWLVt86x9++GGWLVvG7t27yczMZPLkyWRmZvr6lHMXGuDHvyYPoGN8CIcKK/i/N46zf8Jy75ygsmPw0lg4tOWHOxIREWkkHFZ++YQJEzhy5AiPPPIIOTk5dO/enaVLl5KSkgJ4H3z4/WcC9enTx/fzunXrePnll0lJSWHPnj0AHD9+nFtuuYXc3FzCw8Pp06cPK1euZMCAAQ12XM1RdIiTf988iGsXrGXX4RKu/sdXvPjThXTJmAQHN3hHgm5dCWGtrC5VRETkBxmmaZpWF9HYFBYWEh4eTkFBgeYDfc+hwnIm/f1zth0qIsTp4G/j2zNo5SQ49BW0vxSufw3qMIldRESkts7l97fld4FJ0xIfFsArU9IZ1DaK4ooqJr68jQ+7PwZ2J+xcAdlrrS5RRETkBykAyTkLD/TjxZsGcEVP73vDblpayM5WV3hXfnrqp3iLiIg0JgpAUitOh50/XduH6wcmY5pw+84Tc6y++S8c22ttcSIiIj9AAUhqzW4z+N247kwZ3o7tZhKr3N3B9MAXf7W6NBERkTNSAJI6MQyDe8d05roBSbzgHg2Auf4lcJVYXJmIiMjpKQBJvbh3TBcynf3J8sRjlBfAxkVWlyQiInJaCkBSL8ID/bjxwna86B4FgPnpX8DjsbgqERGRU1MAknpzQ3obltovosgMxDiyA3Z9YHVJIiIip6QAJPUmPMiPq9K78Ip7BADmiofAVWptUSIiIqegACT1avKFqbzAleSbYRiHvoLXboKqCqvLEhERqUYBSOpVXGgAF/XrxbTKX+LCH7a/B4uu00iQiIg0KgpAUu9uHd6WL+nGDa5f43YEwq7/wT+vgv1fWl2aiIgIoAAk50HryCDG9r6AtZ5u3O2cjekfAvs+hb9dAq/eCIUHrS5RRERaOAUgOS9+PaoTcaFO3jySxPSQP1De5Sdg2ODrN+HZ/rDmz+CutLpMERFpoRSA5LxICA/gHzf2JzzQj7cPRjBg63iWD1mC2bo/uIph+f3w/DDIWmV1qSIi0gIpAMl50/2CcF6bkk73C8IoLK/ilhWV/MzzCEcufgICoyBvC7x4BSy+Ho7utrpcERFpQQzTNE2ri2hsCgsLCQ8Pp6CggLCwMKvLafKq3B7+8UkWT2Vsp7zSQ4CfjbsvjOGG8n/jl/mi9wWqNj8YNAWG/RoCwq0uWUREmqBz+f2tAHQKCkDnx94jJcx8YzNrdh0BICrYn5lpJlcfnoc960PvRgHh0O0qGPxLiG5nYbUiItLUKADVkQLQ+WOaJm9nHuTpFdvZe8T7bKCYYH8e7ZHDZfuewXZkh3dDwwbdr4Ghd0NcZwsrFhGRpkIBqI4UgM6/KreHNzcc4JkPdrDvaBkAUYE27ulyhHFlb+LMWnFiSwO6XAlpN0DKheAXYF3RIiLSqCkA1ZECUMOpdHt4bd1+/vLxLt+IkL/dxm2dirnZfJ3QrPe+3dgvCNpfAsPvgYQeFlUsIiKNlQJQHSkANTy3xyRjyyH+umo36/Ye87X/X3IRtwf9jzb5qzCKc77doe0I6DsJOl8BDmfDFywiIo2OAlAdKQBZa332Mf62ajfvf5WL58TfzvAAB7d3LuFa1+uE734XOLEiNBHSp0HXH0NEsmU1i4iI9RSA6kgBqHHYf6yUV7/cz6tf7uNgQbmv/aL4Mu6M/oyeef/B9t1RoYSe3vlCna+AuC5gGBZULSIiVlEAqiMFoMbF7TFZteMwr3y5j4wth6h0e//KBturuD9xHaONNUTkr8MwPd/uFNMRLp0N7S8Dh781hYuISINSAKojBaDG61iJi7cyD/D6+v18daDQ194+uIzprXcyzPMZoQdXY7hd3hV+QdD+UujzM0gepIcsiog0YwpAdaQA1DRszSnk9XX7eSvzAPnFLl97+zAPj0a9S9rxZTjKj367g90JncZAzwneUKSRIRGRZkUBqI4UgJqWSreHj7cd5s3MA3z4TR6lLjcABh5GRR/mtuCP6FSWSUDR3m93CoyCAbfAwFshKMqiykVEpD4pANWRAlDTVV7p5qNth3ln4wFWbM3DVXVyXpDJJRGHmBaznp7HMrCXHPI2OwIgsTdEt/feWt/hMgiMsKZ4ERGpEwWgOlIAah4KyytZ/vUh1uzMZ9nXuZScGBlyGB5uj9/CDVWvEV28vfpOgVHQ7+feZwxFtmn4okVEpNYUgOpIAaj5KXVV8e6mHF79cj+f7zk5L8ikg3GAiyLzuSwyh55Fq3AW7vGusjkgvrt34nTfG3RbvYhIE6AAVEcKQM3b/mOlZGw5xPKvD/H5nqO4Tzxt0UEVN4Z+wST/j0ku2VR9p8g2MGQ6tLsYIlMavGYREflhCkB1pADUchwrcfHBN3ks35LLx9sPU17pwcBDN2MPnQOOMinkC7oXf4rNrPx2p6BoCLsAOo7yjhC1GaY7ykREGgEFoDpSAGqZylxuVu/MZ/nXuazYeohjpd7QE0Q5N/u9zxUBm2hfuR0bnuo7hsR7X87a5UpwBHonVLdOs+AIRERaNgWgOlIAkiq3h3V7j7F8yyGWfZ3L/mNlAERSSLxxnH4BB5kQ9CWdXF/hX1lYs4Oe10LqUO88ovjuYHc08BGIiLQ8CkB1pAAk32WaJtlHS3n/q1xW78xn3d5jvmcNhVLKINsWLgrdz0DHDqKNQiKKd1XvwD8EWveHNkO8wajsGNjsEN/NgqMREWm+mlQAmjdvHn/84x/JycmhW7duzJ07l6FDh55y25ycHH71q1+xbt06duzYwS9/+Uvmzp1bY7vXX3+dBx54gF27dtGuXTseffRRrrrqqrOuSQFIzqTS7WFD9nFWbj/Myh2H2XyggO/+WzTA2MpY/y/oE3iIdlU7cFYVn7qjdhdDq77ey2eJvbwTrXWnmYhIrZ3L729Lx+WXLFnC9OnTmTdvHkOGDOH5559nzJgxbNmyheTk5BrbV1RUEBsby6xZs3j66adP2efatWuZMGECv/3tb7nqqqt48803GT9+PKtXr2bgwIHn+5CkBfCz2xiQGsWA1CjuHtWJoyUuvthzlE37j7NxXwGb9vfg8/IuUAE2PHQ09jPIsZ1rnF/Qo2rztx3t+sC7nBSa6A1FgZHQ5ccQkex9SrXD2fAHKSLSzFk6AjRw4ED69u3L/PnzfW1dunRh3LhxzJkz54z7jhgxgt69e9cYAZowYQKFhYW89957vrbRo0cTGRnJokWLzqoujQBJXXg8JltyCvlkZz6fZx3ly73HKCjzTqgOpBwbJhPtGSTYC0gKNulq7CGufBd2T2XNzgLCoeMYiEiClMEQ29n7sEa/gAY+KhGRxq9JjAC5XC7WrVvHvffeW6195MiRrFmzptb9rl27lrvuuqta26hRo055qeykiooKKioqfJ8LC08xqVXkLNlsBt0vCKf7BeHcOrwdpmmy72gZmw4cZ/P+AjbtL+DfB66mqKIKTrzD1YmLEbZMBjl20CnwOL2qviLYXQDlBbBpcfUvMOzQqo/3dvz4rt4Ro+gO0HY4+Ac3/AGLiDRBlgWg/Px83G438fHx1drj4+PJzc2tdb+5ubnn3OecOXN4+OGHa/2dImdiGAbJ0UEkRwdxRc9WgHeUaHd+CRuyj7Fx/3G+PljIypx0lrkG+EKRDQ8X2zbQydhHJ//DDLVtJNxdgM10w4EvvRvtWPbtF/kFf/v6Dr8A6DAKYtpD6ggIjm6owxURaRIsvzfX+N6kT9M0a7Sd7z5nzpzJjBkzfJ8LCwtJSkqqUw0iZ2KzGbSPC6F9XAj/18/7d83tMdlzpISvDxby9cECso+U8uXeQFYUVUDZyT1N2ho5dDf2EGUU0sFxiERnBT0924ipzIG8r7/9kgPrvH8GRsEvPoCo1AY9RhGRxsyyABQTE4Pdbq8xMpOXl1djBOdcJCQknHOfTqcTp1MTTcVadptBu9gQ2sWG8ONerXzt5ZVuvsktYvP+4+w6XMKeI3Fsym/PvmNluCtMqAADDz2N3YQaZZhAWyOHoY6tDDU2Elh2lLIPHifwJ3+x7uBERBoZywKQv78/aWlpZGRkVLtFPSMjg7Fjx9a63/T0dDIyMqrNA1q+fDmDBw+uU70iVgnws9M7KYLeSRHV2l1VHvYfKyUrv+TE0oY9R0rYk1/KmoIy/lkxkj7GDt50PoTfV0sou+x3BIbHWHMQIiKNjKWXwGbMmMHEiRPp168f6enpLFiwgOzsbKZMmQJ4L00dOHCAl156ybdPZmYmAMXFxRw+fJjMzEz8/f3p2rUrAHfeeSfDhg3j8ccfZ+zYsbz99tusWLGC1atXN/jxiZxP/g4bbWNDaBsbUmNdeaWb/cdK2ZmXRtar80k1ctma+TE9hl9jQaUiIo2PpQFowoQJHDlyhEceeYScnBy6d+/O0qVLSUnxvm07JyeH7Ozsavv06dPH9/O6det4+eWXSUlJYc+ePQAMHjyYxYsXc//99/PAAw/Qrl07lixZomcASYsS4GenfVwo7eNCWbe8O6mFuRzf+TkoAImIAI3gSdCNkZ4DJM1J5pJH6b31D3zuTGfAzPetLkdE5Lw5l9/ftgaqSUQsktglHYDk8m8or3RbXI2ISOOgACTSzMV1GkAVNhKMY2Tt3Gp1OSIijYICkEgzZzhD2OvfAYBjWz+2uBoRkcZBAUikBciPSgPAsf9TiysREWkcFIBEWgBbG+9zsFKOfwpVFT+wtYhI86cAJNICXNB3NPlmGPGePEr+cw94PFaXJCJiKQUgkRagVVwsL0b+EoDgjS/Ay+Oh9KjFVYmIWEcBSKSFSB12HTNcUyg3/WBnBiwYDmufg0NbQI8DE5EWRg9CPAU9CFGao0q3h5+/8AVHd33JfL+5pNjyvl0ZkQJdfwx9JkFMBzAM6woVEamlc/n9rQB0CgpA0lyVVFTxyH+28N66b/g/20cMs21mkP0bnLi+3Sg8ybvEdICOo6H9JeBwWle0iMhZUgCqIwUgae62HCzkr6t2s/zrXDyuEobZNnOd/QPSbVtwGpXVtvUERmG7IA3iu0JwrHeJ7gCRKRCst8uLSOOhAFRHCkDSUpS53HzwTR4fb8/js6yjHDpyjBG2jQRTTg/bbkbbvyDBOHbKfU3DhpGcDmGtoO0IaDMUQuLAL7BhD0JE5AQFoDpSAJKW6lBhOVtyCvki6yhf7j1G7tFiYgq/oostm07GPiKNIjoa+4kxCog2ik7ZR1V4CvbE7hgRbSAqFVr1gdjO4Axp2IMRkRZHAaiOFIBEvlVYXsk3OUV8fbCA7YeK2JlXzI68YkLL9jPMtpkE4ygX2zbQ0diPn3Hql62a2CiP7oxfcn8cUW28gajDSLA7GvZgRKRZUwCqIwUgkTMzTZP8Yhe7Dxez63AJuw8Xs/twMUfzcwg+vp0uZBFrHKeHkUU720HijeM1+qiwB1Mc2Q1HfGdCwqOwtx0G0e29l9Tsfg1/UCLS5CkA1ZECkEjtuao8ZB8tZffhYnYeLmbHoWIK8/YSeTSTlMrdtDLyGWHbeNpLaG7DTml4B9xdryas/WBsoXEQGAUhsQ18JCLS1CgA1ZECkEj9M02Tw8UVbM8tZufBfI5nbyE0dy2VxflEu/O51L6eIMpxGlWn3P9YWCeqUi8mot0A/FKHQFUZ+IdCYATY7A17MCLSKCkA1ZECkEjD8XhMDhwvY/uhIrbnFpJ3YDeRB1fStXgtnY29hFFKCGXYjFP/p6oyMBaj/004WqdB6nDwC2jgIxCRxkIBqI4UgESsV+X2sOdIKVtyCtm9J5vArGVEHt9EL883dLLtp8L0q/HMogpbIHlxF2Lv9mPie1yC3eOCyDZ6srVIC6EAVEcKQCKNk2ma7DtaxpY9B8g8VMX2g/kM3P8CXdzbaW87QCuj5gte84LaUxnXk6BhU4lMTVMYEmnGFIDqSAFIpOkwTe8ltK/2Hyf3m0+J2Ps+PYpW0c44WGPbI0SwN6QXZvJgWve5jPh2fcCmd0KLNBcKQHWkACTStLk9Jrv257B9127c2zOIPvwZ/Su/rHHJrJAQdocPxN1mOClt2hLTZRgEhFtUtYjUlQJQHSkAiTQ/BUVF7Nu8moJtHxOc8xkdKr4m2Kiotk0F/mSF9sVsPYDoS6cTFx1tUbUiUhsKQHWkACTS/BWVlLJ9w0oqvn6X4PxNhLtyaWPk+tZXmA42ObpTHNkNW4dLSU0bSVJ0MIbmEIk0WgpAdaQAJNLyFJdXsv3zZRRsX02Xg2+Q4DlUbX2F6aDYCOGT6GuIbNWe5JS2JPW5DJtdzyASaSwUgOpIAUikhTNNivZvJWfd25Tv30y7Ix8SbJbW2Gwf8ewJ6U1h6hUk9LuCHhdE4O/QpGoRqygA1ZECkIhUU1lGec5W8tb/FyPrY0rLykms2E2Y8W0oOmyGE4CLbGd79re+kujW7Wnd62ISoiMtLFykZVEAqiMFIBH5IVXlxRz47A1Kt39ExwNvYcddY5ty049MW1cORfbF1eZiOiaEkNKpLxHhEQ1fsEgLoABURwpAInJOyo5jHvqa/SUGJV/8m+DcLwioyCfWPFJj0yIzkI22roQ7TTwhiRzs9UuS2nahfVwIAX6aTyRSFwpAdaQAJCJ1ZpqUHfiKQ5tWYOz6gKQjq/EADjzVNqsybewwW7PFbMOhoPYYUe2Ij4miVWw0iV0HkxQVgs2mO89EzoYCUB0pAIlIvXNXgWGjdPcnHNn+KQdLHcTs+S/tir887S55ZgQHiWV/UBccofFwQR8iOl5Ih6QEokOcDVi8SNOgAFRHCkAi0mCO7sbM+4bS7R9Rkr8P29GdVFW6iKjIIYCKGpuXmk4+8PShyBFF60AXZngSYZGxGP1vIjUxhrAAPwsOQqRxUACqIwUgEbFc2THcBzI5su8big7uoPLYPmKObySm6tApN883wzhihlFiC+VYYAq5MemYyQNpF+KhXUABgR2GERoS0sAHIdKwFIDqSAFIRBol04QD63FtXcqxgkLKju4n6OhW4sqzfnDX/WYMW2wdsQWGUxbRAXt8V8zYLkQnJNEuNoSYEH895VqaPAWgOlIAEpEmxVUCB9ZRUlbGsYO7qdy/gdbZb2H3VGL73qTr78vyxFOFgxxbHIcC2pEf0gFPbDdCL+hMSoibhNhYLogJJ8TpaKCDEak9BaA6UgASkSbP4wbD5v2zqpyyb1ZwLGcXhUdzsR/eRnjxTmIqD2Lj1L8CPKaBzTA5aoaw1tOVI/Y4wgLsHAnril9YAkZcJ0JiWtMuyp+OraIJ8FdAEus1qQA0b948/vjHP5KTk0O3bt2YO3cuQ4cOPe32H3/8MTNmzODrr7+mVatW/OY3v2HKlCm+9QsXLuTnP/95jf3KysoICAg4q5oUgESkRSg9Crs/wmUP4tjBXVQe3Izz6FbCCrbj9NR89cf3FZsBBODiKGHsNpIo9YtmZ2h/HKGxmNEdCIxrS2KYH/HhISREBBIZ5KfLbHJencvvb0sj+5IlS5g+fTrz5s1jyJAhPP/884wZM4YtW7aQnJxcY/usrCx+9KMf8Ytf/IJ//etffPLJJ9x+++3ExsZyzTXX+LYLCwtj27Zt1fY92/AjItJiBEVB96vxB+K7fKfd44HiQ+AXAAc3UJGzlZK8PZSWFOF3ZCuO8mNElO8jxCgHII7jxHEcKuHiox/BUWCv9441Ox6KCeBt92CyjVZ0dB4l3N/kUFgvnKGReBJ6EREVS0JkKPFhAcSHBeh9atIgLB0BGjhwIH379mX+/Pm+ti5dujBu3DjmzJlTY/t77rmHd955h61bt/rapkyZwsaNG1m7di3gHQGaPn06x48fr3VdGgESEfkBVRVwZCemzUFZ1ucUlpTiOryTkAOrcVdWElG2Fz/Tddbd5ZkRrPZ057AZTolfDKEBdkpDU3FHtCUoKpG2jnzCo2KJSGxHQngQYYEOjSZJDU1iBMjlcrFu3Truvffeau0jR45kzZo1p9xn7dq1jBw5slrbqFGj+Pvf/05lZSV+ft7nXxQXF5OSkoLb7aZ379789re/pU+fPqetpaKigoqKb5+3UVhYWNvDEhFpGRxOiO+GAQTFdiLo++vdVXB0l3cOUv52qvaupTL3G4oCL6C8ooLQw+sxKsuIcOUAEGcc52r7au++JlB2Ysmr3m2Z6c9OsxVr6cmxgGT8g8I4Gt2X+FB/YkOdhMUmEx8RSEJYALGhTvzsGk2SU7MsAOXn5+N2u4mPj6/WHh8fT25u7in3yc3NPeX2VVVV5Ofnk5iYSOfOnVm4cCE9evSgsLCQP/3pTwwZMoSNGzfSoUOHU/Y7Z84cHn744fo5MBERAbsDYjt5f47viqPbOBxA4Pe3q3KBqxhyN2NmraSitBBX/h7K3eBfkEVwSTZ+ngoKbBEEeooJNFz0MPbQgz3gwrsc/7a7fDOMzZ5U1popBOIi2N8gP7AtZaGp2CKTCIxJITYilISwABLCnSRwlJCoRHD4N8BJkcbE8mn73x/CNE3zjMOap9r+u+2DBg1i0KBBvvVDhgyhb9++/PnPf+aZZ545ZZ8zZ85kxowZvs+FhYUkJSWd24GIiMi5c/iDIwraDsdoO5wAIADwXbyoLIfyAsJD472jSsf24Mr+gqpNr+GqrMQoyiG8cDtu7IBJjFHIRfaNXMRG7/4eoOTEkuu9u+0QkRwwYzgGtLdt54gZxhd+/TgY3BV7aAxlsb0Ji4onOirqRFAKICbEiV3vZGtWLAtAMTEx2O32GqM9eXl5NUZ5TkpISDjl9g6Hg+jo6FPuY7PZ6N+/Pzt27DhtLU6nE6dT79UREWl0/AK8C3hHlWLa4x/THv++13172c1Vit0v0Dsv6dDXcHA9Zs5GymxBlFTZMA59hV/RfoJKD+JHBYkcJdE46vuKaKOQ0VUfQMEHUADsB7dp4MKPXDOS1WYHSggizq+c4wGtITiOqtQR+IXFEeK0E+h0EhQSRojTQViAHyEBDkIDHLr81shZFoD8/f1JS0sjIyODq666yteekZHB2LFjT7lPeno6//nPf6q1LV++nH79+vnm/3yfaZpkZmbSo0eP+iteREQaD/8TUcgvAFqnQes079ykE4uPaUJJPhRkw/FsKDxIRVAipXvXUV58HPvRnThK84gq3Y3dMAnERapxiFROvH7EA5SeWA7PrVZCkRlIOX4YQBU2VntSOGpE4Gd4KPGPpswZS4V/FPiH4O/vj7+/H1XBifgFhOAfHE6U0yQo+gIigpyEBjgIC/Qj1OnAplGn88bSS2AzZsxg4sSJ9OvXj/T0dBYsWEB2drbvuT4zZ87kwIEDvPTSS4D3jq9nn32WGTNm8Itf/IK1a9fy97//nUWLFvn6fPjhhxk0aBAdOnSgsLCQZ555hszMTJ577jlLjlFERBoJw4CQWO9yQRoATsDZ6+rq23k8cHgrFOXAgQ14Koooq3RT5PbHfXQP9qO7iCvcVO0hkqFGGaGU+T7H2Y9/21/lieUHFJsB5JvhVAFHMMg046i0BVBkj8DtF0SFXyQh9ipKAlvhCk7ACIrCERxJcEAggYEBBIWEERwaTliAH2EBDkID/Ajws+luudOwNABNmDCBI0eO8Mgjj5CTk0P37t1ZunQpKSkpAOTk5JCdne3bPjU1laVLl3LXXXfx3HPP0apVK5555plqzwA6fvw4t9xyC7m5uYSHh9OnTx9WrlzJgAEDGvz4RESkCbLZIL6bd2l/KTYg+MTi4/GA2+UNVW4XFOV6/8SgqryIyv2ZVJYWUGVCxbGDmMWHcZTlQ1UZuCsxqioIceXh8FTgZ7rwYCPEKPc9WwmgLSemfHiAihMLeC/TnUahGUQFfuSb4RzAnxICKbGHUmkPJsJWRp6zDUVBrQnxg6KIzgQGhRDmtOEMjyMoPJYIPw9hEZGEBfoR6Gdv1vOeLH8SdGOk5wCJiEiD8HigqhzsfnB0N5QdB8DlKseV+w3lrkqqig7jKT0KpUcoNYIJKdiOo7KIANdR/Nxl2M0qHFTVW0n5ZhgFZjDl+FOOE5fNSaURQKUtgCq7E7c9ALc9kCpHMG5HEG6/YDx+Qdjtfvgbbux2BzaHE8PPic3PieEXgN0vALu/E7sjAD8/P/wdNkKCg+jUoVO91Q1N5DlAIiIiLZ7N9u0cpthvw4A/4N9+OCFn249pQkUhFB2CylLM0iOUl5VQXnwcV9ERqkoLKDP98D+yFf/ig7hMg5ji7djMKipxEOwp8l3SizEKiTG+9zw8E3CfWOrJN44ucP+n9dfhOVIAEhERaeoMAwLCvQtg4H3mUo3nLp1CAHgfMVBeAHYH5tEsXKXFVJSXUFVeQmWF9093RQluVykeVxmmqwTjxGKrLMZRVYLpceM2HOBxY/O4sHlc2D0u7J5K7GYlDtOFw6zEZnoAsPlb+4oqBSAREZGWzu6AYO/jZIxWvb2Tw8/zV3Y8z/3/ED2kQERERFocBSARERFpcRSAREREpMVRABIREZEWRwFIREREWhwFIBEREWlxFIBERESkxVEAEhERkRZHAUhERERaHAUgERERaXEUgERERKTFUQASERGRFkcBSERERFocBSARERFpcRxWF9AYmaYJQGFhocWViIiIyNk6+Xv75O/xM1EAOoWioiIAkpKSLK5EREREzlVRURHh4eFn3MYwzyYmtTAej4eDBw8SGhqKYRj12ndhYSFJSUns27ePsLCweu1bvqXz3HB0rhuGznPD0HluOOfjXJumSVFREa1atcJmO/MsH40AnYLNZqN169bn9TvCwsL0L1cD0HluODrXDUPnuWHoPDec+j7XPzTyc5ImQYuIiEiLowAkIiIiLY4CUANzOp089NBDOJ1Oq0tp1nSeG47OdcPQeW4YOs8Nx+pzrUnQIiIi0uJoBEhERERaHAUgERERaXEUgERERKTFUQASERGRFkcBqAHNmzeP1NRUAgICSEtLY9WqVVaX1KSsXLmSK6+8klatWmEYBm+99Va19aZpMnv2bFq1akVgYCAjRozg66+/rrZNRUUFd9xxBzExMQQHB/PjH/+Y/fv3N+BRNH5z5syhf//+hIaGEhcXx7hx49i2bVu1bXSu68f8+fPp2bOn70Fw6enpvPfee771Os/nx5w5czAMg+nTp/vadK7rbvbs2RiGUW1JSEjwrW9059iUBrF48WLTz8/P/Otf/2pu2bLFvPPOO83g4GBz7969VpfWZCxdutScNWuW+frrr5uA+eabb1Zb/9hjj5mhoaHm66+/bm7evNmcMGGCmZiYaBYWFvq2mTJlinnBBReYGRkZ5vr1682LLrrI7NWrl1lVVdXAR9N4jRo1ynzhhRfMr776yszMzDQvv/xyMzk52SwuLvZto3NdP9555x3z3XffNbdt22Zu27bNvO+++0w/Pz/zq6++Mk1T5/l8+Pzzz802bdqYPXv2NO+8805fu8513T300ENmt27dzJycHN+Sl5fnW9/YzrECUAMZMGCAOWXKlGptnTt3Nu+9916LKmravh+APB6PmZCQYD722GO+tvLycjM8PNz8y1/+YpqmaR4/ftz08/MzFy9e7NvmwIEDps1mM99///0Gq72pycvLMwHz448/Nk1T5/p8i4yMNP/2t7/pPJ8HRUVFZocOHcyMjAxz+PDhvgCkc10/HnroIbNXr16nXNcYz7EugTUAl8vFunXrGDlyZLX2kSNHsmbNGouqal6ysrLIzc2tdo6dTifDhw/3neN169ZRWVlZbZtWrVrRvXt3/XM4g4KCAgCioqIAnevzxe12s3jxYkpKSkhPT9d5Pg+mTp3K5ZdfzqWXXlqtXee6/uzYsYNWrVqRmprKtddey+7du4HGeY71MtQGkJ+fj9vtJj4+vlp7fHw8ubm5FlXVvJw8j6c6x3v37vVt4+/vT2RkZI1t9M/h1EzTZMaMGVx44YV0794d0Lmub5s3byY9PZ3y8nJCQkJ488036dq1q+8/+DrP9WPx4sWsX7+eL774osY6/Z2uHwMHDuSll16iY8eOHDp0iN/97ncMHjyYr7/+ulGeYwWgBmQYRrXPpmnWaJO6qc051j+H05s2bRqbNm1i9erVNdbpXNePTp06kZmZyfHjx3n99de54YYb+Pjjj33rdZ7rbt++fdx5550sX76cgICA026nc103Y8aM8f3co0cP0tPTadeuHS+++CKDBg0CGtc51iWwBhATE4Pdbq+RYPPy8mqkYamdk3canOkcJyQk4HK5OHbs2Gm3kW/dcccdvPPOO3z44Ye0bt3a165zXb/8/f1p3749/fr1Y86cOfTq1Ys//elPOs/1aN26deTl5ZGWlobD4cDhcPDxxx/zzDPP4HA4fOdK57p+BQcH06NHD3bs2NEo/z4rADUAf39/0tLSyMjIqNaekZHB4MGDLaqqeUlNTSUhIaHaOXa5XHz88ce+c5yWloafn1+1bXJycvjqq6/0z+E7TNNk2rRpvPHGG3zwwQekpqZWW69zfX6ZpklFRYXOcz265JJL2Lx5M5mZmb6lX79+XH/99WRmZtK2bVud6/OgoqKCrVu3kpiY2Dj/Ptf7tGo5pZO3wf/97383t2zZYk6fPt0MDg429+zZY3VpTUZRUZG5YcMGc8OGDSZgPvXUU+aGDRt8jxJ47LHHzPDwcPONN94wN2/ebF533XWnvMWydevW5ooVK8z169ebF198sW5j/Z7bbrvNDA8PNz/66KNqt7OWlpb6ttG5rh8zZ840V65caWZlZZmbNm0y77vvPtNms5nLly83TVPn+Xz67l1gpqlzXR9+9atfmR999JG5e/du89NPPzWvuOIKMzQ01Pd7rrGdYwWgBvTcc8+ZKSkppr+/v9m3b1/fbcVydj788EMTqLHccMMNpml6b7N86KGHzISEBNPpdJrDhg0zN2/eXK2PsrIyc9q0aWZUVJQZGBhoXnHFFWZ2drYFR9N4neocA+YLL7zg20bnun7cdNNNvv8mxMbGmpdccokv/JimzvP59P0ApHNddyef6+Pn52e2atXKvPrqq82vv/7at76xnWPDNE2z/seVRERERBovzQESERGRFkcBSERERFocBSARERFpcRSAREREpMVRABIREZEWRwFIREREWhwFIBEREWlxFIBERESkxVEAEhE5DcMweOutt6wuQ0TOAwUgEWmUbrzxRgzDqLGMHj3a6tJEpBlwWF2AiMjpjB49mhdeeKFam9PptKgaEWlONAIkIo2W0+kkISGh2hIZGQl4L0/Nnz+fMWPGEBgYSGpqKq+++mq1/Tdv3szFF19MYGAg0dHR3HLLLRQXF1fb5h//+AfdunXD6XSSmJjItGnTqq3Pz8/nqquuIigoiA4dOvDOO+/41h07dozrr7+e2NhYAgMD6dChQ43AJiKNkwKQiDRZDzzwANdccw0bN27kZz/7Gddddx1bt24FoLS0lNGjRxMZGckXX3zBq6++yooVK6oFnPnz5zN16lRuueUWNm/ezDvvvEP79u2rfcfDDz/M+PHj2bRpEz/60Y+4/vrrOXr0qO/7t2zZwnvvvcfWrVuZP38+MTExDXcCRKT2zss75kVE6uiGG24w7Xa7GRwcXG155JFHTNM0TcCcMmVKtX0GDhxo3nbbbaZpmuaCBQvMyMhIs7i42Lf+3XffNW02m5mbm2uapmm2atXKnDVr1mlrAMz777/f97m4uNg0DMN87733TNM0zSuvvNL8+c9/Xj8HLCINSnOARKTRuuiii5g/f361tqioKN/P6enp1dalp6eTmZkJwNatW+nVqxfBwcG+9UOGDMHj8bBt2zYMw+DgwYNccsklZ6yhZ8+evp+Dg4MJDQ0lLy8PgNtuu41rrrmG9evXM3LkSMaNG8fgwYNrdawi0rAUgESk0QoODq5xSeqHGIYBgGmavp9PtU1gYOBZ9efn51djX4/HA8CYMWPYu3cv7777LitWrOCSSy5h6tSpPPHEE+dUs4g0PM0BEpEm69NPP63xuXPnzgB07dqVzMxMSkpKfOs/+eQTbDYbHTt2JDQ0lDZt2vC///2vTjXExsZy44038q9//Yu5c+eyYMGCOvUnIg1DI0Ai0mhVVFSQm5tbrc3hcPgmGr/66qv069ePCy+8kH//+998/vnn/P3vfwfg+uuv56GHHuKGG25g9uzZHD58mDvuuIOJEycSHx8PwOzZs5kyZQpxcXGMGTOGoqIiPvnkE+64446zqu/BBx8kLS2Nbt26UVFRwX//+1+6dOlSj2dARM4XBSARabTef/99EhMTq7V16tSJb775BvDeobV48WJuv/12EhIS+Pe//03Xrl0BCAoKYtmyZdx5553079+foKAgrrnmGp566ilfXzfccAPl5eU8/fTT3H333cTExPCTn/zkrOvz9/dn5syZ7Nmzh8DAQIYOHcrixYvr4chF5HwzTNM0rS5CRORcGYbBm2++ybhx46wuRUSaIM0BEhERkRZHAUhERERaHM0BEpEmSVfvRaQuNAIkIiIiLY4CkIiIiLQ4CkAiIiLS4igAiYiISIujACQiIiItjgKQiIiItDgKQCIiItLiKACJiIhIi/P/ZSEXqLwuu0IAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear layer input: (None, 32)\n",
      "linear layer output: (None, 16)\n",
      "7/7 [==============================] - 1s 4ms/step\n",
      "Comparison of predictions and ground truth:\n",
      "Sample 1:\n",
      "  Predicted:    [1. 0. 2. 1. 1. 1. 3. 0. 1. 3. 2. 2. 1. 0. 4. 2.]\n",
      "  Ground Truth: [1. 0. 1. 1. 1. 1. 3. 0. 1. 3. 1. 2. 1. 1. 4. 2.]\n",
      "----------------------------------------\n",
      "Sample 2:\n",
      "  Predicted:    [4. 0. 2. 1. 2. 0. 3. 1. 1. 2. 2. 1. 3. 0. 1. 0.]\n",
      "  Ground Truth: [4. 0. 2. 0. 2. 0. 3. 2. 1. 2. 2. 2. 3. 0. 1. 0.]\n",
      "----------------------------------------\n",
      "Sample 3:\n",
      "  Predicted:    [3. 2. 1. 1. 2. 0. 4. 4. 3. 3. 2. 1. 4. 3. 1. 1.]\n",
      "  Ground Truth: [3. 2. 0. 0. 2. 0. 4. 4. 3. 3. 1. 3. 4. 3. 0. 1.]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = model.predict([X_real_test, X_imag_test])\n",
    "y_pred_test_rescaled = y_pred_test * (q - 1)\n",
    "# y_pred_test_rescaled = (y_pred_test * y_std) + y_mean\n",
    "y_test_rescaled = y_test * (q - 1)\n",
    "\n",
    "print(\"Comparison of predictions and ground truth:\")\n",
    "for i in range(3):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  Predicted:    {np.round(y_pred_test_rescaled[i])}\")\n",
    "    print(f\"  Ground Truth: {np.round(y_test_rescaled[i])}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoded Unseen Data (Real and Imaginary Parts):\n",
      "Real Part Shape: (5, 16)\n",
      "Imaginary Part Shape: (5, 16)\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "\n",
      "Comparison of Unseen Data and Predictions:\n",
      "============================================================\n",
      "Sample 1:\n",
      "  Ground Truth: [2 3 2 1 0 0 0 4 4 1 2 4 2 2 4 4]\n",
      "  Prediction  : [2 3 3 2 0 0 0 4 4 1 2 2 2 2 4 4]\n",
      "------------------------------------------------------------\n",
      "Sample 2:\n",
      "  Ground Truth: [1 3 0 4 4 2 3 4 3 0 2 1 0 3 3 0]\n",
      "  Prediction  : [1 3 1 4 4 1 3 4 3 1 2 1 1 3 3 0]\n",
      "------------------------------------------------------------\n",
      "Sample 3:\n",
      "  Ground Truth: [3 1 1 3 3 1 3 4 2 4 3 3 2 3 2 1]\n",
      "  Prediction  : [3 1 1 3 3 2 3 4 2 4 3 2 2 2 2 1]\n",
      "------------------------------------------------------------\n",
      "Sample 4:\n",
      "  Ground Truth: [2 2 4 3 1 0 2 0 3 2 0 4 0 4 1 3]\n",
      "  Prediction  : [2 2 3 3 1 0 2 0 3 2 1 2 0 4 1 2]\n",
      "------------------------------------------------------------\n",
      "Sample 5:\n",
      "  Ground Truth: [0 2 3 0 2 2 2 2 0 1 4 4 4 1 2 2]\n",
      "  Prediction  : [0 2 2 1 2 2 2 1 0 1 3 3 4 1 2 2]\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_unseen_samples = 5\n",
    "unseen_data = np.random.randint(0, q, size=(num_unseen_samples, n))\n",
    "\n",
    "unseen_data_normalized = unseen_data.astype(np.float32) / (q - 1)\n",
    "\n",
    "# print(\"Unseen Input Data (Before Encoding):\")\n",
    "# print(unseen_data)\n",
    "\n",
    "unseen_encoded = np.array([dft(message, n) for message in unseen_data])\n",
    "unseen_encoded[np.abs(unseen_encoded) < 1e-10] = 0\n",
    "unseen_encoded = np.round(unseen_encoded, decimals=10)\n",
    "\n",
    "X_real_unseen = np.real(unseen_encoded).astype(np.float32)\n",
    "X_imag_unseen = np.imag(unseen_encoded).astype(np.float32)\n",
    "\n",
    "print(\"\\nEncoded Unseen Data (Real and Imaginary Parts):\")\n",
    "print(\"Real Part Shape:\", X_real_unseen.shape)\n",
    "print(\"Imaginary Part Shape:\", X_imag_unseen.shape)\n",
    "\n",
    "y_pred_unseen = model.predict([X_real_unseen, X_imag_unseen])\n",
    "\n",
    "y_pred_unseen_rescaled = y_pred_unseen * (q - 1)\n",
    "\n",
    "y_pred_unseen_final = np.mod(np.round(y_pred_unseen_rescaled), q)\n",
    "\n",
    "print(\"\\nComparison of Unseen Data and Predictions:\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(num_unseen_samples):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  Ground Truth: {unseen_data[i]}\")\n",
    "    print(f\"  Prediction  : {y_pred_unseen_final[i].astype(int)}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check gradient flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real_input has no trainable variables.\n",
      "imag_input has no trainable variables.\n",
      "real_layer1 Gradient Mean: 0.5921713\n",
      "imag_layer1 Gradient Mean: 0.45857573\n",
      "leaky_re_lu has no trainable variables.\n",
      "leaky_re_lu_3 has no trainable variables.\n",
      "real_support_layer_1 Gradient Mean: 1.7383046\n",
      "imag_support_layer_1 Gradient Mean: 2.430495\n",
      "leaky_re_lu_1 has no trainable variables.\n",
      "leaky_re_lu_4 has no trainable variables.\n",
      "real_layer2 Gradient Mean: 1.3402917\n",
      "imag_layer2 Gradient Mean: 1.4488969\n",
      "leaky_re_lu_2 has no trainable variables.\n",
      "leaky_re_lu_5 has no trainable variables.\n",
      "merge_real_imag has no trainable variables.\n",
      "output_layer Gradient Mean: 2.0532532\n",
      "activation has no trainable variables.\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    if layer.trainable_variables:\n",
    "        grads = tf.reduce_mean(tf.abs(layer.trainable_variables[0]))\n",
    "        print(layer.name, \"Gradient Mean:\", grads.numpy())\n",
    "    else:\n",
    "        print(layer.name, \"has no trainable variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear layer input: (4, 32)\n",
      "linear layer output: (4, 16)\n",
      " Gradient OK for real_layer1/kernel_b1:0, mean: -0.0009824737207964063\n",
      " Gradient OK for real_layer1/kernel_b2:0, mean: -0.0006573835271410644\n",
      " Gradient OK for real_layer1/kernel_d1:0, mean: -0.0021592092234641314\n",
      " Gradient OK for real_layer1/kernel_d2:0, mean: -0.0006244988762773573\n",
      " Gradient OK for real_layer1/bias:0, mean: -0.0009485393529757857\n",
      " Gradient OK for imag_layer1/kernel_b1:0, mean: -0.0068637914955616\n",
      " Gradient OK for imag_layer1/kernel_b2:0, mean: -0.0009827979374676943\n",
      " Gradient OK for imag_layer1/kernel_d1:0, mean: -0.006361318286508322\n",
      " Gradient OK for imag_layer1/kernel_d2:0, mean: 0.00047864290536381304\n",
      " Gradient OK for imag_layer1/bias:0, mean: -0.01564641483128071\n",
      " Gradient OK for real_support_layer_1/kernel_m:0, mean: -0.009587676264345646\n",
      " Gradient OK for real_support_layer_1/bias:0, mean: -0.0012309917947277427\n",
      " Gradient OK for imag_support_layer_1/kernel_m:0, mean: -0.02593168243765831\n",
      " Gradient OK for imag_support_layer_1/bias:0, mean: -0.017101917415857315\n",
      " Gradient OK for real_layer2/kernel_d1:0, mean: 0.0014626577030867338\n",
      " Gradient OK for real_layer2/bias:0, mean: -0.003971002530306578\n",
      " Gradient OK for imag_layer2/kernel_d1:0, mean: 0.009801950305700302\n",
      " Gradient OK for imag_layer2/bias:0, mean: -0.022591670975089073\n",
      " Gradient OK for output_layer/kernel_m_1:0, mean: -0.025708334520459175\n",
      " Gradient OK for output_layer/bias:0, mean: -0.0203692764043808\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4 \n",
    "X_real_dummy = np.random.rand(batch_size, X_real_train.shape[1]).astype(np.float32)\n",
    "X_imag_dummy = np.random.rand(batch_size, X_imag_train.shape[1]).astype(np.float32)\n",
    "y_dummy = np.random.rand(batch_size, y_train.shape[1]).astype(np.float32)\n",
    "\n",
    "def check_gradients(model, X_real_dummy, X_imag_dummy, y_dummy):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(model.trainable_variables)\n",
    "        predictions = model([X_real_dummy, X_imag_dummy], training=True)\n",
    "        loss = tf.keras.losses.mean_squared_error(y_dummy, predictions)\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    for i, (grad, var) in enumerate(zip(grads, model.trainable_variables)):\n",
    "        if grad is None:\n",
    "            print(f\" Gradient is None for {var.name} at index {i}\")\n",
    "        else:\n",
    "            print(f\" Gradient OK for {var.name}, mean: {tf.reduce_mean(grad).numpy()}\")\n",
    "\n",
    "check_gradients(model, X_real_dummy, X_imag_dummy, y_dummy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
