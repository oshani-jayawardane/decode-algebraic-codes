{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN (Padded Generator matrix encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset -f\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original n: 16\n",
      "Padded n: 16\n",
      "Generated dataset shape: (1000, 16)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n = 16\n",
    "q = 5\n",
    "num_samples = 1000\n",
    "\n",
    "def next_power_of_two(x):\n",
    "    return 1 if x == 0 else 2**(x - 1).bit_length()\n",
    "\n",
    "n_padded = next_power_of_two(n)\n",
    "\n",
    "dataset = np.random.randint(0, q, size=(num_samples, n))\n",
    "\n",
    "if n_padded > n:\n",
    "    pad_width = n_padded - n\n",
    "    dataset = np.pad(dataset, ((0, 0), (0, pad_width)), mode='constant', constant_values=0)\n",
    "\n",
    "print(\"Original n:\", n)\n",
    "print(\"Padded n:\", n_padded)\n",
    "print(\"Generated dataset shape:\", dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 4 2 ... 1 3 1]\n",
      " [3 4 0 ... 3 2 3]\n",
      " [3 0 2 ... 0 1 4]\n",
      " ...\n",
      " [4 4 4 ... 0 2 2]\n",
      " [4 2 4 ... 4 0 0]\n",
      " [2 2 2 ... 0 4 2]]\n",
      "(1000, 16)\n",
      "(16,)\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(dataset.shape)\n",
    "print(dataset[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\tilde{M}_{kj} = \\left[ \\left( \\frac{w_0}{z_0} \\right)^j \\zeta^{kj} \\right]_{k,j=0}^{n-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padded_generator_matrix(n, w0, z0):\n",
    "    zeta = np.exp(-2j * np.pi / n)\n",
    "    M_tilde = np.array([[(w0 / z0) ** j * zeta**(k * j) for j in range(n)] for k in range(n)], dtype=complex)\n",
    "    return M_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 16)\n"
     ]
    }
   ],
   "source": [
    "# (x, y, z, w) --> (1, 2, 3, 4)\n",
    "w0 = 4\n",
    "z0 = 3\n",
    "\n",
    "M_tilde = padded_generator_matrix(n_padded, w0, z0)\n",
    "print(M_tilde.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 649.89449698  +0.j           69.97998169+371.13335899j\n",
      "   -83.92481787+137.40000024j ...  -88.37954911-103.22398682j\n",
      "   -83.92481787-137.40000024j   69.97998169-371.13335899j]\n",
      " [ 695.87003951  +0.j          268.59479107+394.57072111j\n",
      "   -36.50274968+309.16102475j ...  -47.36788425-179.88976988j\n",
      "   -36.50274968-309.16102475j  268.59479107-394.57072111j]\n",
      " [ 540.63797786  +0.j          235.29809241+209.31041098j\n",
      "   176.66145652+204.71795948j ...  108.87153914-320.22828928j\n",
      "   176.66145652-204.71795948j  235.29809241-209.31041098j]\n",
      " ...\n",
      " [ 460.45123158  +0.j          162.64910275+168.11969395j\n",
      "    54.45039471+219.81284778j ...  -26.91067254-124.72256855j\n",
      "    54.45039471-219.81284778j  162.64910275-168.11969395j]\n",
      " [ 558.17613119  +0.j          -92.41727316+362.85897707j\n",
      "  -228.64175513 +13.85662093j ...  -76.51498185+176.39278765j\n",
      "  -228.64175513 -13.85662093j  -92.41727316-362.85897707j]\n",
      " [ 671.99178181  +0.j          161.97417378+352.04621278j\n",
      "    43.09272997+244.01831359j ...  -56.20864468-237.97753496j\n",
      "    43.09272997-244.01831359j  161.97417378-352.04621278j]]\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset = np.array([np.dot(M_tilde, x) for x in dataset])\n",
    "encoded_dataset[np.abs(encoded_dataset) < 1e-10] = 0\n",
    "encoded_dataset = np.round(encoded_dataset, decimals=10)\n",
    "print(encoded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 16)\n"
     ]
    }
   ],
   "source": [
    "print(encoded_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 2 4 4 1 2 2 2 4 3 2 4 1 3 1]\n",
      "[ 649.89449698  +0.j           69.97998169+371.13335899j\n",
      "  -83.92481787+137.40000024j  -88.37954911+103.22398682j\n",
      "  -74.53901813 +41.73652084j  -97.48254893 +37.96448017j\n",
      " -147.95858217-108.15626408j   47.97234649-148.66691513j\n",
      "  146.76987906  -0.j           47.97234649+148.66691513j\n",
      " -147.95858217+108.15626408j  -97.48254893 -37.96448017j\n",
      "  -74.53901813 -41.73652084j  -88.37954911-103.22398682j\n",
      "  -83.92481787-137.40000024j   69.97998169-371.13335899j]\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])\n",
    "print(encoded_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train_real : (1000, 16)\n",
      "[ 649.8945     69.97998   -83.92482   -88.37955   -74.53902   -97.48255\n",
      " -147.95859    47.972347  146.76988    47.972347 -147.95859   -97.48255\n",
      "  -74.53902   -88.37955   -83.92482    69.97998 ]\n",
      "\n",
      "X_train_imag : (1000, 16)\n",
      "[   0.        371.13336   137.4       103.22398    41.736523   37.96448\n",
      " -108.156265 -148.66692    -0.        148.66692   108.156265  -37.96448\n",
      "  -41.736523 -103.22398  -137.4      -371.13336 ]\n",
      "\n",
      "X_train : (1000, 32)\n",
      "[ 649.8945     69.97998   -83.92482   -88.37955   -74.53902   -97.48255\n",
      " -147.95859    47.972347  146.76988    47.972347 -147.95859   -97.48255\n",
      "  -74.53902   -88.37955   -83.92482    69.97998     0.        371.13336\n",
      "  137.4       103.22398    41.736523   37.96448  -108.156265 -148.66692\n",
      "   -0.        148.66692   108.156265  -37.96448   -41.736523 -103.22398\n",
      " -137.4      -371.13336 ]\n"
     ]
    }
   ],
   "source": [
    "####### Method 1 ########\n",
    "\n",
    "# Stack real/imaginary parts\n",
    "X_real = np.real(encoded_dataset).astype(np.float32)\n",
    "X_imag = np.imag(encoded_dataset).astype(np.float32)\n",
    "X_real_imag = np.hstack([X_real, X_imag])\n",
    "\n",
    "print(\"\\nX_train_real :\", X_real.shape)\n",
    "print(X_real[0])\n",
    "print(\"\\nX_train_imag :\", X_imag.shape)\n",
    "print(X_imag[0])\n",
    "print(\"\\nX_train :\", X_real_imag.shape)\n",
    "print(X_real_imag[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # normalize x values before feeding to the model cause they are a bit large\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_real_imag = scaler.fit_transform(X_real_imag)\n",
    "\n",
    "# print(\"\\nX_train after scaling:\", X_real_imag.shape)\n",
    "# print(X_real_imag[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: \n",
      "[3. 4. 2. 4. 4. 1. 2. 2. 2. 4. 3. 2. 4. 1. 3. 1.]\n",
      "\n",
      "y_normalized: \n",
      "[0.75 1.   0.5  1.   1.   0.25 0.5  0.5  0.5  1.   0.75 0.5  1.   0.25\n",
      " 0.75 0.25]\n"
     ]
    }
   ],
   "source": [
    "# normalize target data (integers 0-q to [0, 1])\n",
    "y_normalized = dataset.astype(np.float32) / (q - 1)  # Scale to [0, 1]\n",
    "print(\"y: \")\n",
    "print(dataset.astype(np.float32)[0])\n",
    "print(\"\\ny_normalized: \")\n",
    "print(y_normalized[0])\n",
    "\n",
    "labels = y_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (800, 32) (800, 16)\n",
      "Testing data shape: (200, 32) (200, 16)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_real_imag, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "print(\"Training data shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing data shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "output_dim = y_train.shape[1]\n",
    "print(input_dim)\n",
    "print(output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3696 (14.44 KB)\n",
      "Trainable params: 3696 (14.44 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber\n",
    "\n",
    "def cosine_loss(y_true, y_pred):\n",
    "    y_true_norm = tf.nn.l2_normalize(y_true, axis=-1)\n",
    "    y_pred_norm = tf.nn.l2_normalize(y_pred, axis=-1)\n",
    "    return 1 - tf.reduce_mean(tf.reduce_sum(y_true_norm * y_pred_norm, axis=-1))\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    return mse\n",
    "\n",
    "def hybrid_loss(y_true, y_pred):\n",
    "    mse_part = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    cos_part = cosine_loss(y_true, y_pred)\n",
    "    return 0.5 * mse_part + 0.5 * cos_part\n",
    "\n",
    "\n",
    "def simple_NN(input_dim, output_dim, activation='relu'):\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_dim,)),\n",
    "        Dense(n*2, activation=activation),\n",
    "        Dense(n*2, activation=activation),\n",
    "        Dense(n*2, activation=activation),\n",
    "        # Dense(n*2, activation=activation),\n",
    "        Dense(output_dim, activation='linear')  # Linear activation for regression\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=hybrid_loss, #'mean_squared_error', # Huber(delta=1.0)\n",
    "        metrics=['mse', 'mae']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = y_train.shape[1]\n",
    "model = simple_NN(input_dim, output_dim, activation='leaky_relu')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense (None, 32)\n",
      "dense_1 (None, 32)\n",
      "dense_2 (None, 32)\n",
      "dense_3 (None, 16)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name, layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "adjust_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',  # You can change this to 'loss' if you prefer training loss\n",
    "    factor=0.5,          # Reduce learning rate by this factor\n",
    "    patience=5,          # Number of epochs with no improvement before reducing LR\n",
    "    min_lr=1e-7,         # Minimum learning rate\n",
    "    verbose=1            # Print updates\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "50/50 [==============================] - 4s 24ms/step - loss: 1240.6930 - mse: 2480.3491 - mae: 30.2635 - val_loss: 125.3505 - val_mse: 249.7336 - val_mae: 12.2976 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 77.9744 - mse: 154.9789 - mae: 9.6802 - val_loss: 64.2800 - val_mse: 127.5536 - val_mae: 8.7862 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 43.7171 - mse: 86.4680 - mae: 7.2456 - val_loss: 40.5721 - val_mse: 80.1813 - val_mae: 6.9647 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 29.1327 - mse: 57.3216 - mae: 5.8941 - val_loss: 28.3215 - val_mse: 55.6853 - val_mae: 5.7923 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 21.3288 - mse: 41.7244 - mae: 5.0369 - val_loss: 21.5095 - val_mse: 42.0764 - val_mae: 5.0183 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 16.5938 - mse: 32.2761 - mae: 4.4245 - val_loss: 17.1309 - val_mse: 33.3506 - val_mae: 4.4615 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 13.4124 - mse: 25.9246 - mae: 3.9642 - val_loss: 14.2766 - val_mse: 27.6493 - val_mae: 4.0614 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 11.2537 - mse: 21.6212 - mae: 3.6128 - val_loss: 11.9247 - val_mse: 22.9418 - val_mae: 3.7097 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 9.6094 - mse: 18.3447 - mae: 3.3279 - val_loss: 10.2370 - val_mse: 19.5629 - val_mae: 3.4366 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 8.2577 - mse: 15.6508 - mae: 3.0794 - val_loss: 8.9155 - val_mse: 16.9574 - val_mae: 3.1892 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 7.1948 - mse: 13.5498 - mae: 2.8574 - val_loss: 7.7055 - val_mse: 14.5150 - val_mae: 2.9522 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 6.3460 - mse: 11.8654 - mae: 2.6762 - val_loss: 6.7894 - val_mse: 12.7049 - val_mae: 2.7515 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 5.5803 - mse: 10.3422 - mae: 2.4923 - val_loss: 5.9932 - val_mse: 11.1768 - val_mae: 2.5662 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 4.9557 - mse: 9.1219 - mae: 2.3343 - val_loss: 5.3449 - val_mse: 9.8437 - val_mae: 2.4170 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 4.4553 - mse: 8.1198 - mae: 2.1992 - val_loss: 4.7519 - val_mse: 8.7359 - val_mae: 2.2624 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 4.0259 - mse: 7.2837 - mae: 2.0794 - val_loss: 4.3230 - val_mse: 7.8349 - val_mae: 2.1464 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 3.7062 - mse: 6.6483 - mae: 1.9822 - val_loss: 3.9773 - val_mse: 7.2003 - val_mae: 2.0563 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 3.4038 - mse: 6.0619 - mae: 1.8928 - val_loss: 3.6195 - val_mse: 6.4564 - val_mae: 1.9455 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 3.1975 - mse: 5.6560 - mae: 1.8283 - val_loss: 3.3106 - val_mse: 5.8823 - val_mae: 1.8588 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.9697 - mse: 5.2156 - mae: 1.7530 - val_loss: 3.0818 - val_mse: 5.4402 - val_mae: 1.7911 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 2.7822 - mse: 4.8508 - mae: 1.6917 - val_loss: 2.8698 - val_mse: 5.0312 - val_mae: 1.7231 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 2.5984 - mse: 4.4988 - mae: 1.6281 - val_loss: 2.7238 - val_mse: 4.7343 - val_mae: 1.6767 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 2.4840 - mse: 4.2732 - mae: 1.5894 - val_loss: 2.5765 - val_mse: 4.4268 - val_mae: 1.6233 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 2.3238 - mse: 3.9644 - mae: 1.5291 - val_loss: 2.4232 - val_mse: 4.1748 - val_mae: 1.5778 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 2.1959 - mse: 3.7304 - mae: 1.4829 - val_loss: 2.2977 - val_mse: 3.9017 - val_mae: 1.5287 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 2.0606 - mse: 3.4603 - mae: 1.4281 - val_loss: 2.1618 - val_mse: 3.6655 - val_mae: 1.4809 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 1.9679 - mse: 3.2894 - mae: 1.3944 - val_loss: 2.0307 - val_mse: 3.3979 - val_mae: 1.4185 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 1.8652 - mse: 3.0931 - mae: 1.3512 - val_loss: 1.9406 - val_mse: 3.2717 - val_mae: 1.3909 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1.7594 - mse: 2.8977 - mae: 1.3084 - val_loss: 1.8281 - val_mse: 3.0296 - val_mae: 1.3439 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 1.6689 - mse: 2.7295 - mae: 1.2713 - val_loss: 1.7329 - val_mse: 2.8357 - val_mae: 1.2978 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 1.5839 - mse: 2.5631 - mae: 1.2332 - val_loss: 1.6599 - val_mse: 2.7131 - val_mae: 1.2683 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 1.4950 - mse: 2.4029 - mae: 1.1929 - val_loss: 1.5678 - val_mse: 2.5421 - val_mae: 1.2351 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 1.3991 - mse: 2.2301 - mae: 1.1504 - val_loss: 1.4743 - val_mse: 2.3367 - val_mae: 1.1773 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 1.2868 - mse: 2.0225 - mae: 1.0987 - val_loss: 1.3705 - val_mse: 2.1299 - val_mae: 1.1227 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 1.1832 - mse: 1.8272 - mae: 1.0487 - val_loss: 1.3117 - val_mse: 2.0536 - val_mae: 1.1034 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 1.1274 - mse: 1.7235 - mae: 1.0188 - val_loss: 1.2106 - val_mse: 1.8930 - val_mae: 1.0621 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 1.0696 - mse: 1.6199 - mae: 0.9916 - val_loss: 1.1673 - val_mse: 1.7759 - val_mae: 1.0267 - lr: 0.0010\n",
      "Epoch 38/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 1.0279 - mse: 1.5422 - mae: 0.9676 - val_loss: 1.1151 - val_mse: 1.7085 - val_mae: 1.0088 - lr: 0.0010\n",
      "Epoch 39/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.9855 - mse: 1.4681 - mae: 0.9456 - val_loss: 1.0601 - val_mse: 1.5920 - val_mae: 0.9744 - lr: 0.0010\n",
      "Epoch 40/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.9361 - mse: 1.3829 - mae: 0.9183 - val_loss: 1.0327 - val_mse: 1.5488 - val_mae: 0.9601 - lr: 0.0010\n",
      "Epoch 41/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.9042 - mse: 1.3257 - mae: 0.8998 - val_loss: 1.0034 - val_mse: 1.4745 - val_mae: 0.9355 - lr: 0.0010\n",
      "Epoch 42/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.8710 - mse: 1.2661 - mae: 0.8799 - val_loss: 0.9408 - val_mse: 1.4087 - val_mae: 0.9167 - lr: 0.0010\n",
      "Epoch 43/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.8435 - mse: 1.2154 - mae: 0.8608 - val_loss: 0.9158 - val_mse: 1.3735 - val_mae: 0.9037 - lr: 0.0010\n",
      "Epoch 44/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.8065 - mse: 1.1608 - mae: 0.8424 - val_loss: 0.8892 - val_mse: 1.2900 - val_mae: 0.8752 - lr: 0.0010\n",
      "Epoch 45/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.7812 - mse: 1.1133 - mae: 0.8245 - val_loss: 0.8648 - val_mse: 1.2690 - val_mae: 0.8680 - lr: 0.0010\n",
      "Epoch 46/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.7547 - mse: 1.0673 - mae: 0.8067 - val_loss: 0.8400 - val_mse: 1.2030 - val_mae: 0.8445 - lr: 0.0010\n",
      "Epoch 47/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.7241 - mse: 1.0164 - mae: 0.7871 - val_loss: 0.8036 - val_mse: 1.1462 - val_mae: 0.8270 - lr: 0.0010\n",
      "Epoch 48/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.7097 - mse: 0.9946 - mae: 0.7795 - val_loss: 0.7708 - val_mse: 1.1076 - val_mae: 0.8123 - lr: 0.0010\n",
      "Epoch 49/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.6817 - mse: 0.9478 - mae: 0.7598 - val_loss: 0.7610 - val_mse: 1.0797 - val_mae: 0.7993 - lr: 0.0010\n",
      "Epoch 50/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.6565 - mse: 0.9039 - mae: 0.7421 - val_loss: 0.7225 - val_mse: 1.0378 - val_mae: 0.7860 - lr: 0.0010\n",
      "Epoch 51/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.6405 - mse: 0.8800 - mae: 0.7321 - val_loss: 0.6982 - val_mse: 1.0012 - val_mae: 0.7709 - lr: 0.0010\n",
      "Epoch 52/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.6191 - mse: 0.8428 - mae: 0.7173 - val_loss: 0.6821 - val_mse: 0.9498 - val_mae: 0.7495 - lr: 0.0010\n",
      "Epoch 53/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.5987 - mse: 0.8131 - mae: 0.7032 - val_loss: 0.6774 - val_mse: 0.9353 - val_mae: 0.7460 - lr: 0.0010\n",
      "Epoch 54/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.5821 - mse: 0.7836 - mae: 0.6902 - val_loss: 0.6545 - val_mse: 0.8931 - val_mae: 0.7301 - lr: 0.0010\n",
      "Epoch 55/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.5672 - mse: 0.7602 - mae: 0.6798 - val_loss: 0.6351 - val_mse: 0.8542 - val_mae: 0.7105 - lr: 0.0010\n",
      "Epoch 56/500\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.5501 - mse: 0.7343 - mae: 0.6689 - val_loss: 0.6134 - val_mse: 0.8288 - val_mae: 0.7020 - lr: 0.0010\n",
      "Epoch 57/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.5258 - mse: 0.6948 - mae: 0.6487 - val_loss: 0.5831 - val_mse: 0.8031 - val_mae: 0.6891 - lr: 0.0010\n",
      "Epoch 58/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.5120 - mse: 0.6713 - mae: 0.6393 - val_loss: 0.5768 - val_mse: 0.7915 - val_mae: 0.6836 - lr: 0.0010\n",
      "Epoch 59/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.4997 - mse: 0.6567 - mae: 0.6311 - val_loss: 0.5630 - val_mse: 0.7394 - val_mae: 0.6629 - lr: 0.0010\n",
      "Epoch 60/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.4858 - mse: 0.6291 - mae: 0.6185 - val_loss: 0.5570 - val_mse: 0.7370 - val_mae: 0.6595 - lr: 0.0010\n",
      "Epoch 61/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.4753 - mse: 0.6156 - mae: 0.6105 - val_loss: 0.5318 - val_mse: 0.7231 - val_mae: 0.6514 - lr: 0.0010\n",
      "Epoch 62/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.4549 - mse: 0.5849 - mae: 0.5943 - val_loss: 0.5252 - val_mse: 0.6877 - val_mae: 0.6397 - lr: 0.0010\n",
      "Epoch 63/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.4423 - mse: 0.5658 - mae: 0.5844 - val_loss: 0.4945 - val_mse: 0.6503 - val_mae: 0.6179 - lr: 0.0010\n",
      "Epoch 64/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.4272 - mse: 0.5423 - mae: 0.5711 - val_loss: 0.4860 - val_mse: 0.6535 - val_mae: 0.6199 - lr: 0.0010\n",
      "Epoch 65/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.4123 - mse: 0.5180 - mae: 0.5590 - val_loss: 0.4667 - val_mse: 0.6105 - val_mae: 0.5989 - lr: 0.0010\n",
      "Epoch 66/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.4003 - mse: 0.5028 - mae: 0.5492 - val_loss: 0.4471 - val_mse: 0.5931 - val_mae: 0.5882 - lr: 0.0010\n",
      "Epoch 67/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.3907 - mse: 0.4871 - mae: 0.5421 - val_loss: 0.4352 - val_mse: 0.5695 - val_mae: 0.5770 - lr: 0.0010\n",
      "Epoch 68/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.3772 - mse: 0.4664 - mae: 0.5294 - val_loss: 0.4231 - val_mse: 0.5345 - val_mae: 0.5559 - lr: 0.0010\n",
      "Epoch 69/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.3644 - mse: 0.4460 - mae: 0.5159 - val_loss: 0.4112 - val_mse: 0.5407 - val_mae: 0.5604 - lr: 0.0010\n",
      "Epoch 70/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.3511 - mse: 0.4282 - mae: 0.5060 - val_loss: 0.3981 - val_mse: 0.5075 - val_mae: 0.5418 - lr: 0.0010\n",
      "Epoch 71/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.3391 - mse: 0.4105 - mae: 0.4949 - val_loss: 0.3820 - val_mse: 0.4886 - val_mae: 0.5289 - lr: 0.0010\n",
      "Epoch 72/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.3284 - mse: 0.3939 - mae: 0.4839 - val_loss: 0.3724 - val_mse: 0.4669 - val_mae: 0.5180 - lr: 0.0010\n",
      "Epoch 73/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.3225 - mse: 0.3873 - mae: 0.4816 - val_loss: 0.3764 - val_mse: 0.4862 - val_mae: 0.5306 - lr: 0.0010\n",
      "Epoch 74/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.3127 - mse: 0.3723 - mae: 0.4722 - val_loss: 0.3493 - val_mse: 0.4276 - val_mae: 0.4964 - lr: 0.0010\n",
      "Epoch 75/500\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.2992 - mse: 0.3528 - mae: 0.4592 - val_loss: 0.3342 - val_mse: 0.4027 - val_mae: 0.4790 - lr: 0.0010\n",
      "Epoch 76/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.2886 - mse: 0.3375 - mae: 0.4501 - val_loss: 0.3294 - val_mse: 0.4118 - val_mae: 0.4886 - lr: 0.0010\n",
      "Epoch 77/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.2776 - mse: 0.3208 - mae: 0.4382 - val_loss: 0.3155 - val_mse: 0.3856 - val_mae: 0.4686 - lr: 0.0010\n",
      "Epoch 78/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.2733 - mse: 0.3176 - mae: 0.4356 - val_loss: 0.3081 - val_mse: 0.3681 - val_mae: 0.4554 - lr: 0.0010\n",
      "Epoch 79/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.2595 - mse: 0.2958 - mae: 0.4219 - val_loss: 0.2893 - val_mse: 0.3479 - val_mae: 0.4448 - lr: 0.0010\n",
      "Epoch 80/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2536 - mse: 0.2897 - mae: 0.4160 - val_loss: 0.2873 - val_mse: 0.3374 - val_mae: 0.4403 - lr: 0.0010\n",
      "Epoch 81/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.2438 - mse: 0.2749 - mae: 0.4057 - val_loss: 0.2703 - val_mse: 0.3236 - val_mae: 0.4290 - lr: 0.0010\n",
      "Epoch 82/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.2369 - mse: 0.2655 - mae: 0.3993 - val_loss: 0.2641 - val_mse: 0.3058 - val_mae: 0.4193 - lr: 0.0010\n",
      "Epoch 83/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.2279 - mse: 0.2519 - mae: 0.3878 - val_loss: 0.2557 - val_mse: 0.2982 - val_mae: 0.4123 - lr: 0.0010\n",
      "Epoch 84/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2204 - mse: 0.2427 - mae: 0.3806 - val_loss: 0.2513 - val_mse: 0.2975 - val_mae: 0.4134 - lr: 0.0010\n",
      "Epoch 85/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2153 - mse: 0.2354 - mae: 0.3751 - val_loss: 0.2367 - val_mse: 0.2703 - val_mae: 0.3884 - lr: 0.0010\n",
      "Epoch 86/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2084 - mse: 0.2265 - mae: 0.3690 - val_loss: 0.2323 - val_mse: 0.2575 - val_mae: 0.3826 - lr: 0.0010\n",
      "Epoch 87/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2001 - mse: 0.2147 - mae: 0.3587 - val_loss: 0.2213 - val_mse: 0.2475 - val_mae: 0.3738 - lr: 0.0010\n",
      "Epoch 88/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1949 - mse: 0.2091 - mae: 0.3533 - val_loss: 0.2246 - val_mse: 0.2506 - val_mae: 0.3788 - lr: 0.0010\n",
      "Epoch 89/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1919 - mse: 0.2046 - mae: 0.3494 - val_loss: 0.2104 - val_mse: 0.2357 - val_mae: 0.3652 - lr: 0.0010\n",
      "Epoch 90/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1819 - mse: 0.1917 - mae: 0.3385 - val_loss: 0.2040 - val_mse: 0.2182 - val_mae: 0.3487 - lr: 0.0010\n",
      "Epoch 91/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1809 - mse: 0.1903 - mae: 0.3368 - val_loss: 0.1933 - val_mse: 0.2096 - val_mae: 0.3417 - lr: 0.0010\n",
      "Epoch 92/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1737 - mse: 0.1819 - mae: 0.3300 - val_loss: 0.1928 - val_mse: 0.2005 - val_mae: 0.3356 - lr: 0.0010\n",
      "Epoch 93/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1662 - mse: 0.1715 - mae: 0.3201 - val_loss: 0.1875 - val_mse: 0.1961 - val_mae: 0.3329 - lr: 0.0010\n",
      "Epoch 94/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1594 - mse: 0.1630 - mae: 0.3117 - val_loss: 0.1827 - val_mse: 0.1939 - val_mae: 0.3318 - lr: 0.0010\n",
      "Epoch 95/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1573 - mse: 0.1600 - mae: 0.3099 - val_loss: 0.1762 - val_mse: 0.1868 - val_mae: 0.3219 - lr: 0.0010\n",
      "Epoch 96/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1530 - mse: 0.1549 - mae: 0.3046 - val_loss: 0.1750 - val_mse: 0.1871 - val_mae: 0.3261 - lr: 0.0010\n",
      "Epoch 97/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1507 - mse: 0.1522 - mae: 0.3021 - val_loss: 0.1724 - val_mse: 0.1832 - val_mae: 0.3237 - lr: 0.0010\n",
      "Epoch 98/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1472 - mse: 0.1486 - mae: 0.2992 - val_loss: 0.1629 - val_mse: 0.1657 - val_mae: 0.3034 - lr: 0.0010\n",
      "Epoch 99/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1424 - mse: 0.1418 - mae: 0.2920 - val_loss: 0.1630 - val_mse: 0.1727 - val_mae: 0.3140 - lr: 0.0010\n",
      "Epoch 100/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1414 - mse: 0.1408 - mae: 0.2922 - val_loss: 0.1513 - val_mse: 0.1567 - val_mae: 0.2970 - lr: 0.0010\n",
      "Epoch 101/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1331 - mse: 0.1307 - mae: 0.2804 - val_loss: 0.1480 - val_mse: 0.1529 - val_mae: 0.2933 - lr: 0.0010\n",
      "Epoch 102/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1347 - mse: 0.1331 - mae: 0.2838 - val_loss: 0.1457 - val_mse: 0.1477 - val_mae: 0.2874 - lr: 0.0010\n",
      "Epoch 103/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1257 - mse: 0.1217 - mae: 0.2696 - val_loss: 0.1421 - val_mse: 0.1488 - val_mae: 0.2897 - lr: 0.0010\n",
      "Epoch 104/500\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.1257 - mse: 0.1221 - mae: 0.2719 - val_loss: 0.1468 - val_mse: 0.1487 - val_mae: 0.2890 - lr: 0.0010\n",
      "Epoch 105/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1225 - mse: 0.1186 - mae: 0.2678 - val_loss: 0.1423 - val_mse: 0.1406 - val_mae: 0.2834 - lr: 0.0010\n",
      "Epoch 106/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.1211 - mse: 0.1167 - mae: 0.2651 - val_loss: 0.1401 - val_mse: 0.1397 - val_mae: 0.2799 - lr: 0.0010\n",
      "Epoch 107/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1197 - mse: 0.1149 - mae: 0.2631 - val_loss: 0.1335 - val_mse: 0.1354 - val_mae: 0.2775 - lr: 0.0010\n",
      "Epoch 108/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1168 - mse: 0.1121 - mae: 0.2606 - val_loss: 0.1313 - val_mse: 0.1320 - val_mae: 0.2713 - lr: 0.0010\n",
      "Epoch 109/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1131 - mse: 0.1077 - mae: 0.2552 - val_loss: 0.1334 - val_mse: 0.1348 - val_mae: 0.2793 - lr: 0.0010\n",
      "Epoch 110/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1115 - mse: 0.1060 - mae: 0.2533 - val_loss: 0.1299 - val_mse: 0.1289 - val_mae: 0.2668 - lr: 0.0010\n",
      "Epoch 111/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1082 - mse: 0.1024 - mae: 0.2489 - val_loss: 0.1247 - val_mse: 0.1234 - val_mae: 0.2624 - lr: 0.0010\n",
      "Epoch 112/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1067 - mse: 0.1010 - mae: 0.2476 - val_loss: 0.1173 - val_mse: 0.1184 - val_mae: 0.2564 - lr: 0.0010\n",
      "Epoch 113/500\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1020 - mse: 0.0952 - mae: 0.2398 - val_loss: 0.1161 - val_mse: 0.1152 - val_mae: 0.2523 - lr: 0.0010\n",
      "Epoch 114/500\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.1006 - mse: 0.0938 - mae: 0.2380 - val_loss: 0.1168 - val_mse: 0.1178 - val_mae: 0.2577 - lr: 0.0010\n",
      "Epoch 115/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1004 - mse: 0.0936 - mae: 0.2375 - val_loss: 0.1118 - val_mse: 0.1088 - val_mae: 0.2477 - lr: 0.0010\n",
      "Epoch 116/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0990 - mse: 0.0921 - mae: 0.2363 - val_loss: 0.1102 - val_mse: 0.1056 - val_mae: 0.2425 - lr: 0.0010\n",
      "Epoch 117/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0944 - mse: 0.0872 - mae: 0.2296 - val_loss: 0.1079 - val_mse: 0.1061 - val_mae: 0.2432 - lr: 0.0010\n",
      "Epoch 118/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0935 - mse: 0.0859 - mae: 0.2281 - val_loss: 0.1074 - val_mse: 0.1074 - val_mae: 0.2441 - lr: 0.0010\n",
      "Epoch 119/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0914 - mse: 0.0840 - mae: 0.2247 - val_loss: 0.1035 - val_mse: 0.1023 - val_mae: 0.2362 - lr: 0.0010\n",
      "Epoch 120/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0929 - mse: 0.0856 - mae: 0.2271 - val_loss: 0.1010 - val_mse: 0.0949 - val_mae: 0.2281 - lr: 0.0010\n",
      "Epoch 121/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0887 - mse: 0.0812 - mae: 0.2211 - val_loss: 0.1000 - val_mse: 0.0963 - val_mae: 0.2298 - lr: 0.0010\n",
      "Epoch 122/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0891 - mse: 0.0815 - mae: 0.2209 - val_loss: 0.1074 - val_mse: 0.1036 - val_mae: 0.2408 - lr: 0.0010\n",
      "Epoch 123/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0851 - mse: 0.0775 - mae: 0.2161 - val_loss: 0.0998 - val_mse: 0.0963 - val_mae: 0.2313 - lr: 0.0010\n",
      "Epoch 124/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0827 - mse: 0.0750 - mae: 0.2111 - val_loss: 0.1002 - val_mse: 0.0971 - val_mae: 0.2313 - lr: 0.0010\n",
      "Epoch 125/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0813 - mse: 0.0737 - mae: 0.2097 - val_loss: 0.0913 - val_mse: 0.0892 - val_mae: 0.2204 - lr: 0.0010\n",
      "Epoch 126/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0819 - mse: 0.0743 - mae: 0.2106 - val_loss: 0.0994 - val_mse: 0.0971 - val_mae: 0.2374 - lr: 0.0010\n",
      "Epoch 127/500\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.0808 - mse: 0.0731 - mae: 0.2102 - val_loss: 0.0987 - val_mse: 0.0943 - val_mae: 0.2333 - lr: 0.0010\n",
      "Epoch 128/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0793 - mse: 0.0715 - mae: 0.2076 - val_loss: 0.0915 - val_mse: 0.0887 - val_mae: 0.2230 - lr: 0.0010\n",
      "Epoch 129/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0777 - mse: 0.0703 - mae: 0.2051 - val_loss: 0.0872 - val_mse: 0.0818 - val_mae: 0.2131 - lr: 0.0010\n",
      "Epoch 130/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0765 - mse: 0.0689 - mae: 0.2029 - val_loss: 0.0861 - val_mse: 0.0812 - val_mae: 0.2110 - lr: 0.0010\n",
      "Epoch 131/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0771 - mse: 0.0694 - mae: 0.2041 - val_loss: 0.0831 - val_mse: 0.0788 - val_mae: 0.2073 - lr: 0.0010\n",
      "Epoch 132/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0756 - mse: 0.0680 - mae: 0.2019 - val_loss: 0.0856 - val_mse: 0.0804 - val_mae: 0.2104 - lr: 0.0010\n",
      "Epoch 133/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0734 - mse: 0.0657 - mae: 0.1975 - val_loss: 0.0864 - val_mse: 0.0818 - val_mae: 0.2099 - lr: 0.0010\n",
      "Epoch 134/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0728 - mse: 0.0653 - mae: 0.1964 - val_loss: 0.0836 - val_mse: 0.0771 - val_mae: 0.2033 - lr: 0.0010\n",
      "Epoch 135/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0737 - mse: 0.0661 - mae: 0.1978 - val_loss: 0.0837 - val_mse: 0.0788 - val_mae: 0.2098 - lr: 0.0010\n",
      "Epoch 136/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0717 - mse: 0.0643 - mae: 0.1950 - val_loss: 0.0791 - val_mse: 0.0744 - val_mae: 0.2029 - lr: 0.0010\n",
      "Epoch 137/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0709 - mse: 0.0636 - mae: 0.1942 - val_loss: 0.0807 - val_mse: 0.0758 - val_mae: 0.2033 - lr: 0.0010\n",
      "Epoch 138/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0707 - mse: 0.0634 - mae: 0.1935 - val_loss: 0.0762 - val_mse: 0.0716 - val_mae: 0.1975 - lr: 0.0010\n",
      "Epoch 139/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0733 - mse: 0.0660 - mae: 0.1970 - val_loss: 0.0768 - val_mse: 0.0732 - val_mae: 0.1999 - lr: 0.0010\n",
      "Epoch 140/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0688 - mse: 0.0615 - mae: 0.1909 - val_loss: 0.0737 - val_mse: 0.0685 - val_mae: 0.1918 - lr: 0.0010\n",
      "Epoch 141/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0653 - mse: 0.0579 - mae: 0.1835 - val_loss: 0.0748 - val_mse: 0.0699 - val_mae: 0.1929 - lr: 0.0010\n",
      "Epoch 142/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0648 - mse: 0.0577 - mae: 0.1842 - val_loss: 0.0723 - val_mse: 0.0666 - val_mae: 0.1895 - lr: 0.0010\n",
      "Epoch 143/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0651 - mse: 0.0576 - mae: 0.1825 - val_loss: 0.0756 - val_mse: 0.0702 - val_mae: 0.1968 - lr: 0.0010\n",
      "Epoch 144/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0643 - mse: 0.0570 - mae: 0.1822 - val_loss: 0.0750 - val_mse: 0.0697 - val_mae: 0.1960 - lr: 0.0010\n",
      "Epoch 145/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0629 - mse: 0.0556 - mae: 0.1809 - val_loss: 0.0746 - val_mse: 0.0679 - val_mae: 0.1947 - lr: 0.0010\n",
      "Epoch 146/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0648 - mse: 0.0577 - mae: 0.1859 - val_loss: 0.0769 - val_mse: 0.0711 - val_mae: 0.2020 - lr: 0.0010\n",
      "Epoch 147/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0630 - mse: 0.0559 - mae: 0.1818 - val_loss: 0.0675 - val_mse: 0.0620 - val_mae: 0.1842 - lr: 0.0010\n",
      "Epoch 148/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0599 - mse: 0.0529 - mae: 0.1748 - val_loss: 0.0675 - val_mse: 0.0616 - val_mae: 0.1814 - lr: 0.0010\n",
      "Epoch 149/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0607 - mse: 0.0535 - mae: 0.1775 - val_loss: 0.0686 - val_mse: 0.0630 - val_mae: 0.1857 - lr: 0.0010\n",
      "Epoch 150/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0611 - mse: 0.0542 - mae: 0.1783 - val_loss: 0.0701 - val_mse: 0.0658 - val_mae: 0.1894 - lr: 0.0010\n",
      "Epoch 151/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0572 - mse: 0.0504 - mae: 0.1706 - val_loss: 0.0626 - val_mse: 0.0573 - val_mae: 0.1727 - lr: 0.0010\n",
      "Epoch 152/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0573 - mse: 0.0507 - mae: 0.1721 - val_loss: 0.0676 - val_mse: 0.0628 - val_mae: 0.1832 - lr: 0.0010\n",
      "Epoch 153/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0539 - mse: 0.0471 - mae: 0.1638 - val_loss: 0.0632 - val_mse: 0.0582 - val_mae: 0.1747 - lr: 0.0010\n",
      "Epoch 154/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0560 - mse: 0.0494 - mae: 0.1688 - val_loss: 0.0643 - val_mse: 0.0610 - val_mae: 0.1791 - lr: 0.0010\n",
      "Epoch 155/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0592 - mse: 0.0529 - mae: 0.1745 - val_loss: 0.0633 - val_mse: 0.0574 - val_mae: 0.1723 - lr: 0.0010\n",
      "Epoch 156/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0539 - mse: 0.0474 - mae: 0.1654 - val_loss: 0.0620 - val_mse: 0.0564 - val_mae: 0.1736 - lr: 0.0010\n",
      "Epoch 157/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0569 - mse: 0.0501 - mae: 0.1698 - val_loss: 0.0624 - val_mse: 0.0585 - val_mae: 0.1777 - lr: 0.0010\n",
      "Epoch 158/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0532 - mse: 0.0467 - mae: 0.1635 - val_loss: 0.0628 - val_mse: 0.0576 - val_mae: 0.1758 - lr: 0.0010\n",
      "Epoch 159/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0536 - mse: 0.0471 - mae: 0.1647 - val_loss: 0.0691 - val_mse: 0.0627 - val_mae: 0.1899 - lr: 0.0010\n",
      "Epoch 160/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0529 - mse: 0.0465 - mae: 0.1624 - val_loss: 0.0628 - val_mse: 0.0564 - val_mae: 0.1760 - lr: 0.0010\n",
      "Epoch 161/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0541 - mse: 0.0479 - mae: 0.1662 - val_loss: 0.0588 - val_mse: 0.0534 - val_mae: 0.1698 - lr: 0.0010\n",
      "Epoch 162/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0516 - mse: 0.0452 - mae: 0.1601 - val_loss: 0.0649 - val_mse: 0.0592 - val_mae: 0.1829 - lr: 0.0010\n",
      "Epoch 163/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0511 - mse: 0.0447 - mae: 0.1596 - val_loss: 0.0634 - val_mse: 0.0566 - val_mae: 0.1735 - lr: 0.0010\n",
      "Epoch 164/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0520 - mse: 0.0457 - mae: 0.1624 - val_loss: 0.0561 - val_mse: 0.0509 - val_mae: 0.1615 - lr: 0.0010\n",
      "Epoch 165/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0500 - mse: 0.0437 - mae: 0.1583 - val_loss: 0.0623 - val_mse: 0.0559 - val_mae: 0.1739 - lr: 0.0010\n",
      "Epoch 166/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0522 - mse: 0.0462 - mae: 0.1620 - val_loss: 0.0567 - val_mse: 0.0537 - val_mae: 0.1672 - lr: 0.0010\n",
      "Epoch 167/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0487 - mse: 0.0427 - mae: 0.1554 - val_loss: 0.0559 - val_mse: 0.0512 - val_mae: 0.1652 - lr: 0.0010\n",
      "Epoch 168/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0455 - mse: 0.0396 - mae: 0.1475 - val_loss: 0.0553 - val_mse: 0.0501 - val_mae: 0.1599 - lr: 0.0010\n",
      "Epoch 169/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0458 - mse: 0.0400 - mae: 0.1489 - val_loss: 0.0571 - val_mse: 0.0512 - val_mae: 0.1589 - lr: 0.0010\n",
      "Epoch 170/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0482 - mse: 0.0421 - mae: 0.1541 - val_loss: 0.0547 - val_mse: 0.0512 - val_mae: 0.1660 - lr: 0.0010\n",
      "Epoch 171/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0501 - mse: 0.0441 - mae: 0.1597 - val_loss: 0.0528 - val_mse: 0.0480 - val_mae: 0.1570 - lr: 0.0010\n",
      "Epoch 172/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0479 - mse: 0.0421 - mae: 0.1545 - val_loss: 0.0505 - val_mse: 0.0456 - val_mae: 0.1527 - lr: 0.0010\n",
      "Epoch 173/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0451 - mse: 0.0396 - mae: 0.1486 - val_loss: 0.0550 - val_mse: 0.0490 - val_mae: 0.1585 - lr: 0.0010\n",
      "Epoch 174/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0459 - mse: 0.0401 - mae: 0.1490 - val_loss: 0.0563 - val_mse: 0.0527 - val_mae: 0.1628 - lr: 0.0010\n",
      "Epoch 175/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0457 - mse: 0.0399 - mae: 0.1503 - val_loss: 0.0501 - val_mse: 0.0453 - val_mae: 0.1520 - lr: 0.0010\n",
      "Epoch 176/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0447 - mse: 0.0389 - mae: 0.1465 - val_loss: 0.0518 - val_mse: 0.0467 - val_mae: 0.1560 - lr: 0.0010\n",
      "Epoch 177/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0462 - mse: 0.0406 - mae: 0.1518 - val_loss: 0.0514 - val_mse: 0.0464 - val_mae: 0.1529 - lr: 0.0010\n",
      "Epoch 178/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0478 - mse: 0.0421 - mae: 0.1550 - val_loss: 0.0562 - val_mse: 0.0527 - val_mae: 0.1685 - lr: 0.0010\n",
      "Epoch 179/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0463 - mse: 0.0409 - mae: 0.1510 - val_loss: 0.0622 - val_mse: 0.0554 - val_mae: 0.1804 - lr: 0.0010\n",
      "Epoch 180/500\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0452 - mse: 0.0396 - mae: 0.1495\n",
      "Epoch 180: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0453 - mse: 0.0398 - mae: 0.1501 - val_loss: 0.0507 - val_mse: 0.0454 - val_mae: 0.1539 - lr: 0.0010\n",
      "Epoch 181/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0390 - mse: 0.0338 - mae: 0.1342 - val_loss: 0.0473 - val_mse: 0.0428 - val_mae: 0.1441 - lr: 5.0000e-04\n",
      "Epoch 182/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0376 - mse: 0.0326 - mae: 0.1309 - val_loss: 0.0472 - val_mse: 0.0425 - val_mae: 0.1438 - lr: 5.0000e-04\n",
      "Epoch 183/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0376 - mse: 0.0326 - mae: 0.1308 - val_loss: 0.0454 - val_mse: 0.0407 - val_mae: 0.1413 - lr: 5.0000e-04\n",
      "Epoch 184/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0377 - mse: 0.0327 - mae: 0.1311 - val_loss: 0.0463 - val_mse: 0.0417 - val_mae: 0.1426 - lr: 5.0000e-04\n",
      "Epoch 185/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0372 - mse: 0.0322 - mae: 0.1303 - val_loss: 0.0450 - val_mse: 0.0406 - val_mae: 0.1406 - lr: 5.0000e-04\n",
      "Epoch 186/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0378 - mse: 0.0328 - mae: 0.1321 - val_loss: 0.0462 - val_mse: 0.0414 - val_mae: 0.1421 - lr: 5.0000e-04\n",
      "Epoch 187/500\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.0367 - mse: 0.0318 - mae: 0.1292 - val_loss: 0.0427 - val_mse: 0.0384 - val_mae: 0.1341 - lr: 5.0000e-04\n",
      "Epoch 188/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0368 - mse: 0.0319 - mae: 0.1292 - val_loss: 0.0427 - val_mse: 0.0384 - val_mae: 0.1338 - lr: 5.0000e-04\n",
      "Epoch 189/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0373 - mse: 0.0323 - mae: 0.1312 - val_loss: 0.0437 - val_mse: 0.0392 - val_mae: 0.1369 - lr: 5.0000e-04\n",
      "Epoch 190/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0367 - mse: 0.0318 - mae: 0.1300 - val_loss: 0.0449 - val_mse: 0.0406 - val_mae: 0.1442 - lr: 5.0000e-04\n",
      "Epoch 191/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0366 - mse: 0.0317 - mae: 0.1295 - val_loss: 0.0427 - val_mse: 0.0386 - val_mae: 0.1345 - lr: 5.0000e-04\n",
      "Epoch 192/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0354 - mse: 0.0307 - mae: 0.1263 - val_loss: 0.0417 - val_mse: 0.0376 - val_mae: 0.1340 - lr: 5.0000e-04\n",
      "Epoch 193/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0364 - mse: 0.0315 - mae: 0.1281 - val_loss: 0.0436 - val_mse: 0.0390 - val_mae: 0.1388 - lr: 5.0000e-04\n",
      "Epoch 194/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0363 - mse: 0.0314 - mae: 0.1282 - val_loss: 0.0443 - val_mse: 0.0399 - val_mae: 0.1406 - lr: 5.0000e-04\n",
      "Epoch 195/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0370 - mse: 0.0320 - mae: 0.1299 - val_loss: 0.0438 - val_mse: 0.0391 - val_mae: 0.1408 - lr: 5.0000e-04\n",
      "Epoch 196/500\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0364 - mse: 0.0316 - mae: 0.1290 - val_loss: 0.0436 - val_mse: 0.0392 - val_mae: 0.1365 - lr: 5.0000e-04\n",
      "Epoch 197/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0349 - mse: 0.0303 - mae: 0.1252 - val_loss: 0.0410 - val_mse: 0.0368 - val_mae: 0.1333 - lr: 5.0000e-04\n",
      "Epoch 198/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0349 - mse: 0.0303 - mae: 0.1255 - val_loss: 0.0428 - val_mse: 0.0383 - val_mae: 0.1385 - lr: 5.0000e-04\n",
      "Epoch 199/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0359 - mse: 0.0312 - mae: 0.1285 - val_loss: 0.0420 - val_mse: 0.0377 - val_mae: 0.1369 - lr: 5.0000e-04\n",
      "Epoch 200/500\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0351 - mse: 0.0305 - mae: 0.1257 - val_loss: 0.0417 - val_mse: 0.0375 - val_mae: 0.1375 - lr: 5.0000e-04\n",
      "Epoch 201/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0355 - mse: 0.0308 - mae: 0.1275 - val_loss: 0.0414 - val_mse: 0.0371 - val_mae: 0.1330 - lr: 5.0000e-04\n",
      "Epoch 202/500\n",
      "39/50 [======================>.......] - ETA: 0s - loss: 0.0341 - mse: 0.0294 - mae: 0.1236\n",
      "Epoch 202: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0344 - mse: 0.0298 - mae: 0.1248 - val_loss: 0.0422 - val_mse: 0.0378 - val_mae: 0.1347 - lr: 5.0000e-04\n",
      "Epoch 203/500\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0324 - mse: 0.0281 - mae: 0.1194 - val_loss: 0.0393 - val_mse: 0.0353 - val_mae: 0.1288 - lr: 2.5000e-04\n",
      "Epoch 204/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0319 - mse: 0.0275 - mae: 0.1178 - val_loss: 0.0387 - val_mse: 0.0346 - val_mae: 0.1287 - lr: 2.5000e-04\n",
      "Epoch 205/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0322 - mse: 0.0279 - mae: 0.1183 - val_loss: 0.0396 - val_mse: 0.0353 - val_mae: 0.1277 - lr: 2.5000e-04\n",
      "Epoch 206/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0318 - mse: 0.0275 - mae: 0.1175 - val_loss: 0.0397 - val_mse: 0.0354 - val_mae: 0.1306 - lr: 2.5000e-04\n",
      "Epoch 207/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0321 - mse: 0.0278 - mae: 0.1186 - val_loss: 0.0387 - val_mse: 0.0346 - val_mae: 0.1290 - lr: 2.5000e-04\n",
      "Epoch 208/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0316 - mse: 0.0273 - mae: 0.1173 - val_loss: 0.0388 - val_mse: 0.0348 - val_mae: 0.1287 - lr: 2.5000e-04\n",
      "Epoch 209/500\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.0329 - mse: 0.0285 - mae: 0.1203\n",
      "Epoch 209: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0327 - mse: 0.0283 - mae: 0.1198 - val_loss: 0.0391 - val_mse: 0.0350 - val_mae: 0.1280 - lr: 2.5000e-04\n",
      "Epoch 210/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0309 - mse: 0.0267 - mae: 0.1150 - val_loss: 0.0378 - val_mse: 0.0340 - val_mae: 0.1251 - lr: 1.2500e-04\n",
      "Epoch 211/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0310 - mse: 0.0268 - mae: 0.1152 - val_loss: 0.0374 - val_mse: 0.0334 - val_mae: 0.1239 - lr: 1.2500e-04\n",
      "Epoch 212/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0307 - mse: 0.0266 - mae: 0.1149 - val_loss: 0.0379 - val_mse: 0.0338 - val_mae: 0.1255 - lr: 1.2500e-04\n",
      "Epoch 213/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0305 - mse: 0.0264 - mae: 0.1141 - val_loss: 0.0376 - val_mse: 0.0336 - val_mae: 0.1244 - lr: 1.2500e-04\n",
      "Epoch 214/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0303 - mse: 0.0262 - mae: 0.1140 - val_loss: 0.0398 - val_mse: 0.0354 - val_mae: 0.1261 - lr: 1.2500e-04\n",
      "Epoch 215/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0308 - mse: 0.0266 - mae: 0.1150 - val_loss: 0.0375 - val_mse: 0.0335 - val_mae: 0.1241 - lr: 1.2500e-04\n",
      "Epoch 216/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0304 - mse: 0.0263 - mae: 0.1141 - val_loss: 0.0368 - val_mse: 0.0329 - val_mae: 0.1225 - lr: 1.2500e-04\n",
      "Epoch 217/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0302 - mse: 0.0261 - mae: 0.1139 - val_loss: 0.0370 - val_mse: 0.0330 - val_mae: 0.1231 - lr: 1.2500e-04\n",
      "Epoch 218/500\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0303 - mse: 0.0262 - mae: 0.1139 - val_loss: 0.0366 - val_mse: 0.0327 - val_mae: 0.1223 - lr: 1.2500e-04\n",
      "Epoch 219/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0303 - mse: 0.0262 - mae: 0.1138 - val_loss: 0.0367 - val_mse: 0.0327 - val_mae: 0.1227 - lr: 1.2500e-04\n",
      "Epoch 220/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0302 - mse: 0.0261 - mae: 0.1138 - val_loss: 0.0368 - val_mse: 0.0328 - val_mae: 0.1228 - lr: 1.2500e-04\n",
      "Epoch 221/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0303 - mse: 0.0262 - mae: 0.1142 - val_loss: 0.0371 - val_mse: 0.0332 - val_mae: 0.1242 - lr: 1.2500e-04\n",
      "Epoch 222/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0300 - mse: 0.0260 - mae: 0.1130 - val_loss: 0.0362 - val_mse: 0.0323 - val_mae: 0.1215 - lr: 1.2500e-04\n",
      "Epoch 223/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0303 - mse: 0.0262 - mae: 0.1139 - val_loss: 0.0375 - val_mse: 0.0334 - val_mae: 0.1259 - lr: 1.2500e-04\n",
      "Epoch 224/500\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.0301 - mse: 0.0260 - mae: 0.1137 - val_loss: 0.0374 - val_mse: 0.0334 - val_mae: 0.1256 - lr: 1.2500e-04\n",
      "Epoch 225/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0300 - mse: 0.0259 - mae: 0.1133 - val_loss: 0.0368 - val_mse: 0.0328 - val_mae: 0.1232 - lr: 1.2500e-04\n",
      "Epoch 226/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0298 - mse: 0.0258 - mae: 0.1130 - val_loss: 0.0364 - val_mse: 0.0324 - val_mae: 0.1217 - lr: 1.2500e-04\n",
      "Epoch 227/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0297 - mse: 0.0257 - mae: 0.1126 - val_loss: 0.0358 - val_mse: 0.0319 - val_mae: 0.1209 - lr: 1.2500e-04\n",
      "Epoch 228/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0295 - mse: 0.0255 - mae: 0.1122 - val_loss: 0.0356 - val_mse: 0.0317 - val_mae: 0.1199 - lr: 1.2500e-04\n",
      "Epoch 229/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0299 - mse: 0.0258 - mae: 0.1130 - val_loss: 0.0358 - val_mse: 0.0318 - val_mae: 0.1211 - lr: 1.2500e-04\n",
      "Epoch 230/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0294 - mse: 0.0254 - mae: 0.1120 - val_loss: 0.0360 - val_mse: 0.0321 - val_mae: 0.1217 - lr: 1.2500e-04\n",
      "Epoch 231/500\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0297 - mse: 0.0257 - mae: 0.1128 - val_loss: 0.0356 - val_mse: 0.0317 - val_mae: 0.1209 - lr: 1.2500e-04\n",
      "Epoch 232/500\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.0292 - mse: 0.0253 - mae: 0.1117 - val_loss: 0.0353 - val_mse: 0.0314 - val_mae: 0.1208 - lr: 1.2500e-04\n",
      "Epoch 233/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0293 - mse: 0.0253 - mae: 0.1116 - val_loss: 0.0349 - val_mse: 0.0311 - val_mae: 0.1197 - lr: 1.2500e-04\n",
      "Epoch 234/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0289 - mse: 0.0250 - mae: 0.1111 - val_loss: 0.0359 - val_mse: 0.0319 - val_mae: 0.1215 - lr: 1.2500e-04\n",
      "Epoch 235/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0292 - mse: 0.0253 - mae: 0.1114 - val_loss: 0.0358 - val_mse: 0.0318 - val_mae: 0.1214 - lr: 1.2500e-04\n",
      "Epoch 236/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0292 - mse: 0.0253 - mae: 0.1116 - val_loss: 0.0349 - val_mse: 0.0310 - val_mae: 0.1197 - lr: 1.2500e-04\n",
      "Epoch 237/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0287 - mse: 0.0248 - mae: 0.1103 - val_loss: 0.0351 - val_mse: 0.0311 - val_mae: 0.1203 - lr: 1.2500e-04\n",
      "Epoch 238/500\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0289 - mse: 0.0250 - mae: 0.1111\n",
      "Epoch 238: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0290 - mse: 0.0250 - mae: 0.1113 - val_loss: 0.0354 - val_mse: 0.0315 - val_mae: 0.1216 - lr: 1.2500e-04\n",
      "Epoch 239/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0284 - mse: 0.0246 - mae: 0.1093 - val_loss: 0.0344 - val_mse: 0.0306 - val_mae: 0.1182 - lr: 6.2500e-05\n",
      "Epoch 240/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0282 - mse: 0.0243 - mae: 0.1087 - val_loss: 0.0342 - val_mse: 0.0304 - val_mae: 0.1175 - lr: 6.2500e-05\n",
      "Epoch 241/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0283 - mse: 0.0244 - mae: 0.1093 - val_loss: 0.0346 - val_mse: 0.0307 - val_mae: 0.1170 - lr: 6.2500e-05\n",
      "Epoch 242/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0283 - mse: 0.0244 - mae: 0.1091 - val_loss: 0.0345 - val_mse: 0.0306 - val_mae: 0.1185 - lr: 6.2500e-05\n",
      "Epoch 243/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0282 - mse: 0.0244 - mae: 0.1092 - val_loss: 0.0344 - val_mse: 0.0305 - val_mae: 0.1188 - lr: 6.2500e-05\n",
      "Epoch 244/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0281 - mse: 0.0243 - mae: 0.1082 - val_loss: 0.0340 - val_mse: 0.0302 - val_mae: 0.1176 - lr: 6.2500e-05\n",
      "Epoch 245/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0282 - mse: 0.0244 - mae: 0.1090 - val_loss: 0.0349 - val_mse: 0.0309 - val_mae: 0.1190 - lr: 6.2500e-05\n",
      "Epoch 246/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0283 - mse: 0.0244 - mae: 0.1088 - val_loss: 0.0339 - val_mse: 0.0302 - val_mae: 0.1171 - lr: 6.2500e-05\n",
      "Epoch 247/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0280 - mse: 0.0242 - mae: 0.1085 - val_loss: 0.0340 - val_mse: 0.0302 - val_mae: 0.1176 - lr: 6.2500e-05\n",
      "Epoch 248/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0280 - mse: 0.0242 - mae: 0.1086 - val_loss: 0.0340 - val_mse: 0.0302 - val_mae: 0.1170 - lr: 6.2500e-05\n",
      "Epoch 249/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0277 - mse: 0.0239 - mae: 0.1074 - val_loss: 0.0339 - val_mse: 0.0300 - val_mae: 0.1170 - lr: 6.2500e-05\n",
      "Epoch 250/500\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0277 - mse: 0.0239 - mae: 0.1076 - val_loss: 0.0340 - val_mse: 0.0301 - val_mae: 0.1173 - lr: 6.2500e-05\n",
      "Epoch 251/500\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0279 - mse: 0.0242 - mae: 0.1088\n",
      "Epoch 251: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0279 - mse: 0.0241 - mae: 0.1087 - val_loss: 0.0340 - val_mse: 0.0301 - val_mae: 0.1167 - lr: 6.2500e-05\n",
      "Epoch 252/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0274 - mse: 0.0237 - mae: 0.1067 - val_loss: 0.0334 - val_mse: 0.0297 - val_mae: 0.1156 - lr: 3.1250e-05\n",
      "Epoch 253/500\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0274 - mse: 0.0237 - mae: 0.1069 - val_loss: 0.0332 - val_mse: 0.0295 - val_mae: 0.1157 - lr: 3.1250e-05\n",
      "Epoch 254/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0273 - mse: 0.0236 - mae: 0.1065 - val_loss: 0.0335 - val_mse: 0.0297 - val_mae: 0.1159 - lr: 3.1250e-05\n",
      "Epoch 255/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0274 - mse: 0.0237 - mae: 0.1068 - val_loss: 0.0335 - val_mse: 0.0297 - val_mae: 0.1167 - lr: 3.1250e-05\n",
      "Epoch 256/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0276 - mse: 0.0239 - mae: 0.1074 - val_loss: 0.0333 - val_mse: 0.0295 - val_mae: 0.1161 - lr: 3.1250e-05\n",
      "Epoch 257/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0273 - mse: 0.0236 - mae: 0.1067 - val_loss: 0.0331 - val_mse: 0.0294 - val_mae: 0.1152 - lr: 3.1250e-05\n",
      "Epoch 258/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0273 - mse: 0.0236 - mae: 0.1063 - val_loss: 0.0331 - val_mse: 0.0293 - val_mae: 0.1153 - lr: 3.1250e-05\n",
      "Epoch 259/500\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0272 - mse: 0.0235 - mae: 0.1065 - val_loss: 0.0331 - val_mse: 0.0295 - val_mae: 0.1158 - lr: 3.1250e-05\n",
      "Epoch 260/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0272 - mse: 0.0235 - mae: 0.1063 - val_loss: 0.0333 - val_mse: 0.0295 - val_mae: 0.1160 - lr: 3.1250e-05\n",
      "Epoch 261/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0273 - mse: 0.0235 - mae: 0.1064 - val_loss: 0.0333 - val_mse: 0.0294 - val_mae: 0.1157 - lr: 3.1250e-05\n",
      "Epoch 262/500\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0271 - mse: 0.0234 - mae: 0.1061\n",
      "Epoch 262: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0271 - mse: 0.0234 - mae: 0.1062 - val_loss: 0.0332 - val_mse: 0.0293 - val_mae: 0.1150 - lr: 3.1250e-05\n",
      "Epoch 263/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0270 - mse: 0.0233 - mae: 0.1057 - val_loss: 0.0329 - val_mse: 0.0291 - val_mae: 0.1149 - lr: 1.5625e-05\n",
      "Epoch 264/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0270 - mse: 0.0233 - mae: 0.1059 - val_loss: 0.0331 - val_mse: 0.0293 - val_mae: 0.1155 - lr: 1.5625e-05\n",
      "Epoch 265/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0271 - mse: 0.0234 - mae: 0.1061 - val_loss: 0.0327 - val_mse: 0.0290 - val_mae: 0.1144 - lr: 1.5625e-05\n",
      "Epoch 266/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0270 - mse: 0.0233 - mae: 0.1058 - val_loss: 0.0328 - val_mse: 0.0291 - val_mae: 0.1148 - lr: 1.5625e-05\n",
      "Epoch 267/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0268 - mse: 0.0231 - mae: 0.1053 - val_loss: 0.0329 - val_mse: 0.0292 - val_mae: 0.1149 - lr: 1.5625e-05\n",
      "Epoch 268/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0268 - mse: 0.0232 - mae: 0.1054 - val_loss: 0.0328 - val_mse: 0.0290 - val_mae: 0.1141 - lr: 1.5625e-05\n",
      "Epoch 269/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0269 - mse: 0.0232 - mae: 0.1056 - val_loss: 0.0327 - val_mse: 0.0290 - val_mae: 0.1146 - lr: 1.5625e-05\n",
      "Epoch 270/500\n",
      "38/50 [=====================>........] - ETA: 0s - loss: 0.0260 - mse: 0.0225 - mae: 0.1034\n",
      "Epoch 270: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0268 - mse: 0.0232 - mae: 0.1054 - val_loss: 0.0330 - val_mse: 0.0292 - val_mae: 0.1151 - lr: 1.5625e-05\n",
      "Epoch 271/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0268 - mse: 0.0231 - mae: 0.1053 - val_loss: 0.0326 - val_mse: 0.0289 - val_mae: 0.1142 - lr: 7.8125e-06\n",
      "Epoch 272/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0267 - mse: 0.0231 - mae: 0.1050 - val_loss: 0.0326 - val_mse: 0.0289 - val_mae: 0.1142 - lr: 7.8125e-06\n",
      "Epoch 273/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0267 - mse: 0.0231 - mae: 0.1050 - val_loss: 0.0327 - val_mse: 0.0290 - val_mae: 0.1147 - lr: 7.8125e-06\n",
      "Epoch 274/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0267 - mse: 0.0230 - mae: 0.1050 - val_loss: 0.0326 - val_mse: 0.0289 - val_mae: 0.1142 - lr: 7.8125e-06\n",
      "Epoch 275/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0267 - mse: 0.0231 - mae: 0.1051 - val_loss: 0.0326 - val_mse: 0.0288 - val_mae: 0.1141 - lr: 7.8125e-06\n",
      "Epoch 276/500\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0268 - mse: 0.0231 - mae: 0.1051\n",
      "Epoch 276: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0267 - mse: 0.0231 - mae: 0.1052 - val_loss: 0.0326 - val_mse: 0.0289 - val_mae: 0.1140 - lr: 7.8125e-06\n",
      "Epoch 277/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0266 - mse: 0.0230 - mae: 0.1048 - val_loss: 0.0326 - val_mse: 0.0289 - val_mae: 0.1141 - lr: 3.9063e-06\n",
      "Epoch 278/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0266 - mse: 0.0230 - mae: 0.1048 - val_loss: 0.0326 - val_mse: 0.0288 - val_mae: 0.1140 - lr: 3.9063e-06\n",
      "Epoch 279/500\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0266 - mse: 0.0230 - mae: 0.1049 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1139 - lr: 3.9063e-06\n",
      "Epoch 280/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0266 - mse: 0.0230 - mae: 0.1048 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1140 - lr: 3.9063e-06\n",
      "Epoch 281/500\n",
      "44/50 [=========================>....] - ETA: 0s - loss: 0.0266 - mse: 0.0231 - mae: 0.1050\n",
      "Epoch 281: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0266 - mse: 0.0230 - mae: 0.1047 - val_loss: 0.0326 - val_mse: 0.0288 - val_mae: 0.1140 - lr: 3.9063e-06\n",
      "Epoch 282/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0266 - mse: 0.0230 - mae: 0.1047 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1139 - lr: 1.9531e-06\n",
      "Epoch 283/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0266 - mse: 0.0230 - mae: 0.1048 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1140 - lr: 1.9531e-06\n",
      "Epoch 284/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0266 - mse: 0.0230 - mae: 0.1047 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1139 - lr: 1.9531e-06\n",
      "Epoch 285/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0266 - mse: 0.0229 - mae: 0.1047 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.9531e-06\n",
      "Epoch 286/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0266 - mse: 0.0229 - mae: 0.1047 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1139 - lr: 1.9531e-06\n",
      "Epoch 287/500\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0263 - mse: 0.0228 - mae: 0.1043\n",
      "Epoch 287: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1047 - val_loss: 0.0326 - val_mse: 0.0288 - val_mae: 0.1140 - lr: 1.9531e-06\n",
      "Epoch 288/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1139 - lr: 9.7656e-07\n",
      "Epoch 289/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 9.7656e-07\n",
      "Epoch 290/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1139 - lr: 9.7656e-07\n",
      "Epoch 291/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1047 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1139 - lr: 9.7656e-07\n",
      "Epoch 292/500\n",
      "34/50 [===================>..........] - ETA: 0s - loss: 0.0266 - mse: 0.0228 - mae: 0.1046\n",
      "Epoch 292: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1139 - lr: 9.7656e-07\n",
      "Epoch 293/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1139 - lr: 4.8828e-07\n",
      "Epoch 294/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1139 - lr: 4.8828e-07\n",
      "Epoch 295/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1139 - lr: 4.8828e-07\n",
      "Epoch 296/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1139 - lr: 4.8828e-07\n",
      "Epoch 297/500\n",
      "37/50 [=====================>........] - ETA: 0s - loss: 0.0264 - mse: 0.0228 - mae: 0.1043\n",
      "Epoch 297: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1139 - lr: 4.8828e-07\n",
      "Epoch 298/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1139 - lr: 2.4414e-07\n",
      "Epoch 299/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 2.4414e-07\n",
      "Epoch 300/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1139 - lr: 2.4414e-07\n",
      "Epoch 301/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 2.4414e-07\n",
      "Epoch 302/500\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.0263 - mse: 0.0227 - mae: 0.1043\n",
      "Epoch 302: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 2.4414e-07\n",
      "Epoch 303/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.2207e-07\n",
      "Epoch 304/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.2207e-07\n",
      "Epoch 305/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.2207e-07\n",
      "Epoch 306/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.2207e-07\n",
      "Epoch 307/500\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0265 - mse: 0.0229 - mae: 0.1046\n",
      "Epoch 307: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.2207e-07\n",
      "Epoch 308/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.0000e-07\n",
      "Epoch 309/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.0000e-07\n",
      "Epoch 310/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.0000e-07\n",
      "Epoch 311/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.0000e-07\n",
      "Epoch 312/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.0000e-07\n",
      "Epoch 313/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.0000e-07\n",
      "Epoch 314/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.0000e-07\n",
      "Epoch 315/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.0000e-07\n",
      "Epoch 316/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.0000e-07\n",
      "Epoch 317/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.0000e-07\n",
      "Epoch 318/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.0000e-07\n",
      "Epoch 319/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.0000e-07\n",
      "Epoch 320/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.0000e-07\n",
      "Epoch 321/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.0000e-07\n",
      "Epoch 322/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.0000e-07\n",
      "Epoch 323/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.0000e-07\n",
      "Epoch 324/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.0000e-07\n",
      "Epoch 325/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.0000e-07\n",
      "Epoch 326/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.0000e-07\n",
      "Epoch 327/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.0000e-07\n",
      "Epoch 328/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1045 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.0000e-07\n",
      "Epoch 329/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.0000e-07\n",
      "Epoch 330/500\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.0000e-07\n",
      "Epoch 331/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1045 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.0000e-07\n",
      "Epoch 332/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1045 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.0000e-07\n",
      "Epoch 333/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.0000e-07\n",
      "Epoch 334/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.0000e-07\n",
      "Epoch 335/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.0000e-07\n",
      "Epoch 336/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1045 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.0000e-07\n",
      "Epoch 337/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1045 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.0000e-07\n",
      "Epoch 338/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.0000e-07\n",
      "Epoch 339/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1046 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.0000e-07\n",
      "Epoch 340/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1045 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.0000e-07\n",
      "Epoch 341/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1045 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.0000e-07\n",
      "Epoch 342/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1045 - val_loss: 0.0325 - val_mse: 0.0288 - val_mae: 0.1138 - lr: 1.0000e-07\n",
      "Epoch 343/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0265 - mse: 0.0229 - mae: 0.1045 - val_loss: 0.0325 - val_mse: 0.0287 - val_mae: 0.1138 - lr: 1.0000e-07\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=500,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[adjust_lr, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0325 - mse: 0.0288 - mae: 0.1138\n",
      "Test MSE: 0.0288, Test MAE: 0.1138\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "test_loss, test_mse, test_mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test MSE: {test_mse:.4f}, Test MAE: {test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv('model_ANN_history.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 267ms/step\n",
      "\n",
      "Sample Predictions:\n",
      "Sample 1:\n",
      "Original : [3 0 1 1 3 0 1 0 0 0 4 0 4 3 1 1]\n",
      "Predicted: [4 1 3 4 5 1 1 0 0 0 6 0 6 6 2 2]\n",
      "\n",
      "Sample 2:\n",
      "Original : [0 4 0 1 4 3 4 4 3 3 1 3 2 2 1 0]\n",
      "Predicted: [3 5 2 1 4 5 7 6 5 5 1 5 4 3 2 0]\n",
      "\n",
      "Sample 3:\n",
      "Original : [4 2 4 2 1 3 0 3 0 4 4 0 3 0 1 4]\n",
      "Predicted: [4 3 5 3 3 5 0 6 0 7 7 0 5 0 2 7]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def decode_predictions(predictions, q=8):\n",
    "    # denormalize and round to nearest integer\n",
    "    decoded = np.round(predictions * (q - 1)).astype(int)\n",
    "    # clip values to ensure they are within [0, q-1]\n",
    "    decoded = np.clip(decoded, 0, q-1)\n",
    "    return decoded\n",
    "\n",
    "# samples\n",
    "sample_indices = np.random.choice(len(X_test), 3)\n",
    "sample_X = X_test[sample_indices]\n",
    "sample_y_true = y_test[sample_indices]\n",
    "\n",
    "# Predict and decode\n",
    "sample_y_pred = model.predict(sample_X)\n",
    "sample_y_pred_decoded = decode_predictions(sample_y_pred)\n",
    "\n",
    "# Compare with ground truth\n",
    "print(\"\\nSample Predictions:\")\n",
    "for i in range(len(sample_indices)):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(\"Original :\", np.round(sample_y_true[i] * (q-1)).astype(int))\n",
    "    print(\"Predicted:\", sample_y_pred_decoded[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
