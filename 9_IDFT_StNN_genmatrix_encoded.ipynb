{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8twykWyP4sb"
      },
      "source": [
        "### DFTSNN - DFT Encoded dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6fHPxK2P4sf"
      },
      "source": [
        "The imaginary and real values are parellel processed until concatenation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "VTBoQRiKP4sg"
      },
      "outputs": [],
      "source": [
        "# (r + 1) | (q - 1)\n",
        "# (r + 1) | n\n",
        "# q is a prime number\n",
        "# n = 2^t\n",
        "# r < n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_JFe59EP4sh",
        "outputId": "a84361f4-a027-495d-d410-0e50d95adeb2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31442"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "%reset -f\n",
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "9I3hZpocP4sj"
      },
      "outputs": [],
      "source": [
        "w0 = 4\n",
        "z0 = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzmAn2qrP4sj",
        "outputId": "19375ad3-ae6a-4090-ec1f-4c144ab7081a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original n: 27\n",
            "Padded n: 32\n",
            "Generated dataset shape: (1000, 32)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "n = 27\n",
        "q = 7\n",
        "num_samples = 1000\n",
        "\n",
        "def next_power_of_two(x):\n",
        "    return 1 if x == 0 else 2**(x - 1).bit_length()\n",
        "\n",
        "n_padded = next_power_of_two(n)\n",
        "\n",
        "dataset = np.random.randint(0, q, size=(num_samples, n))\n",
        "\n",
        "if n_padded > n:\n",
        "    pad_width = n_padded - n\n",
        "    dataset = np.pad(dataset, ((0, 0), (0, pad_width)), mode='constant', constant_values=0)\n",
        "\n",
        "print(\"Original n:\", n)\n",
        "print(\"Padded n:\", n_padded)\n",
        "print(\"Generated dataset shape:\", dataset.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KplhSnaP4sk",
        "outputId": "b98dddb7-9d6e-47eb-f537-7c9581a28dcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[6 3 4 ... 0 0 0]\n",
            " [1 5 4 ... 0 0 0]\n",
            " [0 6 1 ... 0 0 0]\n",
            " ...\n",
            " [4 1 1 ... 0 0 0]\n",
            " [6 1 2 ... 0 0 0]\n",
            " [1 5 4 ... 0 0 0]]\n",
            "(1000, 32)\n",
            "(32,)\n"
          ]
        }
      ],
      "source": [
        "print(dataset)\n",
        "print(dataset.shape)\n",
        "print(dataset[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk6Ngc1WP4sl"
      },
      "source": [
        "$$\n",
        "\\tilde{M}_{kj} = \\left[ \\left( \\frac{w_0}{z_0} \\right)^j \\zeta^{kj} \\right]_{k,j=0}^{n-1}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "j02vexbkP4sl"
      },
      "outputs": [],
      "source": [
        "def padded_generator_matrix(N, w0, z0):\n",
        "    n = np.arange(N)\n",
        "    k = n.reshape((N, 1))\n",
        "    zeta = np.exp(-2j * np.pi / N)\n",
        "    M_tilde = ((w0 / z0) ** n) * (zeta ** (k * n))\n",
        "    return M_tilde"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvL1X2UNP4sl",
        "outputId": "d70736c8-be4f-4ea3-92e6-7ae6fcda4cc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 32)\n"
          ]
        }
      ],
      "source": [
        "M_tilde = padded_generator_matrix(n_padded, w0, z0)\n",
        "print(M_tilde.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXtND8r0P4sm",
        "outputId": "5a314f0d-979e-4a6f-fa0c-326d83a0b933"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 18563.60052945    +0.j          -3175.07412411+14542.65392496j\n",
            "   -9480.13851167 -1549.89213738j ...   -850.18419129 +5788.51248279j\n",
            "   -9480.13851167 +1549.89213738j  -3175.07412411-14542.65392496j]\n",
            " [ 26201.57565547    +0.j           -888.79870167+21381.54383229j\n",
            "  -15711.89592314 +4225.02464251j ...  -8448.43948841+12321.54894128j\n",
            "  -15711.89592314 -4225.02464251j   -888.79870167-21381.54383229j]\n",
            " [ 18708.3418798     +0.j          -3692.76914231+14315.81246485j\n",
            "   -8001.63897938 -1445.78154582j ...  -2196.87866369 +4388.0650906j\n",
            "   -8001.63897938 +1445.78154582j  -3692.76914231-14315.81246485j]\n",
            " ...\n",
            " [ 23026.93389135    +0.j          -1109.76273792+19062.65254237j\n",
            "  -13040.7009937  +1311.43274328j ...  -3589.38928495 +6599.51636282j\n",
            "  -13040.7009937  -1311.43274328j  -1109.76273792-19062.65254237j]\n",
            " [ 13466.17623911    +0.j          -3519.32538663 +9603.71035865j\n",
            "   -6399.29947006 -1233.3778371j  ...  -1243.67319172 +6320.28545061j\n",
            "   -6399.29947006 +1233.3778371j   -3519.32538663 -9603.71035865j]\n",
            " [ 14588.02290132    +0.j          -1642.56444197+11272.19910547j\n",
            "   -6953.08933372  +889.35665809j ...  -4636.08674653 +4233.01026956j\n",
            "   -6953.08933372  -889.35665809j  -1642.56444197-11272.19910547j]]\n"
          ]
        }
      ],
      "source": [
        "encoded_dataset = np.array([np.dot(M_tilde, x) for x in dataset])\n",
        "encoded_dataset[np.abs(encoded_dataset) < 1e-10] = 0\n",
        "encoded_dataset = np.round(encoded_dataset, decimals=10)\n",
        "print(encoded_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pz2vHXGcP4sm",
        "outputId": "a57fa0cf-1121-480a-b163-1c1a39f992a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 32)\n"
          ]
        }
      ],
      "source": [
        "print(encoded_dataset.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EABkoSUVP4sm",
        "outputId": "fa607138-b6da-47e1-dda7-e3313a39cf79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6 3 4 6 2 4 4 6 1 2 6 2 2 4 3 2 5 4 1 3 5 5 1 3 4 0 3 0 0 0 0 0]\n",
            "[18563.60052945    +0.j         -3175.07412411+14542.65392496j\n",
            " -9480.13851167 -1549.89213738j  -850.18419129 -5788.51248279j\n",
            "  2813.63372348 -2457.36436449j  3312.0476391   +337.88077825j\n",
            "  1059.2907911  +3489.68169028j -2366.82367016  +909.74502726j\n",
            "  -210.29906943  +357.85181443j -3878.07651347  +373.95548592j\n",
            "   -49.25036994 -6370.32567308j  6901.71964373 -1088.5938127j\n",
            "  2896.76301761 +7252.43377368j -6714.83312544 +4427.90575271j\n",
            " -5496.28870703 -5330.43278044j  2828.24226801 -6162.17891893j\n",
            "  6446.94186958    -0.j          2828.24226801 +6162.17891893j\n",
            " -5496.28870703 +5330.43278044j -6714.83312544 -4427.90575271j\n",
            "  2896.76301761 -7252.43377368j  6901.71964373 +1088.5938127j\n",
            "   -49.25036994 +6370.32567308j -3878.07651347  -373.95548592j\n",
            "  -210.29906943  -357.85181443j -2366.82367016  -909.74502726j\n",
            "  1059.2907911  -3489.68169028j  3312.0476391   -337.88077825j\n",
            "  2813.63372348 +2457.36436449j  -850.18419129 +5788.51248279j\n",
            " -9480.13851167 +1549.89213738j -3175.07412411-14542.65392496j]\n"
          ]
        }
      ],
      "source": [
        "print(dataset[0])\n",
        "print(encoded_dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcXxvLEBP4sn",
        "outputId": "af5c4478-eeb8-4777-c7e7-34407c238524"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "X_train_real : (1000, 32)\n",
            "[18563.6     -3175.0742  -9480.139    -850.1842   2813.6338   3312.0476\n",
            "  1059.2908  -2366.8237   -210.29907 -3878.0764    -49.25037  6901.7197\n",
            "  2896.763   -6714.833   -5496.2886   2828.2422   6446.942    2828.2422\n",
            " -5496.2886  -6714.833    2896.763    6901.7197    -49.25037 -3878.0764\n",
            "  -210.29907 -2366.8237   1059.2908   3312.0476   2813.6338   -850.1842\n",
            " -9480.139   -3175.0742 ]\n",
            "\n",
            "X_train_imag : (1000, 32)\n",
            "[     0.       14542.654    -1549.8921   -5788.5127   -2457.3643\n",
            "    337.88077   3489.6816     909.74506    357.8518     373.95547\n",
            "  -6370.3257   -1088.5939    7252.4336    4427.906    -5330.4326\n",
            "  -6162.1787      -0.        6162.1787    5330.4326   -4427.906\n",
            "  -7252.4336    1088.5939    6370.3257    -373.95547   -357.8518\n",
            "   -909.74506  -3489.6816    -337.88077   2457.3643    5788.5127\n",
            "   1549.8921  -14542.654  ]\n"
          ]
        }
      ],
      "source": [
        "X_real = np.real(encoded_dataset).astype(np.float32)\n",
        "X_imag = np.imag(encoded_dataset).astype(np.float32)\n",
        "\n",
        "print(\"\\nX_train_real :\", X_real.shape)\n",
        "print(X_real[0])\n",
        "print(\"\\nX_train_imag :\", X_imag.shape)\n",
        "print(X_imag[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oszMd19P4sn",
        "outputId": "6775d1f3-c52b-4027-d560-9b8bc3cf2d41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.5349025  -0.6596515   0.7757606   0.5577905  -0.9361864  -0.52822787\n",
            "  0.26922125  0.5281825   0.72306824 -1.3387995  -0.9591273   1.0555125\n",
            "  1.3157916  -0.90254843 -0.8986112   0.8085635   0.6702558   0.8085635\n",
            " -0.8986112  -0.90254843  1.3157916   1.0555125  -0.9591273  -1.3387995\n",
            "  0.72306824  0.5281825   0.26922125 -0.52822787 -0.9361864   0.5577905\n",
            "  0.7757606  -0.6596515 ]\n",
            "[ 0.0000000e+00 -6.2298882e-01 -6.1096108e-01  9.1586119e-01\n",
            "  5.5390155e-01 -7.0446509e-01 -4.5943576e-01 -2.8642637e-01\n",
            "  9.1341501e-01  1.0008837e+00 -1.3572260e+00 -1.1246126e+00\n",
            "  9.5678961e-01  1.1653606e+00 -9.0699339e-01 -7.2105145e-01\n",
            "  1.9968052e-04  7.2105145e-01  9.0699339e-01 -1.1653606e+00\n",
            " -9.5678961e-01  1.1246126e+00  1.3572260e+00 -1.0008837e+00\n",
            " -9.1341501e-01  2.8642637e-01  4.5943576e-01  7.0446509e-01\n",
            " -5.5390155e-01 -9.1586119e-01  6.1096108e-01  6.2298882e-01]\n"
          ]
        }
      ],
      "source": [
        "mean_real = np.mean(X_real, axis=0)\n",
        "std_real = np.std(X_real, axis=0) + 1e-8  # Avoid division by zero\n",
        "X_real = (X_real - mean_real) / std_real\n",
        "\n",
        "mean_imag = np.mean(X_imag, axis=0)\n",
        "std_imag = np.std(X_imag, axis=0) + 1e-8  # Avoid division by zero\n",
        "X_imag = (X_imag - mean_imag) / std_imag\n",
        "\n",
        "print(X_real[0])\n",
        "print(X_imag[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDBXE_VsP4sn",
        "outputId": "e0b1c6c2-c9b6-46ba-ff72-31b47a77c941"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y: \n",
            "[6. 3. 4. 6. 2. 4. 4. 6. 1. 2. 6. 2. 2. 4. 3. 2. 5. 4. 1. 3. 5. 5. 1. 3.\n",
            " 4. 0. 3. 0. 0. 0. 0. 0.]\n",
            "\n",
            "y_normalized: \n",
            "[1.         0.5        0.6666667  1.         0.33333334 0.6666667\n",
            " 0.6666667  1.         0.16666667 0.33333334 1.         0.33333334\n",
            " 0.33333334 0.6666667  0.5        0.33333334 0.8333333  0.6666667\n",
            " 0.16666667 0.5        0.8333333  0.8333333  0.16666667 0.5\n",
            " 0.6666667  0.         0.5        0.         0.         0.\n",
            " 0.         0.        ]\n"
          ]
        }
      ],
      "source": [
        "# normalize target data (integers 0-q to [0, 1])\n",
        "y_normalized = dataset.astype(np.float32) / (q - 1)  # Scale to [0, 1]\n",
        "print(\"y: \")\n",
        "print(dataset.astype(np.float32)[0])\n",
        "print(\"\\ny_normalized: \")\n",
        "print(y_normalized[0])\n",
        "\n",
        "labels = y_normalized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJLXeIZXP4so",
        "outputId": "a894dc41-fe3c-4a98-902b-8fe46823e222"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shapes: X_real: (800, 32) X_imag: (800, 32) y: (800, 32)\n",
            "Testing data shapes: X_real: (200, 32) X_imag: (200, 32) y: (200, 32)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_real_train, X_real_test, y_train, y_test = train_test_split(\n",
        "    X_real, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "X_imag_train, X_imag_test, _, _ = train_test_split(\n",
        "    X_imag, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Training data shapes: X_real:\", X_real_train.shape, \"X_imag:\", X_imag_train.shape, \"y:\", y_train.shape)\n",
        "print(\"Testing data shapes: X_real:\", X_real_test.shape, \"X_imag:\", X_imag_test.shape, \"y:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSIv6cPUP4so",
        "outputId": "ae98709c-25d1-4553-93cf-2ecee1ca3f5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n"
          ]
        }
      ],
      "source": [
        "print(X_real_train.shape[1])\n",
        "print(X_real_test.shape[1])\n",
        "print(X_imag_train.shape[1])\n",
        "print(X_imag_test.shape[1])\n",
        "print(y_train.shape[1])\n",
        "print(y_test.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Snnk7Bh-P4so",
        "outputId": "35ab0046-ac24-4d27-ce93-80ba947d720d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n",
            "32\n"
          ]
        }
      ],
      "source": [
        "input_dim = X_real_train.shape[1]\n",
        "output_dim = y_train.shape[1]\n",
        "print(input_dim)\n",
        "print(output_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1d1GgSsP4so"
      },
      "source": [
        "IDFT - Structure Imposed Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "6hGEhIomP4so"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "j86PMbDuP4sp"
      },
      "outputs": [],
      "source": [
        "class FirstLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, units, kernel_initializer='he_normal', bias_initializer='zeros', use_bias=True, **kwargs):\n",
        "        super(FirstLayer, self).__init__(**kwargs)\n",
        "        self.units = units  # Features/neurons\n",
        "        self.kernel_initializer = kernel_initializer\n",
        "        self.bias_initializer = bias_initializer\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        n = self.units\n",
        "        n1 = n // 2\n",
        "        num_blocks = n1 // 2\n",
        "\n",
        "        self.b_1 = self.add_weight(name=\"kernel_b1\",\n",
        "                                   shape=(num_blocks, 2, 2),\n",
        "                                   initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
        "                                   trainable=True)\n",
        "        self.b_2 = self.add_weight(name=\"kernel_b2\",\n",
        "                                   shape=(num_blocks, 2, 2),\n",
        "                                   initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
        "                                   trainable=True)\n",
        "        self.d_1 = self.add_weight(name=\"kernel_d1\",\n",
        "                                   shape=(n1 - 2,),\n",
        "                                   initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
        "                                   trainable=True)\n",
        "        self.d_2 = self.add_weight(name=\"kernel_d2\",\n",
        "                                   shape=(n1 - 2,),\n",
        "                                   initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
        "                                   trainable=True)\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(name=\"bias\",\n",
        "                                        shape=(self.units,),\n",
        "                                        initializer=tf.keras.initializers.get(self.bias_initializer),\n",
        "                                        trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        def recursiveIDFT(inputVector, B, d, block_index):\n",
        "            n = inputVector.shape[1]\n",
        "            n1 = n // 2\n",
        "\n",
        "            if n == 2:\n",
        "                out = tf.matmul(inputVector, B[block_index])\n",
        "                return out\n",
        "            else:\n",
        "                q = tf.concat([inputVector[:, ::2], inputVector[:, 1::2]], axis=1)\n",
        "\n",
        "                B1 = recursiveIDFT(q[:, :n1], B, d[n1:], block_index)\n",
        "                B2 = recursiveIDFT(q[:, n1:], B, d[n1:], block_index + (n // 4))\n",
        "\n",
        "                d_n = tf.reshape(d[:n1], (1, -1))\n",
        "                z1 = tf.concat([(B1 + tf.multiply(B2, d_n)), (B1 - tf.multiply(B2, d_n))], axis=1)\n",
        "\n",
        "                return z1 / tf.sqrt(tf.constant(2.0, dtype=tf.float32))\n",
        "\n",
        "        n = self.units\n",
        "        n1 = n // 2\n",
        "\n",
        "        q = tf.concat([inputs[:, ::2], inputs[:, 1::2]], axis=1)\n",
        "\n",
        "        B1 = recursiveIDFT(q[:, :n1], self.b_1, self.d_1, block_index=0)\n",
        "        B2 = recursiveIDFT(q[:, n1:], self.b_2, self.d_2, block_index=0)\n",
        "\n",
        "        out = tf.concat([B1, B2], axis=1)\n",
        "\n",
        "        if self.use_bias:\n",
        "            out += self.bias\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "5m0COEYYP4sp"
      },
      "outputs": [],
      "source": [
        "class SecondLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, units, kernel_initializer='he_normal', bias_initializer='zeros', use_bias=True, **kwargs):\n",
        "        super(SecondLayer, self).__init__(**kwargs)\n",
        "        self.units = units  # Features/neurons\n",
        "        self.kernel_initializer = kernel_initializer\n",
        "        self.bias_initializer = bias_initializer\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        n = self.units\n",
        "        n1 = n // 2\n",
        "\n",
        "        self.d_1 = self.add_weight(name=\"kernel_d1\",\n",
        "                                   shape=(n1,),\n",
        "                                   initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
        "                                   trainable=True)\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(name=\"bias\",\n",
        "                                        shape=(n,),\n",
        "                                        initializer=tf.keras.initializers.get(self.bias_initializer),\n",
        "                                        trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        out1 = inputs[:, :int(inputs.shape[1] / 2)]\n",
        "        out2 = inputs[:, int(inputs.shape[1] / 2):]\n",
        "\n",
        "        z1 = tf.concat([(out1 + tf.multiply(out2, self.d_1)), (out1 - tf.multiply(out2, self.d_1))], axis=1)\n",
        "        out = z1 / tf.sqrt(tf.constant(2.0, dtype=tf.float32))\n",
        "\n",
        "        if self.use_bias:\n",
        "            out += self.bias\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "h2ZTRoT3P4sp"
      },
      "outputs": [],
      "source": [
        "class CustomLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, units, kernel_initializer='he_normal', bias_initializer='zeros', use_bias=True, **kwargs):\n",
        "        super(CustomLayer, self).__init__(**kwargs)\n",
        "        self.units = units  # features/neurons\n",
        "        self.kernel_initializer = kernel_initializer\n",
        "        self.bias_initializer = bias_initializer\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        n = self.units\n",
        "\n",
        "        self.m = self.add_weight(name=\"kernel_m\",\n",
        "                                 shape=(n,),\n",
        "                                 initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
        "                                 trainable=True)\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(name=\"bias\",\n",
        "                                        shape=(self.units,),\n",
        "                                        initializer=tf.keras.initializers.get(self.bias_initializer),\n",
        "                                        trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        out = tf.multiply(inputs, self.m)\n",
        "        if self.use_bias:\n",
        "            out += self.bias\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "pZFra2-PP4sp"
      },
      "outputs": [],
      "source": [
        "class ScalingLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, units, initial_scale=0.1, kernel_initializer='ones', bias_initializer='zeros', use_bias=True, **kwargs):\n",
        "        super(ScalingLayer, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.kernel_initializer = kernel_initializer\n",
        "        self.bias_initializer = bias_initializer\n",
        "        self.use_bias = use_bias\n",
        "        self.initial_scale = initial_scale\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        n = self.units\n",
        "\n",
        "        self.m = self.add_weight(name=\"kernel_m_1\",\n",
        "                                 shape=(n,),\n",
        "                                 initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
        "                                 regularizer=tf.keras.regularizers.l2(1e-4),\n",
        "                                 trainable=True)\n",
        "\n",
        "        # self.scale = self.add_weight(name=\"kernel_scale\",\n",
        "        #                              shape=(),\n",
        "        #                              initializer=tf.keras.initializers.Constant(1.0),\n",
        "        #                              regularizer=tf.keras.regularizers.l2(1e-4),\n",
        "        #                              trainable=True)\n",
        "\n",
        "        self.log_scale = self.add_weight(\n",
        "                                  shape=(),\n",
        "                                  initializer=tf.keras.initializers.Constant(self.initial_scale),\n",
        "                                  trainable=True,\n",
        "                                  name=\"log_scale\",\n",
        "                                  )\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(name=\"bias\",\n",
        "                                        shape=(self.units,),\n",
        "                                        initializer=tf.keras.initializers.get(self.bias_initializer),\n",
        "                                        trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # print(\"linear layer input:\", inputs.shape)\n",
        "\n",
        "        inputs1 = inputs[:, :int(inputs.shape[1] / 2)]\n",
        "        inputs2 = inputs[:, int(inputs.shape[1] / 2):]\n",
        "\n",
        "        out = tf.math.sqrt(inputs1**2 + inputs2**2)\n",
        "\n",
        "        dim = tf.shape(out)[1]\n",
        "        # D_hat_n = tf.pow(self.scale, tf.cast(tf.range(dim), dtype=tf.float32))\n",
        "        D_hat_n = tf.exp(self.log_scale * tf.cast(tf.range(dim), tf.float32))\n",
        "\n",
        "        out2 = out * D_hat_n * self.m\n",
        "\n",
        "        # out = tf.multiply(out, self.m) # diagonal scaling\n",
        "\n",
        "        if self.use_bias:\n",
        "            out2 += self.bias\n",
        "\n",
        "        # print(\"linear layer output:\", out.shape)\n",
        "        return out2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfYlzemAP4sq"
      },
      "source": [
        "Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "id": "t1H7vNT0P4sq",
        "outputId": "44e1c882-cdf6-473b-d6c7-800bd1d7c2fb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ real_input (\u001b[38;5;33mInputLayer\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ imag_input (\u001b[38;5;33mInputLayer\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ real_layer1 (\u001b[38;5;33mFirstLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │            \u001b[38;5;34m124\u001b[0m │ real_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ imag_layer1 (\u001b[38;5;33mFirstLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │            \u001b[38;5;34m124\u001b[0m │ imag_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ leaky_re_lu_4 (\u001b[38;5;33mLeakyReLU\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ real_layer1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ leaky_re_lu_7 (\u001b[38;5;33mLeakyReLU\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ imag_layer1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ real_support_layer_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m64\u001b[0m │ leaky_re_lu_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mCustomLayer\u001b[0m)             │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ imag_support_layer_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m64\u001b[0m │ leaky_re_lu_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mCustomLayer\u001b[0m)             │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ leaky_re_lu_5 (\u001b[38;5;33mLeakyReLU\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ real_support_layer_1[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ leaky_re_lu_8 (\u001b[38;5;33mLeakyReLU\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ imag_support_layer_1[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ real_layer2 (\u001b[38;5;33mSecondLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m48\u001b[0m │ leaky_re_lu_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ imag_layer2 (\u001b[38;5;33mSecondLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m48\u001b[0m │ leaky_re_lu_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ leaky_re_lu_6 (\u001b[38;5;33mLeakyReLU\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ real_layer2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ leaky_re_lu_9 (\u001b[38;5;33mLeakyReLU\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ imag_layer2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ merge_real_imag           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ leaky_re_lu_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ leaky_re_lu_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ output_layer              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m65\u001b[0m │ merge_real_imag[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "│ (\u001b[38;5;33mScalingLayer\u001b[0m)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ output_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ real_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ imag_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ real_layer1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">FirstLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span> │ real_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ imag_layer1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">FirstLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span> │ imag_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ leaky_re_lu_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ real_layer1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ leaky_re_lu_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ imag_layer1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ real_support_layer_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ leaky_re_lu_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CustomLayer</span>)             │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ imag_support_layer_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ leaky_re_lu_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CustomLayer</span>)             │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ leaky_re_lu_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ real_support_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ leaky_re_lu_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ imag_support_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ real_layer2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SecondLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span> │ leaky_re_lu_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ imag_layer2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SecondLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span> │ leaky_re_lu_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ leaky_re_lu_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ real_layer2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ leaky_re_lu_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ imag_layer2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ merge_real_imag           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ leaky_re_lu_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ leaky_re_lu_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ output_layer              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ merge_real_imag[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ScalingLayer</span>)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ output_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m537\u001b[0m (2.10 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">537</span> (2.10 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m537\u001b[0m (2.10 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">537</span> (2.10 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Concatenate, LeakyReLU, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "global initial_log_scale\n",
        "initial_log_scale = np.log(z0 / w0)\n",
        "\n",
        "def structured_NN(input_dim, output_dim):\n",
        "    real_input = Input(shape=(input_dim,), name=\"real_input\")\n",
        "    imag_input = Input(shape=(input_dim,), name=\"imag_input\")\n",
        "\n",
        "    real_x = FirstLayer(units=input_dim, kernel_initializer='he_normal', bias_initializer='zeros', name=\"real_layer1\")(real_input)\n",
        "    real_x = LeakyReLU(alpha=0.1)(real_x)\n",
        "    real_x = CustomLayer(units=input_dim, kernel_initializer='he_normal', bias_initializer='zeros', name=\"real_support_layer_1\")(real_x)\n",
        "    real_x = LeakyReLU(alpha=0.1)(real_x)\n",
        "    real_x = SecondLayer(units=input_dim, kernel_initializer='he_normal', bias_initializer='zeros', name=\"real_layer2\")(real_x)\n",
        "    real_x = LeakyReLU(alpha=0.1)(real_x)\n",
        "\n",
        "    imag_x = FirstLayer(units=input_dim, kernel_initializer='he_normal', bias_initializer='zeros', name=\"imag_layer1\")(imag_input)\n",
        "    imag_x = LeakyReLU(alpha=0.1)(imag_x)\n",
        "    imag_x = CustomLayer(units=input_dim, kernel_initializer='he_normal', bias_initializer='zeros', name=\"imag_support_layer_1\")(imag_x)\n",
        "    imag_x = LeakyReLU(alpha=0.1)(imag_x)\n",
        "    imag_x = SecondLayer(units=input_dim, kernel_initializer='he_normal', bias_initializer='zeros', name=\"imag_layer2\")(imag_x)\n",
        "    imag_x = LeakyReLU(alpha=0.1)(imag_x)\n",
        "\n",
        "    merged = Concatenate(name=\"merge_real_imag\")([real_x, imag_x])\n",
        "\n",
        "    output = ScalingLayer(units=output_dim, initial_scale=initial_log_scale, kernel_initializer='ones', bias_initializer='zeros', name=\"output_layer\")(merged)\n",
        "    output = Activation('linear')(output)\n",
        "\n",
        "    model = Model(inputs=[real_input, imag_input], outputs=output)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mse', 'mae'])\n",
        "\n",
        "    return model\n",
        "\n",
        "input_dim = X_real_train.shape[1]\n",
        "output_dim = y_train.shape[1]\n",
        "model = structured_NN(input_dim, output_dim)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsjEhlGcP4sr"
      },
      "source": [
        "Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "kzJ_harIP4sr"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "adjust_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=5,\n",
        "    min_lr=1e-7,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGsWtAjdP4sr",
        "outputId": "bda31b14-4771-41f0-8e09-e0f2b1533946"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 92ms/step - loss: 0.3016 - mae: 0.4182 - mse: 0.2984 - val_loss: 0.2723 - val_mae: 0.3935 - val_mse: 0.2692 - learning_rate: 0.0010\n",
            "Epoch 2/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2751 - mae: 0.3972 - mse: 0.2720 - val_loss: 0.2440 - val_mae: 0.3705 - val_mse: 0.2409 - learning_rate: 0.0010\n",
            "Epoch 3/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2452 - mae: 0.3722 - mse: 0.2421 - val_loss: 0.2152 - val_mae: 0.3458 - val_mse: 0.2121 - learning_rate: 0.0010\n",
            "Epoch 4/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2151 - mae: 0.3466 - mse: 0.2120 - val_loss: 0.1839 - val_mae: 0.3180 - val_mse: 0.1807 - learning_rate: 0.0010\n",
            "Epoch 5/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1811 - mae: 0.3165 - mse: 0.1778 - val_loss: 0.1469 - val_mae: 0.2858 - val_mse: 0.1435 - learning_rate: 0.0010\n",
            "Epoch 6/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1427 - mae: 0.2842 - mse: 0.1393 - val_loss: 0.1104 - val_mae: 0.2561 - val_mse: 0.1068 - learning_rate: 0.0010\n",
            "Epoch 7/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1075 - mae: 0.2536 - mse: 0.1039 - val_loss: 0.1018 - val_mae: 0.2502 - val_mse: 0.0981 - learning_rate: 0.0010\n",
            "Epoch 8/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1004 - mae: 0.2480 - mse: 0.0968 - val_loss: 0.0987 - val_mae: 0.2465 - val_mse: 0.0951 - learning_rate: 0.0010\n",
            "Epoch 9/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0986 - mae: 0.2449 - mse: 0.0950 - val_loss: 0.0981 - val_mae: 0.2458 - val_mse: 0.0946 - learning_rate: 0.0010\n",
            "Epoch 10/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0983 - mae: 0.2453 - mse: 0.0948 - val_loss: 0.0976 - val_mae: 0.2452 - val_mse: 0.0942 - learning_rate: 0.0010\n",
            "Epoch 11/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0964 - mae: 0.2419 - mse: 0.0930 - val_loss: 0.0973 - val_mae: 0.2448 - val_mse: 0.0938 - learning_rate: 0.0010\n",
            "Epoch 12/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0962 - mae: 0.2420 - mse: 0.0928 - val_loss: 0.0972 - val_mae: 0.2448 - val_mse: 0.0938 - learning_rate: 0.0010\n",
            "Epoch 13/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0965 - mae: 0.2426 - mse: 0.0931 - val_loss: 0.0965 - val_mae: 0.2439 - val_mse: 0.0932 - learning_rate: 0.0010\n",
            "Epoch 14/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0962 - mae: 0.2418 - mse: 0.0928 - val_loss: 0.0962 - val_mae: 0.2433 - val_mse: 0.0929 - learning_rate: 0.0010\n",
            "Epoch 15/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0954 - mae: 0.2407 - mse: 0.0921 - val_loss: 0.0956 - val_mae: 0.2425 - val_mse: 0.0924 - learning_rate: 0.0010\n",
            "Epoch 16/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0951 - mae: 0.2405 - mse: 0.0919 - val_loss: 0.0951 - val_mae: 0.2415 - val_mse: 0.0918 - learning_rate: 0.0010\n",
            "Epoch 17/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0950 - mae: 0.2403 - mse: 0.0917 - val_loss: 0.0944 - val_mae: 0.2403 - val_mse: 0.0912 - learning_rate: 0.0010\n",
            "Epoch 18/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0925 - mae: 0.2360 - mse: 0.0893 - val_loss: 0.0940 - val_mae: 0.2394 - val_mse: 0.0908 - learning_rate: 0.0010\n",
            "Epoch 19/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0934 - mae: 0.2374 - mse: 0.0902 - val_loss: 0.0929 - val_mae: 0.2377 - val_mse: 0.0898 - learning_rate: 0.0010\n",
            "Epoch 20/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0917 - mae: 0.2345 - mse: 0.0885 - val_loss: 0.0924 - val_mae: 0.2367 - val_mse: 0.0894 - learning_rate: 0.0010\n",
            "Epoch 21/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0915 - mae: 0.2346 - mse: 0.0884 - val_loss: 0.0921 - val_mae: 0.2360 - val_mse: 0.0890 - learning_rate: 0.0010\n",
            "Epoch 22/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0907 - mae: 0.2330 - mse: 0.0876 - val_loss: 0.0912 - val_mae: 0.2346 - val_mse: 0.0882 - learning_rate: 0.0010\n",
            "Epoch 23/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0901 - mae: 0.2315 - mse: 0.0870 - val_loss: 0.0906 - val_mae: 0.2335 - val_mse: 0.0876 - learning_rate: 0.0010\n",
            "Epoch 24/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0913 - mae: 0.2335 - mse: 0.0883 - val_loss: 0.0901 - val_mae: 0.2323 - val_mse: 0.0871 - learning_rate: 0.0010\n",
            "Epoch 25/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0890 - mae: 0.2294 - mse: 0.0860 - val_loss: 0.0892 - val_mae: 0.2309 - val_mse: 0.0863 - learning_rate: 0.0010\n",
            "Epoch 26/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0874 - mae: 0.2266 - mse: 0.0845 - val_loss: 0.0886 - val_mae: 0.2295 - val_mse: 0.0857 - learning_rate: 0.0010\n",
            "Epoch 27/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0870 - mae: 0.2258 - mse: 0.0841 - val_loss: 0.0881 - val_mae: 0.2284 - val_mse: 0.0852 - learning_rate: 0.0010\n",
            "Epoch 28/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0869 - mae: 0.2252 - mse: 0.0841 - val_loss: 0.0876 - val_mae: 0.2273 - val_mse: 0.0847 - learning_rate: 0.0010\n",
            "Epoch 29/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0863 - mae: 0.2239 - mse: 0.0835 - val_loss: 0.0869 - val_mae: 0.2262 - val_mse: 0.0841 - learning_rate: 0.0010\n",
            "Epoch 30/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0866 - mae: 0.2245 - mse: 0.0838 - val_loss: 0.0863 - val_mae: 0.2250 - val_mse: 0.0835 - learning_rate: 0.0010\n",
            "Epoch 31/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0852 - mae: 0.2218 - mse: 0.0824 - val_loss: 0.0856 - val_mae: 0.2236 - val_mse: 0.0829 - learning_rate: 0.0010\n",
            "Epoch 32/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0852 - mae: 0.2216 - mse: 0.0825 - val_loss: 0.0850 - val_mae: 0.2223 - val_mse: 0.0823 - learning_rate: 0.0010\n",
            "Epoch 33/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0834 - mae: 0.2187 - mse: 0.0807 - val_loss: 0.0839 - val_mae: 0.2204 - val_mse: 0.0812 - learning_rate: 0.0010\n",
            "Epoch 34/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0833 - mae: 0.2177 - mse: 0.0806 - val_loss: 0.0830 - val_mae: 0.2183 - val_mse: 0.0804 - learning_rate: 0.0010\n",
            "Epoch 35/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0824 - mae: 0.2163 - mse: 0.0797 - val_loss: 0.0820 - val_mae: 0.2163 - val_mse: 0.0793 - learning_rate: 0.0010\n",
            "Epoch 36/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0810 - mae: 0.2137 - mse: 0.0783 - val_loss: 0.0810 - val_mae: 0.2142 - val_mse: 0.0784 - learning_rate: 0.0010\n",
            "Epoch 37/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0799 - mae: 0.2110 - mse: 0.0773 - val_loss: 0.0802 - val_mae: 0.2123 - val_mse: 0.0777 - learning_rate: 0.0010\n",
            "Epoch 38/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0793 - mae: 0.2094 - mse: 0.0767 - val_loss: 0.0794 - val_mae: 0.2106 - val_mse: 0.0768 - learning_rate: 0.0010\n",
            "Epoch 39/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0792 - mae: 0.2094 - mse: 0.0766 - val_loss: 0.0786 - val_mae: 0.2088 - val_mse: 0.0761 - learning_rate: 0.0010\n",
            "Epoch 40/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0780 - mae: 0.2062 - mse: 0.0755 - val_loss: 0.0779 - val_mae: 0.2071 - val_mse: 0.0754 - learning_rate: 0.0010\n",
            "Epoch 41/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0774 - mae: 0.2052 - mse: 0.0749 - val_loss: 0.0772 - val_mae: 0.2055 - val_mse: 0.0748 - learning_rate: 0.0010\n",
            "Epoch 42/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0763 - mae: 0.2026 - mse: 0.0738 - val_loss: 0.0766 - val_mae: 0.2038 - val_mse: 0.0741 - learning_rate: 0.0010\n",
            "Epoch 43/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0760 - mae: 0.2013 - mse: 0.0736 - val_loss: 0.0759 - val_mae: 0.2021 - val_mse: 0.0735 - learning_rate: 0.0010\n",
            "Epoch 44/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0754 - mae: 0.2000 - mse: 0.0730 - val_loss: 0.0754 - val_mae: 0.2008 - val_mse: 0.0730 - learning_rate: 0.0010\n",
            "Epoch 45/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0754 - mae: 0.1996 - mse: 0.0730 - val_loss: 0.0750 - val_mae: 0.1997 - val_mse: 0.0726 - learning_rate: 0.0010\n",
            "Epoch 46/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0747 - mae: 0.1980 - mse: 0.0723 - val_loss: 0.0745 - val_mae: 0.1985 - val_mse: 0.0722 - learning_rate: 0.0010\n",
            "Epoch 47/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0738 - mae: 0.1961 - mse: 0.0715 - val_loss: 0.0741 - val_mae: 0.1976 - val_mse: 0.0718 - learning_rate: 0.0010\n",
            "Epoch 48/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0735 - mae: 0.1952 - mse: 0.0711 - val_loss: 0.0737 - val_mae: 0.1968 - val_mse: 0.0714 - learning_rate: 0.0010\n",
            "Epoch 49/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0734 - mae: 0.1952 - mse: 0.0711 - val_loss: 0.0734 - val_mae: 0.1960 - val_mse: 0.0711 - learning_rate: 0.0010\n",
            "Epoch 50/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0735 - mae: 0.1952 - mse: 0.0712 - val_loss: 0.0731 - val_mae: 0.1956 - val_mse: 0.0709 - learning_rate: 0.0010\n",
            "Epoch 51/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0726 - mae: 0.1937 - mse: 0.0703 - val_loss: 0.0727 - val_mae: 0.1945 - val_mse: 0.0704 - learning_rate: 0.0010\n",
            "Epoch 52/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0717 - mae: 0.1920 - mse: 0.0695 - val_loss: 0.0724 - val_mae: 0.1938 - val_mse: 0.0702 - learning_rate: 0.0010\n",
            "Epoch 53/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0728 - mae: 0.1937 - mse: 0.0706 - val_loss: 0.0720 - val_mae: 0.1929 - val_mse: 0.0698 - learning_rate: 0.0010\n",
            "Epoch 54/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0716 - mae: 0.1914 - mse: 0.0694 - val_loss: 0.0716 - val_mae: 0.1920 - val_mse: 0.0694 - learning_rate: 0.0010\n",
            "Epoch 55/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0715 - mae: 0.1909 - mse: 0.0693 - val_loss: 0.0713 - val_mae: 0.1914 - val_mse: 0.0691 - learning_rate: 0.0010\n",
            "Epoch 56/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0699 - mae: 0.1886 - mse: 0.0678 - val_loss: 0.0710 - val_mae: 0.1903 - val_mse: 0.0688 - learning_rate: 0.0010\n",
            "Epoch 57/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0704 - mae: 0.1887 - mse: 0.0682 - val_loss: 0.0706 - val_mae: 0.1895 - val_mse: 0.0685 - learning_rate: 0.0010\n",
            "Epoch 58/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0700 - mae: 0.1872 - mse: 0.0678 - val_loss: 0.0700 - val_mae: 0.1884 - val_mse: 0.0679 - learning_rate: 0.0010\n",
            "Epoch 59/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0705 - mae: 0.1884 - mse: 0.0684 - val_loss: 0.0697 - val_mae: 0.1877 - val_mse: 0.0676 - learning_rate: 0.0010\n",
            "Epoch 60/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0694 - mae: 0.1871 - mse: 0.0673 - val_loss: 0.0694 - val_mae: 0.1868 - val_mse: 0.0673 - learning_rate: 0.0010\n",
            "Epoch 61/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0696 - mae: 0.1867 - mse: 0.0675 - val_loss: 0.0689 - val_mae: 0.1861 - val_mse: 0.0669 - learning_rate: 0.0010\n",
            "Epoch 62/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0687 - mae: 0.1848 - mse: 0.0667 - val_loss: 0.0684 - val_mae: 0.1852 - val_mse: 0.0664 - learning_rate: 0.0010\n",
            "Epoch 63/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0681 - mae: 0.1837 - mse: 0.0660 - val_loss: 0.0681 - val_mae: 0.1843 - val_mse: 0.0661 - learning_rate: 0.0010\n",
            "Epoch 64/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0676 - mae: 0.1829 - mse: 0.0656 - val_loss: 0.0676 - val_mae: 0.1833 - val_mse: 0.0656 - learning_rate: 0.0010\n",
            "Epoch 65/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0679 - mae: 0.1835 - mse: 0.0659 - val_loss: 0.0670 - val_mae: 0.1821 - val_mse: 0.0650 - learning_rate: 0.0010\n",
            "Epoch 66/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0665 - mae: 0.1806 - mse: 0.0645 - val_loss: 0.0667 - val_mae: 0.1813 - val_mse: 0.0647 - learning_rate: 0.0010\n",
            "Epoch 67/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0667 - mae: 0.1807 - mse: 0.0648 - val_loss: 0.0664 - val_mae: 0.1807 - val_mse: 0.0644 - learning_rate: 0.0010\n",
            "Epoch 68/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0655 - mae: 0.1782 - mse: 0.0636 - val_loss: 0.0659 - val_mae: 0.1794 - val_mse: 0.0640 - learning_rate: 0.0010\n",
            "Epoch 69/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0648 - mae: 0.1771 - mse: 0.0629 - val_loss: 0.0656 - val_mae: 0.1786 - val_mse: 0.0637 - learning_rate: 0.0010\n",
            "Epoch 70/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0660 - mae: 0.1787 - mse: 0.0641 - val_loss: 0.0655 - val_mae: 0.1782 - val_mse: 0.0636 - learning_rate: 0.0010\n",
            "Epoch 71/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0657 - mae: 0.1776 - mse: 0.0638 - val_loss: 0.0652 - val_mae: 0.1776 - val_mse: 0.0633 - learning_rate: 0.0010\n",
            "Epoch 72/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0652 - mae: 0.1766 - mse: 0.0633 - val_loss: 0.0648 - val_mae: 0.1768 - val_mse: 0.0630 - learning_rate: 0.0010\n",
            "Epoch 73/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0645 - mae: 0.1754 - mse: 0.0627 - val_loss: 0.0646 - val_mae: 0.1762 - val_mse: 0.0628 - learning_rate: 0.0010\n",
            "Epoch 74/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0651 - mae: 0.1765 - mse: 0.0633 - val_loss: 0.0644 - val_mae: 0.1756 - val_mse: 0.0626 - learning_rate: 0.0010\n",
            "Epoch 75/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0640 - mae: 0.1741 - mse: 0.0622 - val_loss: 0.0642 - val_mae: 0.1750 - val_mse: 0.0624 - learning_rate: 0.0010\n",
            "Epoch 76/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0642 - mae: 0.1742 - mse: 0.0624 - val_loss: 0.0640 - val_mae: 0.1746 - val_mse: 0.0622 - learning_rate: 0.0010\n",
            "Epoch 77/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0637 - mae: 0.1731 - mse: 0.0619 - val_loss: 0.0637 - val_mae: 0.1737 - val_mse: 0.0620 - learning_rate: 0.0010\n",
            "Epoch 78/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0636 - mae: 0.1727 - mse: 0.0618 - val_loss: 0.0634 - val_mae: 0.1730 - val_mse: 0.0616 - learning_rate: 0.0010\n",
            "Epoch 79/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0629 - mae: 0.1712 - mse: 0.0611 - val_loss: 0.0632 - val_mae: 0.1719 - val_mse: 0.0614 - learning_rate: 0.0010\n",
            "Epoch 80/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0620 - mae: 0.1695 - mse: 0.0603 - val_loss: 0.0628 - val_mae: 0.1711 - val_mse: 0.0611 - learning_rate: 0.0010\n",
            "Epoch 81/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0628 - mae: 0.1702 - mse: 0.0611 - val_loss: 0.0627 - val_mae: 0.1703 - val_mse: 0.0610 - learning_rate: 0.0010\n",
            "Epoch 82/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0625 - mae: 0.1694 - mse: 0.0608 - val_loss: 0.0624 - val_mae: 0.1697 - val_mse: 0.0607 - learning_rate: 0.0010\n",
            "Epoch 83/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0622 - mae: 0.1683 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1695 - val_mse: 0.0607 - learning_rate: 0.0010\n",
            "Epoch 84/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0626 - mae: 0.1692 - mse: 0.0609 - val_loss: 0.0623 - val_mae: 0.1684 - val_mse: 0.0607 - learning_rate: 0.0010\n",
            "Epoch 85/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0615 - mae: 0.1661 - mse: 0.0598 - val_loss: 0.0622 - val_mae: 0.1677 - val_mse: 0.0605 - learning_rate: 0.0010\n",
            "Epoch 86/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0622 - mae: 0.1674 - mse: 0.0606 - val_loss: 0.0620 - val_mae: 0.1672 - val_mse: 0.0604 - learning_rate: 0.0010\n",
            "Epoch 87/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0618 - mae: 0.1666 - mse: 0.0602 - val_loss: 0.0621 - val_mae: 0.1671 - val_mse: 0.0604 - learning_rate: 0.0010\n",
            "Epoch 88/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0618 - mae: 0.1656 - mse: 0.0602 - val_loss: 0.0620 - val_mae: 0.1669 - val_mse: 0.0604 - learning_rate: 0.0010\n",
            "Epoch 89/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0609 - mae: 0.1645 - mse: 0.0592 - val_loss: 0.0619 - val_mae: 0.1665 - val_mse: 0.0603 - learning_rate: 0.0010\n",
            "Epoch 90/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0612 - mae: 0.1648 - mse: 0.0596 - val_loss: 0.0620 - val_mae: 0.1665 - val_mse: 0.0604 - learning_rate: 0.0010\n",
            "Epoch 91/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.0619 - mae: 0.1656 - mse: 0.0603 - val_loss: 0.0618 - val_mae: 0.1666 - val_mse: 0.0602 - learning_rate: 0.0010\n",
            "Epoch 92/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0616 - mae: 0.1656 - mse: 0.0601 - val_loss: 0.0618 - val_mae: 0.1660 - val_mse: 0.0602 - learning_rate: 0.0010\n",
            "Epoch 93/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0612 - mae: 0.1642 - mse: 0.0596 - val_loss: 0.0617 - val_mae: 0.1660 - val_mse: 0.0602 - learning_rate: 0.0010\n",
            "Epoch 94/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0620 - mae: 0.1659 - mse: 0.0604 - val_loss: 0.0619 - val_mae: 0.1662 - val_mse: 0.0603 - learning_rate: 0.0010\n",
            "Epoch 95/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0605 - mae: 0.1630 - mse: 0.0589 - val_loss: 0.0616 - val_mae: 0.1657 - val_mse: 0.0601 - learning_rate: 0.0010\n",
            "Epoch 96/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0612 - mae: 0.1645 - mse: 0.0596 - val_loss: 0.0618 - val_mae: 0.1659 - val_mse: 0.0603 - learning_rate: 0.0010\n",
            "Epoch 97/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0615 - mae: 0.1646 - mse: 0.0600 - val_loss: 0.0616 - val_mae: 0.1654 - val_mse: 0.0601 - learning_rate: 0.0010\n",
            "Epoch 98/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0611 - mae: 0.1644 - mse: 0.0596 - val_loss: 0.0617 - val_mae: 0.1655 - val_mse: 0.0602 - learning_rate: 0.0010\n",
            "Epoch 99/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0609 - mae: 0.1633 - mse: 0.0594 - val_loss: 0.0617 - val_mae: 0.1654 - val_mse: 0.0602 - learning_rate: 0.0010\n",
            "Epoch 100/500\n",
            "\u001b[1m22/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0612 - mae: 0.1640 - mse: 0.0597\n",
            "Epoch 100: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0612 - mae: 0.1640 - mse: 0.0597 - val_loss: 0.0615 - val_mae: 0.1650 - val_mse: 0.0601 - learning_rate: 0.0010\n",
            "Epoch 101/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0620 - mae: 0.1653 - mse: 0.0606 - val_loss: 0.0615 - val_mae: 0.1650 - val_mse: 0.0601 - learning_rate: 5.0000e-04\n",
            "Epoch 102/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0608 - mae: 0.1634 - mse: 0.0593 - val_loss: 0.0615 - val_mae: 0.1647 - val_mse: 0.0600 - learning_rate: 5.0000e-04\n",
            "Epoch 103/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0614 - mae: 0.1640 - mse: 0.0599 - val_loss: 0.0615 - val_mae: 0.1648 - val_mse: 0.0600 - learning_rate: 5.0000e-04\n",
            "Epoch 104/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0609 - mae: 0.1635 - mse: 0.0594 - val_loss: 0.0615 - val_mae: 0.1647 - val_mse: 0.0600 - learning_rate: 5.0000e-04\n",
            "Epoch 105/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0608 - mae: 0.1630 - mse: 0.0593 - val_loss: 0.0614 - val_mae: 0.1645 - val_mse: 0.0600 - learning_rate: 5.0000e-04\n",
            "Epoch 106/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0609 - mae: 0.1635 - mse: 0.0594 - val_loss: 0.0614 - val_mae: 0.1646 - val_mse: 0.0600 - learning_rate: 5.0000e-04\n",
            "Epoch 107/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0607 - mae: 0.1633 - mse: 0.0592 - val_loss: 0.0614 - val_mae: 0.1645 - val_mse: 0.0600 - learning_rate: 5.0000e-04\n",
            "Epoch 108/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0609 - mae: 0.1632 - mse: 0.0595 - val_loss: 0.0614 - val_mae: 0.1645 - val_mse: 0.0600 - learning_rate: 5.0000e-04\n",
            "Epoch 109/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0602 - mae: 0.1622 - mse: 0.0587 - val_loss: 0.0613 - val_mae: 0.1645 - val_mse: 0.0599 - learning_rate: 5.0000e-04\n",
            "Epoch 110/500\n",
            "\u001b[1m22/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0604 - mae: 0.1626 - mse: 0.0590\n",
            "Epoch 110: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0605 - mae: 0.1627 - mse: 0.0591 - val_loss: 0.0614 - val_mae: 0.1645 - val_mse: 0.0600 - learning_rate: 5.0000e-04\n",
            "Epoch 111/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0606 - mae: 0.1627 - mse: 0.0592 - val_loss: 0.0613 - val_mae: 0.1644 - val_mse: 0.0599 - learning_rate: 2.5000e-04\n",
            "Epoch 112/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0605 - mae: 0.1627 - mse: 0.0591 - val_loss: 0.0613 - val_mae: 0.1644 - val_mse: 0.0599 - learning_rate: 2.5000e-04\n",
            "Epoch 113/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0609 - mae: 0.1628 - mse: 0.0595 - val_loss: 0.0613 - val_mae: 0.1643 - val_mse: 0.0599 - learning_rate: 2.5000e-04\n",
            "Epoch 114/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0608 - mae: 0.1628 - mse: 0.0593 - val_loss: 0.0613 - val_mae: 0.1643 - val_mse: 0.0599 - learning_rate: 2.5000e-04\n",
            "Epoch 115/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0606 - mae: 0.1623 - mse: 0.0592 - val_loss: 0.0613 - val_mae: 0.1643 - val_mse: 0.0599 - learning_rate: 2.5000e-04\n",
            "Epoch 116/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0602 - mae: 0.1625 - mse: 0.0588 - val_loss: 0.0613 - val_mae: 0.1642 - val_mse: 0.0599 - learning_rate: 2.5000e-04\n",
            "Epoch 117/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0602 - mae: 0.1622 - mse: 0.0588 - val_loss: 0.0613 - val_mae: 0.1642 - val_mse: 0.0599 - learning_rate: 2.5000e-04\n",
            "Epoch 118/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0608 - mae: 0.1629 - mse: 0.0594 - val_loss: 0.0613 - val_mae: 0.1642 - val_mse: 0.0599 - learning_rate: 2.5000e-04\n",
            "Epoch 119/500\n",
            "\u001b[1m20/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0594 - mae: 0.1603 - mse: 0.0580\n",
            "Epoch 119: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0596 - mae: 0.1608 - mse: 0.0582 - val_loss: 0.0612 - val_mae: 0.1641 - val_mse: 0.0599 - learning_rate: 2.5000e-04\n",
            "Epoch 120/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0606 - mae: 0.1625 - mse: 0.0593 - val_loss: 0.0612 - val_mae: 0.1642 - val_mse: 0.0599 - learning_rate: 1.2500e-04\n",
            "Epoch 121/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0604 - mae: 0.1622 - mse: 0.0590 - val_loss: 0.0612 - val_mae: 0.1641 - val_mse: 0.0599 - learning_rate: 1.2500e-04\n",
            "Epoch 122/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0610 - mae: 0.1631 - mse: 0.0596 - val_loss: 0.0612 - val_mae: 0.1641 - val_mse: 0.0599 - learning_rate: 1.2500e-04\n",
            "Epoch 123/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0598 - mae: 0.1615 - mse: 0.0584 - val_loss: 0.0612 - val_mae: 0.1641 - val_mse: 0.0599 - learning_rate: 1.2500e-04\n",
            "Epoch 124/500\n",
            "\u001b[1m22/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0607 - mae: 0.1631 - mse: 0.0594\n",
            "Epoch 124: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0607 - mae: 0.1630 - mse: 0.0593 - val_loss: 0.0612 - val_mae: 0.1642 - val_mse: 0.0598 - learning_rate: 1.2500e-04\n",
            "Epoch 125/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0607 - mae: 0.1628 - mse: 0.0593 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 6.2500e-05\n",
            "Epoch 126/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0605 - mae: 0.1623 - mse: 0.0591 - val_loss: 0.0612 - val_mae: 0.1641 - val_mse: 0.0598 - learning_rate: 6.2500e-05\n",
            "Epoch 127/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0608 - mae: 0.1628 - mse: 0.0595 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 6.2500e-05\n",
            "Epoch 128/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0612 - mae: 0.1634 - mse: 0.0598 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 6.2500e-05\n",
            "Epoch 129/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0606 - mae: 0.1621 - mse: 0.0592 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 6.2500e-05\n",
            "Epoch 130/500\n",
            "\u001b[1m22/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0604 - mae: 0.1622 - mse: 0.0590\n",
            "Epoch 130: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0604 - mae: 0.1623 - mse: 0.0590 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 6.2500e-05\n",
            "Epoch 131/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0599 - mae: 0.1613 - mse: 0.0586 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 3.1250e-05\n",
            "Epoch 132/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0601 - mae: 0.1621 - mse: 0.0587 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 3.1250e-05\n",
            "Epoch 133/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0598 - mae: 0.1609 - mse: 0.0584 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 3.1250e-05\n",
            "Epoch 134/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0604 - mae: 0.1624 - mse: 0.0591 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 3.1250e-05\n",
            "Epoch 135/500\n",
            "\u001b[1m21/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0608 - mae: 0.1626 - mse: 0.0594\n",
            "Epoch 135: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0607 - mae: 0.1625 - mse: 0.0594 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 3.1250e-05\n",
            "Epoch 136/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0605 - mae: 0.1625 - mse: 0.0591 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.5625e-05\n",
            "Epoch 137/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0606 - mae: 0.1625 - mse: 0.0593 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.5625e-05\n",
            "Epoch 138/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0602 - mae: 0.1619 - mse: 0.0589 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.5625e-05\n",
            "Epoch 139/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0598 - mae: 0.1610 - mse: 0.0584 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.5625e-05\n",
            "Epoch 140/500\n",
            "\u001b[1m23/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0611 - mae: 0.1634 - mse: 0.0597\n",
            "Epoch 140: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0610 - mae: 0.1633 - mse: 0.0596 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.5625e-05\n",
            "Epoch 141/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0604 - mae: 0.1623 - mse: 0.0591 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 7.8125e-06\n",
            "Epoch 142/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0606 - mae: 0.1628 - mse: 0.0592 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 7.8125e-06\n",
            "Epoch 143/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0600 - mae: 0.1617 - mse: 0.0586 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 7.8125e-06\n",
            "Epoch 144/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0610 - mae: 0.1631 - mse: 0.0596 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 7.8125e-06\n",
            "Epoch 145/500\n",
            "\u001b[1m22/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0607 - mae: 0.1628 - mse: 0.0593\n",
            "Epoch 145: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0607 - mae: 0.1628 - mse: 0.0593 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 7.8125e-06\n",
            "Epoch 146/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0607 - mae: 0.1628 - mse: 0.0594 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 3.9063e-06\n",
            "Epoch 147/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0607 - mae: 0.1629 - mse: 0.0593 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 3.9063e-06\n",
            "Epoch 148/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0608 - mae: 0.1631 - mse: 0.0594 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 3.9063e-06\n",
            "Epoch 149/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0600 - mae: 0.1615 - mse: 0.0586 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 3.9063e-06\n",
            "Epoch 150/500\n",
            "\u001b[1m19/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0602 - mae: 0.1618 - mse: 0.0589\n",
            "Epoch 150: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0603 - mae: 0.1619 - mse: 0.0589 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 3.9063e-06\n",
            "Epoch 151/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0603 - mae: 0.1618 - mse: 0.0590 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.9531e-06\n",
            "Epoch 152/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0595 - mae: 0.1607 - mse: 0.0582 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.9531e-06\n",
            "Epoch 153/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0609 - mae: 0.1630 - mse: 0.0595 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.9531e-06\n",
            "Epoch 154/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0605 - mae: 0.1623 - mse: 0.0591 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.9531e-06\n",
            "Epoch 155/500\n",
            "\u001b[1m21/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0601 - mae: 0.1614 - mse: 0.0587\n",
            "Epoch 155: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0601 - mae: 0.1615 - mse: 0.0588 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.9531e-06\n",
            "Epoch 156/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0605 - mae: 0.1623 - mse: 0.0591 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 9.7656e-07\n",
            "Epoch 157/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0605 - mae: 0.1623 - mse: 0.0591 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 9.7656e-07\n",
            "Epoch 158/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0602 - mae: 0.1615 - mse: 0.0589 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 9.7656e-07\n",
            "Epoch 159/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0607 - mae: 0.1627 - mse: 0.0594 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 9.7656e-07\n",
            "Epoch 160/500\n",
            "\u001b[1m21/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0610 - mae: 0.1633 - mse: 0.0597\n",
            "Epoch 160: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0609 - mae: 0.1632 - mse: 0.0596 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 9.7656e-07\n",
            "Epoch 161/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0605 - mae: 0.1625 - mse: 0.0592 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 4.8828e-07\n",
            "Epoch 162/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0610 - mae: 0.1634 - mse: 0.0596 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 4.8828e-07\n",
            "Epoch 163/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0601 - mae: 0.1615 - mse: 0.0587 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 4.8828e-07\n",
            "Epoch 164/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0608 - mae: 0.1631 - mse: 0.0595 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 4.8828e-07\n",
            "Epoch 165/500\n",
            "\u001b[1m19/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0611 - mae: 0.1634 - mse: 0.0597\n",
            "Epoch 165: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0609 - mae: 0.1631 - mse: 0.0596 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 4.8828e-07\n",
            "Epoch 166/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0608 - mae: 0.1631 - mse: 0.0594 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 2.4414e-07\n",
            "Epoch 167/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0611 - mae: 0.1633 - mse: 0.0597 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 2.4414e-07\n",
            "Epoch 168/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0593 - mae: 0.1604 - mse: 0.0579 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 2.4414e-07\n",
            "Epoch 169/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0602 - mae: 0.1618 - mse: 0.0589 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 2.4414e-07\n",
            "Epoch 170/500\n",
            "\u001b[1m22/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0613 - mae: 0.1635 - mse: 0.0600 \n",
            "Epoch 170: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0612 - mae: 0.1633 - mse: 0.0598 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 2.4414e-07\n",
            "Epoch 171/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0603 - mae: 0.1616 - mse: 0.0589 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.2207e-07\n",
            "Epoch 172/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0607 - mae: 0.1628 - mse: 0.0594 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.2207e-07\n",
            "Epoch 173/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0601 - mae: 0.1618 - mse: 0.0587 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.2207e-07\n",
            "Epoch 174/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0601 - mae: 0.1620 - mse: 0.0588 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.2207e-07\n",
            "Epoch 175/500\n",
            "\u001b[1m20/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0607 - mae: 0.1628 - mse: 0.0593\n",
            "Epoch 175: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0606 - mae: 0.1626 - mse: 0.0592 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.2207e-07\n",
            "Epoch 176/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0608 - mae: 0.1629 - mse: 0.0594 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 177/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0604 - mae: 0.1622 - mse: 0.0591 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 178/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0607 - mae: 0.1628 - mse: 0.0593 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 179/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0610 - mae: 0.1632 - mse: 0.0596 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 180/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0607 - mae: 0.1625 - mse: 0.0593 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 181/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0603 - mae: 0.1619 - mse: 0.0589 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 182/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0605 - mae: 0.1631 - mse: 0.0591 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 183/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0601 - mae: 0.1613 - mse: 0.0587 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 184/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0605 - mae: 0.1628 - mse: 0.0592 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 185/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0608 - mae: 0.1628 - mse: 0.0594 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 186/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0609 - mae: 0.1631 - mse: 0.0595 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 187/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0604 - mae: 0.1625 - mse: 0.0590 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 188/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0600 - mae: 0.1615 - mse: 0.0587 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 189/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0611 - mae: 0.1636 - mse: 0.0597 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 190/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0605 - mae: 0.1625 - mse: 0.0592 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 191/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0606 - mae: 0.1625 - mse: 0.0592 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 192/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0599 - mae: 0.1616 - mse: 0.0585 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 193/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0615 - mae: 0.1639 - mse: 0.0601 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 194/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0601 - mae: 0.1619 - mse: 0.0587 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 195/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0605 - mae: 0.1624 - mse: 0.0592 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 196/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0609 - mae: 0.1629 - mse: 0.0595 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 197/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0608 - mae: 0.1626 - mse: 0.0594 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 198/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0606 - mae: 0.1628 - mse: 0.0592 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 199/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0603 - mae: 0.1620 - mse: 0.0589 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 200/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0603 - mae: 0.1619 - mse: 0.0589 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 201/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0605 - mae: 0.1621 - mse: 0.0591 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 202/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0607 - mae: 0.1631 - mse: 0.0593 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 203/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0603 - mae: 0.1620 - mse: 0.0589 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 204/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0602 - mae: 0.1619 - mse: 0.0588 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 205/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0606 - mae: 0.1623 - mse: 0.0592 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 206/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0605 - mae: 0.1624 - mse: 0.0591 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 207/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0603 - mae: 0.1620 - mse: 0.0589 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 208/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0604 - mae: 0.1620 - mse: 0.0590 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 209/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0598 - mae: 0.1614 - mse: 0.0584 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 210/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0611 - mae: 0.1634 - mse: 0.0597 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 211/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0607 - mae: 0.1627 - mse: 0.0593 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 212/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0600 - mae: 0.1615 - mse: 0.0586 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 213/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0605 - mae: 0.1626 - mse: 0.0591 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 214/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0601 - mae: 0.1619 - mse: 0.0587 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 215/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0600 - mae: 0.1617 - mse: 0.0586 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 216/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0613 - mae: 0.1639 - mse: 0.0600 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 217/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0608 - mae: 0.1630 - mse: 0.0594 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 218/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0599 - mae: 0.1613 - mse: 0.0585 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 219/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0604 - mae: 0.1621 - mse: 0.0591 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 220/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0611 - mae: 0.1635 - mse: 0.0598 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 221/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0609 - mae: 0.1632 - mse: 0.0596 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 222/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0601 - mae: 0.1616 - mse: 0.0587 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 223/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0605 - mae: 0.1623 - mse: 0.0592 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 224/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0605 - mae: 0.1621 - mse: 0.0591 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 225/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0610 - mae: 0.1632 - mse: 0.0596 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 226/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0607 - mae: 0.1628 - mse: 0.0593 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 227/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0598 - mae: 0.1615 - mse: 0.0585 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 228/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0605 - mae: 0.1624 - mse: 0.0591 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 229/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0612 - mae: 0.1635 - mse: 0.0599 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 230/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0606 - mae: 0.1625 - mse: 0.0592 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 231/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0603 - mae: 0.1620 - mse: 0.0589 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 232/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0604 - mae: 0.1622 - mse: 0.0590 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 233/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0608 - mae: 0.1628 - mse: 0.0594 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 234/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0604 - mae: 0.1621 - mse: 0.0590 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 235/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0607 - mae: 0.1630 - mse: 0.0593 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 236/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0609 - mae: 0.1630 - mse: 0.0596 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 237/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0601 - mae: 0.1617 - mse: 0.0588 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 238/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0613 - mae: 0.1635 - mse: 0.0599 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 239/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0610 - mae: 0.1632 - mse: 0.0596 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 240/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0607 - mae: 0.1629 - mse: 0.0594 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 241/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0605 - mae: 0.1626 - mse: 0.0591 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 242/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0605 - mae: 0.1626 - mse: 0.0591 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 243/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0607 - mae: 0.1631 - mse: 0.0593 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 244/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0607 - mae: 0.1631 - mse: 0.0593 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 245/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0608 - mae: 0.1630 - mse: 0.0594 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 246/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0604 - mae: 0.1621 - mse: 0.0590 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 247/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0605 - mae: 0.1625 - mse: 0.0591 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 248/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0604 - mae: 0.1624 - mse: 0.0591 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 249/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0608 - mae: 0.1624 - mse: 0.0594 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 250/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0599 - mae: 0.1618 - mse: 0.0585 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 251/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0606 - mae: 0.1624 - mse: 0.0592 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 252/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0603 - mae: 0.1620 - mse: 0.0590 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 253/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0609 - mae: 0.1632 - mse: 0.0596 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 254/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0602 - mae: 0.1616 - mse: 0.0588 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 255/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0602 - mae: 0.1618 - mse: 0.0589 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 256/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0606 - mae: 0.1625 - mse: 0.0592 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 257/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0599 - mae: 0.1616 - mse: 0.0585 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 258/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0602 - mae: 0.1618 - mse: 0.0588 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 259/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0607 - mae: 0.1629 - mse: 0.0594 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 260/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0602 - mae: 0.1621 - mse: 0.0588 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 261/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0599 - mae: 0.1615 - mse: 0.0586 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 262/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0603 - mae: 0.1621 - mse: 0.0589 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 263/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0602 - mae: 0.1617 - mse: 0.0588 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 264/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0612 - mae: 0.1637 - mse: 0.0598 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 265/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0601 - mae: 0.1620 - mse: 0.0587 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 266/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0610 - mae: 0.1633 - mse: 0.0597 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 267/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0599 - mae: 0.1613 - mse: 0.0585 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 268/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0608 - mae: 0.1630 - mse: 0.0594 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 269/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0613 - mae: 0.1637 - mse: 0.0599 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 270/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0597 - mae: 0.1610 - mse: 0.0583 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 271/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0601 - mae: 0.1615 - mse: 0.0587 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 272/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0604 - mae: 0.1622 - mse: 0.0590 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 273/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0607 - mae: 0.1629 - mse: 0.0593 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 274/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0602 - mae: 0.1622 - mse: 0.0589 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 275/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0610 - mae: 0.1633 - mse: 0.0597 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 276/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0605 - mae: 0.1624 - mse: 0.0591 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 277/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0601 - mae: 0.1615 - mse: 0.0587 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 278/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0604 - mae: 0.1626 - mse: 0.0591 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 279/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0602 - mae: 0.1618 - mse: 0.0588 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 280/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0605 - mae: 0.1625 - mse: 0.0591 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 281/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0608 - mae: 0.1631 - mse: 0.0595 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 282/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0607 - mae: 0.1630 - mse: 0.0593 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 283/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0610 - mae: 0.1631 - mse: 0.0596 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 284/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0606 - mae: 0.1626 - mse: 0.0593 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 285/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0605 - mae: 0.1626 - mse: 0.0591 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 286/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0607 - mae: 0.1631 - mse: 0.0593 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 287/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0604 - mae: 0.1621 - mse: 0.0590 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 288/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0604 - mae: 0.1620 - mse: 0.0591 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 289/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0600 - mae: 0.1619 - mse: 0.0587 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 290/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0599 - mae: 0.1612 - mse: 0.0586 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 291/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0606 - mae: 0.1625 - mse: 0.0592 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 292/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0601 - mae: 0.1617 - mse: 0.0587 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 293/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0607 - mae: 0.1629 - mse: 0.0594 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 294/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0604 - mae: 0.1620 - mse: 0.0591 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 295/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0608 - mae: 0.1629 - mse: 0.0594 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 296/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0609 - mae: 0.1632 - mse: 0.0595 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 297/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0604 - mae: 0.1622 - mse: 0.0590 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 298/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0607 - mae: 0.1627 - mse: 0.0594 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 299/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0600 - mae: 0.1615 - mse: 0.0586 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 300/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0602 - mae: 0.1618 - mse: 0.0589 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 301/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0604 - mae: 0.1623 - mse: 0.0590 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 302/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0604 - mae: 0.1619 - mse: 0.0591 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 303/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0605 - mae: 0.1626 - mse: 0.0591 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 304/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0604 - mae: 0.1624 - mse: 0.0590 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 305/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0600 - mae: 0.1617 - mse: 0.0587 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 306/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0609 - mae: 0.1630 - mse: 0.0595 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 307/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0601 - mae: 0.1615 - mse: 0.0588 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 308/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0606 - mae: 0.1624 - mse: 0.0593 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 309/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0608 - mae: 0.1628 - mse: 0.0595 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 310/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0595 - mae: 0.1605 - mse: 0.0581 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 311/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0605 - mae: 0.1625 - mse: 0.0591 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 312/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0604 - mae: 0.1623 - mse: 0.0591 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 313/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0611 - mae: 0.1631 - mse: 0.0597 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 314/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0597 - mae: 0.1610 - mse: 0.0583 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 315/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0598 - mae: 0.1609 - mse: 0.0584 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 316/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0600 - mae: 0.1616 - mse: 0.0586 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 317/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0602 - mae: 0.1621 - mse: 0.0589 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 318/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0606 - mae: 0.1625 - mse: 0.0592 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 319/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0605 - mae: 0.1625 - mse: 0.0591 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 320/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0606 - mae: 0.1626 - mse: 0.0593 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 321/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0597 - mae: 0.1609 - mse: 0.0583 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 322/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0610 - mae: 0.1634 - mse: 0.0596 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 323/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0608 - mae: 0.1627 - mse: 0.0595 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 324/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0603 - mae: 0.1620 - mse: 0.0589 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 325/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0602 - mae: 0.1616 - mse: 0.0588 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 326/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0601 - mae: 0.1618 - mse: 0.0587 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 327/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0598 - mae: 0.1611 - mse: 0.0584 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 328/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0602 - mae: 0.1619 - mse: 0.0589 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 329/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0607 - mae: 0.1626 - mse: 0.0593 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 330/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0600 - mae: 0.1614 - mse: 0.0586 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 331/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0604 - mae: 0.1621 - mse: 0.0590 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 332/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0608 - mae: 0.1626 - mse: 0.0594 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 333/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0601 - mae: 0.1619 - mse: 0.0587 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 334/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0610 - mae: 0.1633 - mse: 0.0597 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 335/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0604 - mae: 0.1619 - mse: 0.0590 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 336/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0607 - mae: 0.1625 - mse: 0.0593 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 337/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0607 - mae: 0.1629 - mse: 0.0593 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 338/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0609 - mae: 0.1632 - mse: 0.0595 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 339/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0603 - mae: 0.1619 - mse: 0.0590 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 340/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0606 - mae: 0.1624 - mse: 0.0592 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 341/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0606 - mae: 0.1626 - mse: 0.0592 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 342/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0607 - mae: 0.1627 - mse: 0.0593 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 343/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0605 - mae: 0.1625 - mse: 0.0592 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 344/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0607 - mae: 0.1624 - mse: 0.0593 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 345/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0593 - mae: 0.1604 - mse: 0.0579 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 346/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0604 - mae: 0.1622 - mse: 0.0590 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 347/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0601 - mae: 0.1617 - mse: 0.0587 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 348/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0612 - mae: 0.1635 - mse: 0.0598 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 349/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0610 - mae: 0.1631 - mse: 0.0597 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 350/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0598 - mae: 0.1612 - mse: 0.0584 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 351/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0605 - mae: 0.1623 - mse: 0.0591 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 352/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0605 - mae: 0.1623 - mse: 0.0591 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 353/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0602 - mae: 0.1618 - mse: 0.0589 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 354/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0605 - mae: 0.1623 - mse: 0.0592 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 355/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0599 - mae: 0.1612 - mse: 0.0586 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 356/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0604 - mae: 0.1626 - mse: 0.0590 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 357/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0608 - mae: 0.1628 - mse: 0.0594 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 358/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0600 - mae: 0.1616 - mse: 0.0587 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n",
            "Epoch 359/500\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0604 - mae: 0.1625 - mse: 0.0591 - val_loss: 0.0612 - val_mae: 0.1640 - val_mse: 0.0598 - learning_rate: 1.0000e-07\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    [X_real_train, X_imag_train],\n",
        "    y_train,\n",
        "    epochs=500,\n",
        "    batch_size=32,\n",
        "    validation_data=([X_real_test, X_imag_test], y_test),\n",
        "    callbacks=[adjust_lr, early_stopping]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE8CNDhzP4sr"
      },
      "source": [
        "Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBH6RRTXP4ss",
        "outputId": "1d6c36dc-5be6-42ee-fe58-7475f8ce063b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0601 - mae: 0.1618 - mse: 0.0588 \n",
            "Test MSE: 0.0598, Test MAE: 0.1640\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_mse, test_mae = model.evaluate([X_real_test, X_imag_test], y_test)\n",
        "print(f\"Test MSE: {test_mse:.4f}, Test MAE: {test_mae:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "9KZhdzmWP4st",
        "outputId": "20f33fb2-8c5b-4539-eb1a-2eafb7d74cc7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKBMguEAP4st",
        "outputId": "32379641-4eb7-459e-8449-4bc259c52da5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 220ms/step\n",
            "Comparison of predictions and ground truth:\n",
            "Sample 1:\n",
            "  Predicted:    [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 2. 5. 1. 3. 2. 4. 4. 6. 5.\n",
            " 2. 3. 1. 0. 0. 0. 0. 0.]\n",
            "  Ground Truth: [5. 6. 5. 1. 3. 4. 1. 1. 6. 2. 3. 1. 4. 0. 3. 2. 5. 1. 4. 3. 4. 4. 6. 5.\n",
            " 2. 3. 1. 0. 0. 0. 0. 0.]\n",
            "----------------------------------------\n",
            "Sample 2:\n",
            "  Predicted:    [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 2. 1. 1. 3. 1. 1. 5. 2. 1.\n",
            " 4. 0. 4. 0. 0. 0. 0. 0.]\n",
            "  Ground Truth: [1. 4. 1. 3. 3. 3. 0. 5. 5. 2. 4. 3. 3. 1. 3. 1. 1. 2. 0. 1. 1. 5. 2. 1.\n",
            " 4. 0. 4. 0. 0. 0. 0. 0.]\n",
            "----------------------------------------\n",
            "Sample 3:\n",
            "  Predicted:    [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 2. 5. 1. 3. 6. 0. 4. 3. 2.\n",
            " 3. 4. 4. 0. 0. 0. 0. 0.]\n",
            "  Ground Truth: [2. 6. 0. 2. 2. 1. 3. 4. 2. 6. 4. 1. 0. 5. 2. 3. 5. 1. 4. 6. 0. 4. 3. 2.\n",
            " 3. 4. 4. 0. 0. 0. 0. 0.]\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "y_pred_test = model.predict([X_real_test, X_imag_test])\n",
        "y_pred_test_rescaled = y_pred_test * (q - 1)\n",
        "y_test_rescaled = y_test * (q - 1)\n",
        "\n",
        "print(\"Comparison of predictions and ground truth:\")\n",
        "for i in range(3):\n",
        "    print(f\"Sample {i+1}:\")\n",
        "    print(f\"  Predicted:    {np.abs(np.round(y_pred_test_rescaled[i]))}\")\n",
        "    print(f\"  Ground Truth: {np.round(y_test_rescaled[i])}\")\n",
        "    print(\"-\" * 40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqrSkd26P4st"
      },
      "source": [
        "predict on unseen data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EpCycZ-P4sz",
        "outputId": "82122bbc-393f-43b0-c06d-0b98a0c6a7e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "\n",
            "Comparison of Unseen Data and Predictions:\n",
            "============================================================\n",
            "Sample 1:\n",
            "  Ground Truth: [0 5 3 5 5 5 3 5 3 4 6 0 1 5 2 4 2 1 3 1 2 1 0 1 5 0 5 0 0 0 0 0]\n",
            "  Prediction  : [1 2 2 6 0 0 5 2 3 5 2 5 6 0 4 6 4 3 5 3 2 0 2 5 1 6 2 0 2 0 0 0]\n",
            "------------------------------------------------------------\n",
            "Sample 2:\n",
            "  Ground Truth: [6 6 1 0 0 6 1 4 3 3 3 0 6 3 6 0 6 4 1 6 6 1 0 0 6 4 3 0 0 0 0 0]\n",
            "  Prediction  : [4 2 6 4 2 6 4 3 5 1 0 0 2 5 2 6 5 6 3 1 4 1 3 3 5 1 2 0 0 0 0 0]\n",
            "------------------------------------------------------------\n",
            "Sample 3:\n",
            "  Ground Truth: [4 1 1 5 3 6 1 5 0 1 2 3 5 3 3 2 4 0 3 1 0 1 0 6 0 5 1 0 0 0 0 0]\n",
            "  Prediction  : [4 0 5 2 5 2 2 3 5 1 1 2 5 6 6 0 3 2 5 0 6 3 3 1 0 6 1 0 1 0 0 0]\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "num_unseen_samples = 3\n",
        "unseen_data = np.random.randint(0, q, size=(num_unseen_samples, n))\n",
        "\n",
        "padded_unseen_data = np.hstack((unseen_data, np.zeros((num_unseen_samples, n_padded - n))))\n",
        "\n",
        "unseen_data_normalized = unseen_data.astype(np.float32) / (q - 1)\n",
        "\n",
        "# print(\"Unseen Input Data (Before Encoding):\")\n",
        "# print(unseen_data)\n",
        "\n",
        "# unseen_encoded = np.array([dft(message, n_padded) for message in padded_unseen_data])\n",
        "unseen_encoded = np.array([np.dot(M_tilde, x) for x in padded_unseen_data])\n",
        "unseen_encoded[np.abs(unseen_encoded) < 1e-10] = 0\n",
        "unseen_encoded = np.round(unseen_encoded, decimals=10)\n",
        "\n",
        "X_real_unseen = np.real(unseen_encoded).astype(np.float32)\n",
        "X_imag_unseen = np.imag(unseen_encoded).astype(np.float32)\n",
        "\n",
        "# print(\"\\nEncoded Unseen Data (Real and Imaginary Parts):\")\n",
        "# print(\"Real Part Shape:\", X_real_unseen.shape)\n",
        "# print(\"Imaginary Part Shape:\", X_imag_unseen.shape)\n",
        "\n",
        "y_pred_unseen = model.predict([X_real_unseen, X_imag_unseen])\n",
        "\n",
        "y_pred_unseen_rescaled = y_pred_unseen * (q - 1)\n",
        "\n",
        "y_pred_unseen_final = np.mod(np.round(y_pred_unseen_rescaled), q)\n",
        "\n",
        "print(\"\\nComparison of Unseen Data and Predictions:\")\n",
        "print(\"=\" * 60)\n",
        "for i in range(num_unseen_samples):\n",
        "    print(f\"Sample {i+1}:\")\n",
        "    print(f\"  Ground Truth: {padded_unseen_data[i].astype(int)}\")\n",
        "    print(f\"  Prediction  : {y_pred_unseen_final[i].astype(int)}\")\n",
        "    print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXex2ev9P4s0"
      },
      "source": [
        "Check gradient flow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "kP9HEZxkP4s1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}