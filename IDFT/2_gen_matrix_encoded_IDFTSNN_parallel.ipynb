{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DFTSNN - Generator Matrix Encoded dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is encoded using the padded generator matrix. the imaginary and real values are parellel processed until concatenation. This mimics the IDFT SNN. One additional layer is included to account for the scaling diagonal matrix $\\hat{D}_n$ in the classical algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (r + 1) | (q - 1)\n",
    "# (r + 1) | n\n",
    "# q is a prime number\n",
    "# n = 2^t\n",
    "# r < n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset -f\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original n: 16\n",
      "Padded n: 16\n",
      "Generated dataset shape: (1000, 16)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n = 16\n",
    "q = 5\n",
    "num_samples = 1000\n",
    "\n",
    "def next_power_of_two(x):\n",
    "    return 1 if x == 0 else 2**(x - 1).bit_length()\n",
    "\n",
    "n_padded = next_power_of_two(n)\n",
    "\n",
    "dataset = np.random.randint(0, q, size=(num_samples, n))\n",
    "\n",
    "if n_padded > n:\n",
    "    pad_width = n_padded - n\n",
    "    dataset = np.pad(dataset, ((0, 0), (0, pad_width)), mode='constant', constant_values=0)\n",
    "\n",
    "print(\"Original n:\", n)\n",
    "print(\"Padded n:\", n_padded)\n",
    "print(\"Generated dataset shape:\", dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 4 2 ... 1 3 1]\n",
      " [3 4 0 ... 3 2 3]\n",
      " [3 0 2 ... 0 1 4]\n",
      " ...\n",
      " [4 4 4 ... 0 2 2]\n",
      " [4 2 4 ... 4 0 0]\n",
      " [2 2 2 ... 0 4 2]]\n",
      "(1000, 16)\n",
      "(16,)\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(dataset.shape)\n",
    "print(dataset[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\tilde{M}_{kj} = \\left[ \\left( \\frac{w_0}{z_0} \\right)^j \\zeta^{kj} \\right]_{k,j=0}^{n-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padded_generator_matrix(n, w0, z0):\n",
    "    zeta = np.exp(-2j * np.pi / n)\n",
    "    M_tilde = np.array([[(w0 / z0) ** j * zeta**(k * j) for j in range(n)] for k in range(n)], dtype=complex)\n",
    "    return M_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 16)\n"
     ]
    }
   ],
   "source": [
    "# (x, y, z, w) --> (1, 2, 3, 4)\n",
    "w0 = 4\n",
    "z0 = 3\n",
    "\n",
    "M_tilde = padded_generator_matrix(n_padded, w0, z0)\n",
    "print(M_tilde.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 649.89449698  +0.j           69.97998169+371.13335899j\n",
      "   -83.92481787+137.40000024j ...  -88.37954911-103.22398682j\n",
      "   -83.92481787-137.40000024j   69.97998169-371.13335899j]\n",
      " [ 695.87003951  +0.j          268.59479107+394.57072111j\n",
      "   -36.50274968+309.16102475j ...  -47.36788425-179.88976988j\n",
      "   -36.50274968-309.16102475j  268.59479107-394.57072111j]\n",
      " [ 540.63797786  +0.j          235.29809241+209.31041098j\n",
      "   176.66145652+204.71795948j ...  108.87153914-320.22828928j\n",
      "   176.66145652-204.71795948j  235.29809241-209.31041098j]\n",
      " ...\n",
      " [ 460.45123158  +0.j          162.64910275+168.11969395j\n",
      "    54.45039471+219.81284778j ...  -26.91067254-124.72256855j\n",
      "    54.45039471-219.81284778j  162.64910275-168.11969395j]\n",
      " [ 558.17613119  +0.j          -92.41727316+362.85897707j\n",
      "  -228.64175513 +13.85662093j ...  -76.51498185+176.39278765j\n",
      "  -228.64175513 -13.85662093j  -92.41727316-362.85897707j]\n",
      " [ 671.99178181  +0.j          161.97417378+352.04621278j\n",
      "    43.09272997+244.01831359j ...  -56.20864468-237.97753496j\n",
      "    43.09272997-244.01831359j  161.97417378-352.04621278j]]\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset = np.array([np.dot(M_tilde, x) for x in dataset])\n",
    "encoded_dataset[np.abs(encoded_dataset) < 1e-10] = 0\n",
    "encoded_dataset = np.round(encoded_dataset, decimals=10)\n",
    "print(encoded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 16)\n"
     ]
    }
   ],
   "source": [
    "print(encoded_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 2 4 4 1 2 2 2 4 3 2 4 1 3 1]\n",
      "[ 649.89449698  +0.j           69.97998169+371.13335899j\n",
      "  -83.92481787+137.40000024j  -88.37954911+103.22398682j\n",
      "  -74.53901813 +41.73652084j  -97.48254893 +37.96448017j\n",
      " -147.95858217-108.15626408j   47.97234649-148.66691513j\n",
      "  146.76987906  -0.j           47.97234649+148.66691513j\n",
      " -147.95858217+108.15626408j  -97.48254893 -37.96448017j\n",
      "  -74.53901813 -41.73652084j  -88.37954911-103.22398682j\n",
      "  -83.92481787-137.40000024j   69.97998169-371.13335899j]\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])\n",
    "print(encoded_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to zero mean and unit variance - output results of normalizing made unseen data converge to q-1\n",
    "# encoded_dataset = (encoded_dataset - np.mean(encoded_dataset, axis=0)) / np.std(encoded_dataset, axis=0)\n",
    "# print(encoded_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train_real : (1000, 16)\n",
      "[ 649.8945     69.97998   -83.92482   -88.37955   -74.53902   -97.48255\n",
      " -147.95859    47.972347  146.76988    47.972347 -147.95859   -97.48255\n",
      "  -74.53902   -88.37955   -83.92482    69.97998 ]\n",
      "\n",
      "X_train_imag : (1000, 16)\n",
      "[   0.        371.13336   137.4       103.22398    41.736523   37.96448\n",
      " -108.156265 -148.66692    -0.        148.66692   108.156265  -37.96448\n",
      "  -41.736523 -103.22398  -137.4      -371.13336 ]\n"
     ]
    }
   ],
   "source": [
    "X_real = np.real(encoded_dataset).astype(np.float32)\n",
    "X_imag = np.imag(encoded_dataset).astype(np.float32)\n",
    "\n",
    "print(\"\\nX_train_real :\", X_real.shape)\n",
    "print(X_real[0])\n",
    "print(\"\\nX_train_imag :\", X_imag.shape)\n",
    "print(X_imag[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: \n",
      "[3. 4. 2. 4. 4. 1. 2. 2. 2. 4. 3. 2. 4. 1. 3. 1.]\n",
      "\n",
      "y_normalized: \n",
      "[0.75 1.   0.5  1.   1.   0.25 0.5  0.5  0.5  1.   0.75 0.5  1.   0.25\n",
      " 0.75 0.25]\n"
     ]
    }
   ],
   "source": [
    "# normalize target data (integers 0-q to [0, 1])\n",
    "y_normalized = dataset.astype(np.float32) / (q - 1)  # Scale to [0, 1]\n",
    "print(\"y: \")\n",
    "print(dataset.astype(np.float32)[0])\n",
    "print(\"\\ny_normalized: \")\n",
    "print(y_normalized[0])\n",
    "\n",
    "labels = y_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shapes: X_real: (800, 16) X_imag: (800, 16) y: (800, 16)\n",
      "Testing data shapes: X_real: (200, 16) X_imag: (200, 16) y: (200, 16)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_real_train, X_real_test, y_train, y_test = train_test_split(\n",
    "    X_real, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_imag_train, X_imag_test, _, _ = train_test_split(\n",
    "    X_imag, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training data shapes: X_real:\", X_real_train.shape, \"X_imag:\", X_imag_train.shape, \"y:\", y_train.shape)\n",
    "print(\"Testing data shapes: X_real:\", X_real_test.shape, \"X_imag:\", X_imag_test.shape, \"y:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "print(X_real_train.shape[1])\n",
    "print(X_real_test.shape[1])\n",
    "print(X_imag_train.shape[1])\n",
    "print(X_imag_test.shape[1])\n",
    "print(y_train.shape[1])\n",
    "print(y_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_real_train.shape[1]\n",
    "output_dim = y_train.shape[1]\n",
    "print(input_dim)\n",
    "print(output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDFT - Structure Imposed Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, kernel_initializer='he_normal', bias_initializer='zeros', use_bias=True, **kwargs):\n",
    "        super(FirstLayer, self).__init__(**kwargs)\n",
    "        self.units = units  # Features/neurons\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n = self.units\n",
    "        n1 = n // 2\n",
    "        num_blocks = n1 // 2\n",
    "\n",
    "        self.b_1 = self.add_weight(name=\"kernel_b1\",\n",
    "                                   shape=(num_blocks, 2, 2),\n",
    "                                   initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                   trainable=True)\n",
    "        self.b_2 = self.add_weight(name=\"kernel_b2\",\n",
    "                                   shape=(num_blocks, 2, 2),\n",
    "                                   initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                   trainable=True)\n",
    "        self.d_1 = self.add_weight(name=\"kernel_d1\",\n",
    "                                   shape=(n1 - 2,),\n",
    "                                   initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                   trainable=True)\n",
    "        self.d_2 = self.add_weight(name=\"kernel_d2\",\n",
    "                                   shape=(n1 - 2,),\n",
    "                                   initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                   trainable=True)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name=\"bias\",\n",
    "                                        shape=(self.units,),\n",
    "                                        initializer=tf.keras.initializers.get(self.bias_initializer),\n",
    "                                        trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        def recursiveIDFT(inputVector, B, d, level):\n",
    "            n = inputVector.shape[1]\n",
    "            n1 = n // 2\n",
    "\n",
    "            if n == 2:\n",
    "                out = tf.matmul(inputVector, B[level])\n",
    "                return out\n",
    "            else:\n",
    "                q = tf.concat([inputVector[:, ::2], inputVector[:, 1::2]], axis=1)\n",
    "\n",
    "                B1 = recursiveIDFT(q[:, :n1], B, d[n1:], level + 1)\n",
    "                B2 = recursiveIDFT(q[:, n1:], B, d[n1:], level + 1)\n",
    "\n",
    "                d_n = tf.reshape(d[:n1], (1, -1))\n",
    "                z1 = tf.concat([(B1 + tf.multiply(B2, d_n)), (B1 - tf.multiply(B2, d_n))], axis=1)\n",
    "\n",
    "                return z1 / tf.sqrt(tf.constant(2.0, dtype=tf.float32))\n",
    "\n",
    "        n = self.units\n",
    "        n1 = n // 2\n",
    "\n",
    "        q = tf.concat([inputs[:, ::2], inputs[:, 1::2]], axis=1)\n",
    "\n",
    "        B1 = recursiveIDFT(q[:, :n1], self.b_1, self.d_1, level=0)\n",
    "        B2 = recursiveIDFT(q[:, n1:], self.b_2, self.d_2, level=0)\n",
    "\n",
    "        out = tf.concat([B1, B2], axis=1)\n",
    "\n",
    "        if self.use_bias:\n",
    "            out += self.bias\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, kernel_initializer='he_normal', bias_initializer='zeros', use_bias=True, **kwargs):\n",
    "        super(SecondLayer, self).__init__(**kwargs)\n",
    "        self.units = units  # Features/neurons\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n = self.units\n",
    "        n1 = n // 2\n",
    "\n",
    "        self.d_1 = self.add_weight(name=\"kernel_d1\",\n",
    "                                   shape=(n1,),\n",
    "                                   initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                   trainable=True)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name=\"bias\",\n",
    "                                        shape=(n,),\n",
    "                                        initializer=tf.keras.initializers.get(self.bias_initializer),\n",
    "                                        trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        out1 = inputs[:, :int(inputs.shape[1] / 2)]\n",
    "        out2 = inputs[:, int(inputs.shape[1] / 2):]\n",
    "\n",
    "        z1 = tf.concat([(out1 + tf.multiply(out2, self.d_1)), (out1 - tf.multiply(out2, self.d_1))], axis=1)\n",
    "        out = z1 / tf.sqrt(tf.constant(2.0, dtype=tf.float32))\n",
    "\n",
    "        if self.use_bias:\n",
    "            out += self.bias\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, kernel_initializer='he_normal', bias_initializer='zeros', use_bias=True, **kwargs):\n",
    "        super(CustomLayer, self).__init__(**kwargs)\n",
    "        self.units = units  # features/neurons\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n = self.units\n",
    "\n",
    "        self.m = self.add_weight(name=\"kernel_m\",\n",
    "                                 shape=(n,),\n",
    "                                 initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                 trainable=True)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name=\"bias\",\n",
    "                                        shape=(self.units,),\n",
    "                                        initializer=tf.keras.initializers.get(self.bias_initializer),\n",
    "                                        trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        out = tf.multiply(inputs, self.m)\n",
    "        if self.use_bias:\n",
    "            out += self.bias\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, kernel_initializer='ones', bias_initializer='zeros', use_bias=True, **kwargs):\n",
    "        super(LinearLayer, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n = self.units\n",
    "\n",
    "        self.m = self.add_weight(name=\"kernel_m_1\",\n",
    "                                 shape=(n,),\n",
    "                                 initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                 trainable=True)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name=\"bias\",\n",
    "                                        shape=(self.units,),\n",
    "                                        initializer=tf.keras.initializers.get(self.bias_initializer),\n",
    "                                        trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        print(\"linear layer input:\", inputs.shape)\n",
    "\n",
    "        inputs1 = inputs[:, :int(inputs.shape[1] / 2)]\n",
    "        inputs2 = inputs[:, int(inputs.shape[1] / 2):]\n",
    "\n",
    "        out = tf.math.sqrt(inputs1**2 + inputs2**2)\n",
    "        out = tf.multiply(out, self.m)\n",
    "\n",
    "        if self.use_bias:\n",
    "            out += self.bias\n",
    "\n",
    "        print(\"linear layer output:\", out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import Initializer\n",
    "\n",
    "class CustomScalingInitializer(Initializer):\n",
    "    def __init__(self, n, w0, z0):\n",
    "        self.n = n\n",
    "        self.w0 = w0\n",
    "        self.z0 = z0\n",
    "\n",
    "    def __call__(self, shape, dtype=None):\n",
    "        return tf.convert_to_tensor([(self.z0 / self.w0) ** k for k in range(self.n)], dtype=dtype)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"n\": self.n, \"w0\": self.w0, \"z0\": self.z0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "linear layer input: (None, 32)\n",
      "linear layer output: (None, 16)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " real_input (InputLayer)     [(None, 16)]                 0         []                            \n",
      "                                                                                                  \n",
      " imag_input (InputLayer)     [(None, 16)]                 0         []                            \n",
      "                                                                                                  \n",
      " real_layer1 (FirstLayer)    (None, 16)                   60        ['real_input[0][0]']          \n",
      "                                                                                                  \n",
      " imag_layer1 (FirstLayer)    (None, 16)                   60        ['imag_input[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)     (None, 16)                   0         ['real_layer1[0][0]']         \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 16)                   0         ['imag_layer1[0][0]']         \n",
      "                                                                                                  \n",
      " real_support_layer_1 (Cust  (None, 16)                   32        ['leaky_re_lu[0][0]']         \n",
      " omLayer)                                                                                         \n",
      "                                                                                                  \n",
      " imag_support_layer_1 (Cust  (None, 16)                   32        ['leaky_re_lu_3[0][0]']       \n",
      " omLayer)                                                                                         \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 16)                   0         ['real_support_layer_1[0][0]']\n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 16)                   0         ['imag_support_layer_1[0][0]']\n",
      "                                                                                                  \n",
      " real_layer2 (SecondLayer)   (None, 16)                   24        ['leaky_re_lu_1[0][0]']       \n",
      "                                                                                                  \n",
      " imag_layer2 (SecondLayer)   (None, 16)                   24        ['leaky_re_lu_4[0][0]']       \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 16)                   0         ['real_layer2[0][0]']         \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 16)                   0         ['imag_layer2[0][0]']         \n",
      "                                                                                                  \n",
      " merge_real_imag (Concatena  (None, 32)                   0         ['leaky_re_lu_2[0][0]',       \n",
      " te)                                                                 'leaky_re_lu_5[0][0]']       \n",
      "                                                                                                  \n",
      " diagonal_scaling_layer (Cu  (None, 32)                   64        ['merge_real_imag[0][0]']     \n",
      " stomLayer)                                                                                       \n",
      "                                                                                                  \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 32)                   0         ['diagonal_scaling_layer[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " output_layer (LinearLayer)  (None, 16)                   32        ['leaky_re_lu_6[0][0]']       \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 16)                   0         ['output_layer[0][0]']        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 328 (1.28 KB)\n",
      "Trainable params: 328 (1.28 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Concatenate, LeakyReLU, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def cosine_loss(y_true, y_pred):\n",
    "    y_true_norm = tf.nn.l2_normalize(y_true, axis=-1)\n",
    "    y_pred_norm = tf.nn.l2_normalize(y_pred, axis=-1)\n",
    "    return 1 - tf.reduce_mean(tf.reduce_sum(y_true_norm * y_pred_norm, axis=-1))\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    return mse\n",
    "\n",
    "def hybrid_loss(y_true, y_pred):\n",
    "    mse_part = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    cos_part = cosine_loss(y_true, y_pred)\n",
    "    return 0.4* mse_part + 0.6 * cos_part\n",
    "\n",
    "\n",
    "def structured_NN(input_dim, output_dim):\n",
    "    real_input = Input(shape=(input_dim,), name=\"real_input\")\n",
    "    imag_input = Input(shape=(input_dim,), name=\"imag_input\")\n",
    "    \n",
    "    # he_normal\n",
    "    # glorot_normal\n",
    "    # glorot_uniform\n",
    "    \n",
    "    real_x = FirstLayer(units=input_dim, kernel_initializer='glorot_normal', bias_initializer='glorot_uniform', name=\"real_layer1\")(real_input)\n",
    "    real_x = LeakyReLU(alpha=0.1)(real_x)\n",
    "    real_x = CustomLayer(units=input_dim, kernel_initializer='glorot_normal', bias_initializer='glorot_uniform', name=\"real_support_layer_1\")(real_x)\n",
    "    real_x = LeakyReLU(alpha=0.1)(real_x)\n",
    "    real_x = SecondLayer(units=input_dim, kernel_initializer='glorot_normal', bias_initializer='glorot_uniform', name=\"real_layer2\")(real_x)\n",
    "    real_x = LeakyReLU(alpha=0.1)(real_x)\n",
    "\n",
    "    imag_x = FirstLayer(units=input_dim, kernel_initializer='glorot_normal', bias_initializer='glorot_uniform', name=\"imag_layer1\")(imag_input)\n",
    "    imag_x = LeakyReLU(alpha=0.1)(imag_x)\n",
    "    imag_x = CustomLayer(units=input_dim, kernel_initializer='glorot_normal', bias_initializer='glorot_uniform', name=\"imag_support_layer_1\")(imag_x)\n",
    "    imag_x = LeakyReLU(alpha=0.1)(imag_x)\n",
    "    imag_x = SecondLayer(units=input_dim, kernel_initializer='glorot_normal', bias_initializer='glorot_uniform', name=\"imag_layer2\")(imag_x)\n",
    "    imag_x = LeakyReLU(alpha=0.1)(imag_x)\n",
    "\n",
    "    merged = Concatenate(name=\"merge_real_imag\")([real_x, imag_x])\n",
    "    \n",
    "    # output = CustomLayer(units=input_dim * 2, kernel_initializer=CustomScalingInitializer(n=input_dim * 2, w0=w0, z0=z0), \n",
    "    #                      bias_initializer='zeros', name=\"diagonal_scaling_layer\")(merged)\n",
    "    output = CustomLayer(units=input_dim * 2, kernel_initializer='glorot_normal', bias_initializer='glorot_uniform', name=\"diagonal_scaling_layer\")(merged)\n",
    "    output = LeakyReLU(alpha=0.1)(output)\n",
    "    output = LinearLayer(units=output_dim, kernel_initializer='ones', bias_initializer='zeros', name=\"output_layer\")(output)\n",
    "    output = Activation('linear')(output) # sigmoid # tanh\n",
    "\n",
    "    model = Model(inputs=[real_input, imag_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss=hybrid_loss, metrics=['mse', 'mae'])\n",
    "\n",
    "    return model\n",
    "\n",
    "input_dim = X_real_train.shape[1]\n",
    "output_dim = y_train.shape[1]\n",
    "model = structured_NN(input_dim, output_dim)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real_input [(None, 16)]\n",
      "imag_input [(None, 16)]\n",
      "real_layer1 (None, 16)\n",
      "imag_layer1 (None, 16)\n",
      "leaky_re_lu (None, 16)\n",
      "leaky_re_lu_3 (None, 16)\n",
      "real_support_layer_1 (None, 16)\n",
      "imag_support_layer_1 (None, 16)\n",
      "leaky_re_lu_1 (None, 16)\n",
      "leaky_re_lu_4 (None, 16)\n",
      "real_layer2 (None, 16)\n",
      "imag_layer2 (None, 16)\n",
      "leaky_re_lu_2 (None, 16)\n",
      "leaky_re_lu_5 (None, 16)\n",
      "merge_real_imag (None, 32)\n",
      "diagonal_scaling_layer (None, 32)\n",
      "leaky_re_lu_6 (None, 32)\n",
      "output_layer (None, 16)\n",
      "activation (None, 16)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name, layer.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "adjust_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "linear layer input: (16, 32)\n",
      "linear layer output: (16, 16)\n",
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "linear layer input: (16, 32)\n",
      "linear layer output: (16, 16)\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.3182 - mse: 0.2481 - mae: 0.4057linear layer input: (None, 32)\n",
      "linear layer output: (None, 16)\n",
      "50/50 [==============================] - 10s 49ms/step - loss: 0.3157 - mse: 0.2466 - mae: 0.4046 - val_loss: 0.2434 - val_mse: 0.2122 - val_mae: 0.3804 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.2187 - mse: 0.2066 - mae: 0.3719 - val_loss: 0.2050 - val_mse: 0.1935 - val_mae: 0.3612 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1971 - mse: 0.1886 - mae: 0.3556 - val_loss: 0.1909 - val_mse: 0.1757 - val_mae: 0.3473 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1856 - mse: 0.1716 - mae: 0.3439 - val_loss: 0.1810 - val_mse: 0.1593 - val_mae: 0.3362 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1767 - mse: 0.1563 - mae: 0.3334 - val_loss: 0.1725 - val_mse: 0.1461 - val_mae: 0.3261 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1693 - mse: 0.1437 - mae: 0.3236 - val_loss: 0.1657 - val_mse: 0.1356 - val_mae: 0.3173 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1634 - mse: 0.1346 - mae: 0.3155 - val_loss: 0.1601 - val_mse: 0.1271 - val_mae: 0.3082 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1582 - mse: 0.1271 - mae: 0.3074 - val_loss: 0.1548 - val_mse: 0.1210 - val_mae: 0.3006 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1530 - mse: 0.1214 - mae: 0.3000 - val_loss: 0.1493 - val_mse: 0.1159 - val_mae: 0.2928 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1480 - mse: 0.1169 - mae: 0.2930 - val_loss: 0.1445 - val_mse: 0.1120 - val_mae: 0.2856 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1423 - mse: 0.1121 - mae: 0.2842 - val_loss: 0.1389 - val_mse: 0.1076 - val_mae: 0.2771 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1379 - mse: 0.1088 - mae: 0.2776 - val_loss: 0.1354 - val_mse: 0.1052 - val_mae: 0.2719 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1350 - mse: 0.1067 - mae: 0.2734 - val_loss: 0.1328 - val_mse: 0.1033 - val_mae: 0.2682 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.1325 - mse: 0.1049 - mae: 0.2702 - val_loss: 0.1305 - val_mse: 0.1018 - val_mae: 0.2652 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1304 - mse: 0.1035 - mae: 0.2673 - val_loss: 0.1284 - val_mse: 0.1003 - val_mae: 0.2622 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1281 - mse: 0.1018 - mae: 0.2644 - val_loss: 0.1262 - val_mse: 0.0987 - val_mae: 0.2592 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1259 - mse: 0.1003 - mae: 0.2616 - val_loss: 0.1238 - val_mse: 0.0971 - val_mae: 0.2567 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1232 - mse: 0.0983 - mae: 0.2582 - val_loss: 0.1209 - val_mse: 0.0950 - val_mae: 0.2529 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1203 - mse: 0.0962 - mae: 0.2545 - val_loss: 0.1172 - val_mse: 0.0925 - val_mae: 0.2485 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1170 - mse: 0.0938 - mae: 0.2497 - val_loss: 0.1130 - val_mse: 0.0895 - val_mae: 0.2426 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.1138 - mse: 0.0915 - mae: 0.2451 - val_loss: 0.1110 - val_mse: 0.0881 - val_mae: 0.2394 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1116 - mse: 0.0898 - mae: 0.2415 - val_loss: 0.1089 - val_mse: 0.0865 - val_mae: 0.2361 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1100 - mse: 0.0886 - mae: 0.2385 - val_loss: 0.1074 - val_mse: 0.0854 - val_mae: 0.2332 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1084 - mse: 0.0874 - mae: 0.2357 - val_loss: 0.1057 - val_mse: 0.0842 - val_mae: 0.2300 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1070 - mse: 0.0863 - mae: 0.2329 - val_loss: 0.1044 - val_mse: 0.0832 - val_mae: 0.2281 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1058 - mse: 0.0854 - mae: 0.2307 - val_loss: 0.1033 - val_mse: 0.0823 - val_mae: 0.2257 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1048 - mse: 0.0846 - mae: 0.2284 - val_loss: 0.1023 - val_mse: 0.0815 - val_mae: 0.2232 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1037 - mse: 0.0837 - mae: 0.2264 - val_loss: 0.1014 - val_mse: 0.0808 - val_mae: 0.2214 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1027 - mse: 0.0829 - mae: 0.2244 - val_loss: 0.1003 - val_mse: 0.0800 - val_mae: 0.2197 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1017 - mse: 0.0822 - mae: 0.2228 - val_loss: 0.0993 - val_mse: 0.0792 - val_mae: 0.2179 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1006 - mse: 0.0813 - mae: 0.2211 - val_loss: 0.0980 - val_mse: 0.0782 - val_mae: 0.2160 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0992 - mse: 0.0802 - mae: 0.2190 - val_loss: 0.0962 - val_mse: 0.0768 - val_mae: 0.2132 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0977 - mse: 0.0791 - mae: 0.2166 - val_loss: 0.0944 - val_mse: 0.0753 - val_mae: 0.2105 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0962 - mse: 0.0778 - mae: 0.2139 - val_loss: 0.0931 - val_mse: 0.0743 - val_mae: 0.2077 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0947 - mse: 0.0767 - mae: 0.2111 - val_loss: 0.0915 - val_mse: 0.0730 - val_mae: 0.2047 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0936 - mse: 0.0757 - mae: 0.2082 - val_loss: 0.0903 - val_mse: 0.0721 - val_mae: 0.2021 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0926 - mse: 0.0750 - mae: 0.2061 - val_loss: 0.0892 - val_mse: 0.0712 - val_mae: 0.1997 - lr: 0.0010\n",
      "Epoch 38/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0916 - mse: 0.0741 - mae: 0.2040 - val_loss: 0.0883 - val_mse: 0.0705 - val_mae: 0.1979 - lr: 0.0010\n",
      "Epoch 39/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0903 - mse: 0.0731 - mae: 0.2021 - val_loss: 0.0870 - val_mse: 0.0695 - val_mae: 0.1958 - lr: 0.0010\n",
      "Epoch 40/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0889 - mse: 0.0720 - mae: 0.1994 - val_loss: 0.0850 - val_mse: 0.0679 - val_mae: 0.1927 - lr: 0.0010\n",
      "Epoch 41/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0872 - mse: 0.0707 - mae: 0.1965 - val_loss: 0.0833 - val_mse: 0.0666 - val_mae: 0.1893 - lr: 0.0010\n",
      "Epoch 42/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0858 - mse: 0.0696 - mae: 0.1935 - val_loss: 0.0824 - val_mse: 0.0659 - val_mae: 0.1873 - lr: 0.0010\n",
      "Epoch 43/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0850 - mse: 0.0689 - mae: 0.1917 - val_loss: 0.0819 - val_mse: 0.0654 - val_mae: 0.1858 - lr: 0.0010\n",
      "Epoch 44/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0845 - mse: 0.0685 - mae: 0.1903 - val_loss: 0.0817 - val_mse: 0.0652 - val_mae: 0.1846 - lr: 0.0010\n",
      "Epoch 45/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0842 - mse: 0.0682 - mae: 0.1896 - val_loss: 0.0814 - val_mse: 0.0650 - val_mae: 0.1839 - lr: 0.0010\n",
      "Epoch 46/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0840 - mse: 0.0681 - mae: 0.1888 - val_loss: 0.0810 - val_mse: 0.0647 - val_mae: 0.1827 - lr: 0.0010\n",
      "Epoch 47/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0836 - mse: 0.0677 - mae: 0.1878 - val_loss: 0.0806 - val_mse: 0.0643 - val_mae: 0.1816 - lr: 0.0010\n",
      "Epoch 48/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0836 - mse: 0.0677 - mae: 0.1873 - val_loss: 0.0809 - val_mse: 0.0645 - val_mae: 0.1817 - lr: 0.0010\n",
      "Epoch 49/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0832 - mse: 0.0674 - mae: 0.1866 - val_loss: 0.0809 - val_mse: 0.0645 - val_mae: 0.1814 - lr: 0.0010\n",
      "Epoch 50/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0830 - mse: 0.0672 - mae: 0.1858 - val_loss: 0.0804 - val_mse: 0.0641 - val_mae: 0.1804 - lr: 0.0010\n",
      "Epoch 51/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0829 - mse: 0.0671 - mae: 0.1857 - val_loss: 0.0801 - val_mse: 0.0639 - val_mae: 0.1798 - lr: 0.0010\n",
      "Epoch 52/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0827 - mse: 0.0669 - mae: 0.1849 - val_loss: 0.0801 - val_mse: 0.0638 - val_mae: 0.1793 - lr: 0.0010\n",
      "Epoch 53/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0826 - mse: 0.0668 - mae: 0.1846 - val_loss: 0.0801 - val_mse: 0.0639 - val_mae: 0.1795 - lr: 0.0010\n",
      "Epoch 54/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0825 - mse: 0.0667 - mae: 0.1843 - val_loss: 0.0800 - val_mse: 0.0638 - val_mae: 0.1788 - lr: 0.0010\n",
      "Epoch 55/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0824 - mse: 0.0666 - mae: 0.1838 - val_loss: 0.0798 - val_mse: 0.0637 - val_mae: 0.1785 - lr: 0.0010\n",
      "Epoch 56/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0822 - mse: 0.0665 - mae: 0.1836 - val_loss: 0.0798 - val_mse: 0.0637 - val_mae: 0.1784 - lr: 0.0010\n",
      "Epoch 57/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0821 - mse: 0.0664 - mae: 0.1833 - val_loss: 0.0798 - val_mse: 0.0637 - val_mae: 0.1782 - lr: 0.0010\n",
      "Epoch 58/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0821 - mse: 0.0664 - mae: 0.1832 - val_loss: 0.0799 - val_mse: 0.0638 - val_mae: 0.1787 - lr: 0.0010\n",
      "Epoch 59/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0822 - mse: 0.0665 - mae: 0.1832 - val_loss: 0.0797 - val_mse: 0.0636 - val_mae: 0.1778 - lr: 0.0010\n",
      "Epoch 60/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0821 - mse: 0.0664 - mae: 0.1831 - val_loss: 0.0798 - val_mse: 0.0638 - val_mae: 0.1785 - lr: 0.0010\n",
      "Epoch 61/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0819 - mse: 0.0663 - mae: 0.1828 - val_loss: 0.0795 - val_mse: 0.0635 - val_mae: 0.1772 - lr: 0.0010\n",
      "Epoch 62/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0818 - mse: 0.0662 - mae: 0.1825 - val_loss: 0.0797 - val_mse: 0.0636 - val_mae: 0.1774 - lr: 0.0010\n",
      "Epoch 63/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0818 - mse: 0.0661 - mae: 0.1821 - val_loss: 0.0797 - val_mse: 0.0637 - val_mae: 0.1777 - lr: 0.0010\n",
      "Epoch 64/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0817 - mse: 0.0661 - mae: 0.1822 - val_loss: 0.0796 - val_mse: 0.0636 - val_mae: 0.1772 - lr: 0.0010\n",
      "Epoch 65/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0817 - mse: 0.0661 - mae: 0.1820 - val_loss: 0.0796 - val_mse: 0.0636 - val_mae: 0.1774 - lr: 0.0010\n",
      "Epoch 66/500\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.0818 - mse: 0.0661 - mae: 0.1822\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0817 - mse: 0.0661 - mae: 0.1820 - val_loss: 0.0796 - val_mse: 0.0636 - val_mae: 0.1771 - lr: 0.0010\n",
      "Epoch 67/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0815 - mse: 0.0659 - mae: 0.1815 - val_loss: 0.0794 - val_mse: 0.0635 - val_mae: 0.1770 - lr: 5.0000e-04\n",
      "Epoch 68/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0814 - mse: 0.0658 - mae: 0.1815 - val_loss: 0.0794 - val_mse: 0.0635 - val_mae: 0.1767 - lr: 5.0000e-04\n",
      "Epoch 69/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0814 - mse: 0.0658 - mae: 0.1815 - val_loss: 0.0794 - val_mse: 0.0635 - val_mae: 0.1767 - lr: 5.0000e-04\n",
      "Epoch 70/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0814 - mse: 0.0658 - mae: 0.1812 - val_loss: 0.0794 - val_mse: 0.0636 - val_mae: 0.1774 - lr: 5.0000e-04\n",
      "Epoch 71/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0813 - mse: 0.0658 - mae: 0.1814 - val_loss: 0.0794 - val_mse: 0.0635 - val_mae: 0.1765 - lr: 5.0000e-04\n",
      "Epoch 72/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0813 - mse: 0.0658 - mae: 0.1813 - val_loss: 0.0794 - val_mse: 0.0635 - val_mae: 0.1765 - lr: 5.0000e-04\n",
      "Epoch 73/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0813 - mse: 0.0657 - mae: 0.1811 - val_loss: 0.0794 - val_mse: 0.0636 - val_mae: 0.1767 - lr: 5.0000e-04\n",
      "Epoch 74/500\n",
      "39/50 [======================>.......] - ETA: 0s - loss: 0.0810 - mse: 0.0653 - mae: 0.1802\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0812 - mse: 0.0657 - mae: 0.1810 - val_loss: 0.0794 - val_mse: 0.0635 - val_mae: 0.1764 - lr: 5.0000e-04\n",
      "Epoch 75/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0812 - mse: 0.0657 - mae: 0.1810 - val_loss: 0.0793 - val_mse: 0.0636 - val_mae: 0.1764 - lr: 2.5000e-04\n",
      "Epoch 76/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0811 - mse: 0.0656 - mae: 0.1809 - val_loss: 0.0794 - val_mse: 0.0636 - val_mae: 0.1764 - lr: 2.5000e-04\n",
      "Epoch 77/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0811 - mse: 0.0656 - mae: 0.1807 - val_loss: 0.0793 - val_mse: 0.0636 - val_mae: 0.1764 - lr: 2.5000e-04\n",
      "Epoch 78/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0811 - mse: 0.0657 - mae: 0.1809 - val_loss: 0.0794 - val_mse: 0.0636 - val_mae: 0.1762 - lr: 2.5000e-04\n",
      "Epoch 79/500\n",
      "37/50 [=====================>........] - ETA: 0s - loss: 0.0805 - mse: 0.0653 - mae: 0.1805\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0811 - mse: 0.0656 - mae: 0.1808 - val_loss: 0.0794 - val_mse: 0.0636 - val_mae: 0.1763 - lr: 2.5000e-04\n",
      "Epoch 80/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0810 - mse: 0.0656 - mae: 0.1807 - val_loss: 0.0793 - val_mse: 0.0636 - val_mae: 0.1763 - lr: 1.2500e-04\n",
      "Epoch 81/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0811 - mse: 0.0656 - mae: 0.1806 - val_loss: 0.0793 - val_mse: 0.0636 - val_mae: 0.1763 - lr: 1.2500e-04\n",
      "Epoch 82/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0810 - mse: 0.0656 - mae: 0.1806 - val_loss: 0.0793 - val_mse: 0.0636 - val_mae: 0.1762 - lr: 1.2500e-04\n",
      "Epoch 83/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0810 - mse: 0.0656 - mae: 0.1806 - val_loss: 0.0793 - val_mse: 0.0636 - val_mae: 0.1762 - lr: 1.2500e-04\n",
      "Epoch 84/500\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0804 - mse: 0.0651 - mae: 0.1798\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0810 - mse: 0.0656 - mae: 0.1805 - val_loss: 0.0793 - val_mse: 0.0636 - val_mae: 0.1762 - lr: 1.2500e-04\n",
      "Epoch 85/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0810 - mse: 0.0655 - mae: 0.1806 - val_loss: 0.0793 - val_mse: 0.0636 - val_mae: 0.1762 - lr: 6.2500e-05\n",
      "Epoch 86/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0810 - mse: 0.0655 - mae: 0.1806 - val_loss: 0.0793 - val_mse: 0.0636 - val_mae: 0.1762 - lr: 6.2500e-05\n",
      "Epoch 87/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0810 - mse: 0.0655 - mae: 0.1806 - val_loss: 0.0793 - val_mse: 0.0636 - val_mae: 0.1761 - lr: 6.2500e-05\n",
      "Epoch 88/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0810 - mse: 0.0655 - mae: 0.1805 - val_loss: 0.0793 - val_mse: 0.0636 - val_mae: 0.1761 - lr: 6.2500e-05\n",
      "Epoch 89/500\n",
      "45/50 [==========================>...] - ETA: 0s - loss: 0.0814 - mse: 0.0659 - mae: 0.1813\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0810 - mse: 0.0655 - mae: 0.1805 - val_loss: 0.0793 - val_mse: 0.0636 - val_mae: 0.1761 - lr: 6.2500e-05\n",
      "Epoch 90/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0810 - mse: 0.0655 - mae: 0.1805 - val_loss: 0.0793 - val_mse: 0.0636 - val_mae: 0.1761 - lr: 3.1250e-05\n",
      "Epoch 91/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0810 - mse: 0.0655 - mae: 0.1805 - val_loss: 0.0793 - val_mse: 0.0636 - val_mae: 0.1762 - lr: 3.1250e-05\n",
      "Epoch 92/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0810 - mse: 0.0655 - mae: 0.1805 - val_loss: 0.0793 - val_mse: 0.0636 - val_mae: 0.1761 - lr: 3.1250e-05\n",
      "Epoch 93/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0810 - mse: 0.0655 - mae: 0.1805 - val_loss: 0.0793 - val_mse: 0.0636 - val_mae: 0.1761 - lr: 3.1250e-05\n",
      "Epoch 94/500\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0810 - mse: 0.0656 - mae: 0.1806\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0810 - mse: 0.0655 - mae: 0.1805 - val_loss: 0.0793 - val_mse: 0.0636 - val_mae: 0.1761 - lr: 3.1250e-05\n",
      "Epoch 95/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0810 - mse: 0.0655 - mae: 0.1805 - val_loss: 0.0793 - val_mse: 0.0636 - val_mae: 0.1761 - lr: 1.5625e-05\n",
      "Epoch 96/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0810 - mse: 0.0655 - mae: 0.1805 - val_loss: 0.0793 - val_mse: 0.0636 - val_mae: 0.1761 - lr: 1.5625e-05\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [X_real_train, X_imag_train],\n",
    "    y_train,\n",
    "    epochs=500,\n",
    "    batch_size=16,\n",
    "    validation_data=([X_real_test, X_imag_test], y_test),\n",
    "    callbacks=[adjust_lr, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0793 - mse: 0.0636 - mae: 0.1762\n",
      "Test MSE: 0.0636, Test MAE: 0.1762\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_mse, test_mae = model.evaluate([X_real_test, X_imag_test], y_test)\n",
    "print(f\"Test MSE: {test_mse:.4f}, Test MAE: {test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear layer input: (None, 32)\n",
      "linear layer output: (None, 16)\n",
      "7/7 [==============================] - 1s 3ms/step\n",
      "Comparison of predictions and ground truth:\n",
      "Sample 1:\n",
      "predicted as is:  [0.44383672 0.48603478 0.53472614 0.48886588 0.48485476 0.48317832\n",
      " 0.5665616  0.46203163 0.27088946 0.48238748 0.35986286 0.5413457\n",
      " 0.19802317 0.25216937 0.9065247  0.45193687]\n",
      "  Predicted:    [2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1. 2. 1. 1. 4. 2.]\n",
      "  Ground Truth: [1. 0. 1. 1. 1. 1. 3. 0. 1. 3. 1. 2. 1. 1. 4. 2.]\n",
      "----------------------------------------\n",
      "Sample 2:\n",
      "predicted as is:  [0.527374   0.45661992 0.44533956 0.5143304  0.5160997  0.48124358\n",
      " 0.42599738 0.4313909  0.21398124 0.5331556  0.5336961  0.60673815\n",
      " 0.7473113  0.00763299 0.33910936 0.00702644]\n",
      "  Predicted:    [2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 3. 0. 1. 0.]\n",
      "  Ground Truth: [4. 0. 2. 0. 2. 0. 3. 2. 1. 2. 2. 2. 3. 0. 1. 0.]\n",
      "----------------------------------------\n",
      "Sample 3:\n",
      "predicted as is:  [0.4628202  0.4711332  0.46305507 0.40925178 0.54056644 0.47382215\n",
      " 0.4732408  0.45232108 0.73687696 0.8575354  0.3616426  0.832108\n",
      " 1.0264826  0.7581826  0.00811872 0.2940851 ]\n",
      "  Predicted:    [2. 2. 2. 2. 2. 2. 2. 2. 3. 3. 1. 3. 4. 3. 0. 1.]\n",
      "  Ground Truth: [3. 2. 0. 0. 2. 0. 4. 4. 3. 3. 1. 3. 4. 3. 0. 1.]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = model.predict([X_real_test, X_imag_test])\n",
    "y_pred_test_rescaled = y_pred_test * (q - 1)\n",
    "# y_pred_test_rescaled = (y_pred_test * y_std) + y_mean\n",
    "y_test_rescaled = y_test * (q - 1)\n",
    "\n",
    "print(\"Comparison of predictions and ground truth:\")\n",
    "for i in range(3):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"predicted as is: \", y_pred_test[i])\n",
    "    print(f\"  Predicted:    {np.round(y_pred_test_rescaled[i])}\")\n",
    "    print(f\"  Ground Truth: {np.round(y_test_rescaled[i])}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoded Unseen Data (Real and Imaginary Parts):\n",
      "Real Part Shape: (5, 16)\n",
      "Imaginary Part Shape: (5, 16)\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\n",
      "Comparison of Unseen Data and Predictions:\n",
      "============================================================\n",
      "Sample 1:\n",
      "  Ground Truth: [2 3 2 1 0 0 0 4 4 1 2 4 2 2 4 4]\n",
      "  Prediction  : [2 2 2 2 2 2 2 2 4 2 2 4 2 2 4 4]\n",
      "------------------------------------------------------------\n",
      "Sample 2:\n",
      "  Ground Truth: [1 3 0 4 4 2 3 4 3 0 2 1 0 3 3 0]\n",
      "  Prediction  : [2 2 2 2 2 2 2 2 3 1 2 1 0 3 3 0]\n",
      "------------------------------------------------------------\n",
      "Sample 3:\n",
      "  Ground Truth: [3 1 1 3 3 1 3 4 2 4 3 3 2 3 2 1]\n",
      "  Prediction  : [2 2 2 2 2 2 2 2 2 4 3 3 2 3 2 1]\n",
      "------------------------------------------------------------\n",
      "Sample 4:\n",
      "  Ground Truth: [2 2 4 3 1 0 2 0 3 2 0 4 0 4 1 3]\n",
      "  Prediction  : [2 2 2 2 2 2 2 2 3 1 0 4 0 4 1 3]\n",
      "------------------------------------------------------------\n",
      "Sample 5:\n",
      "  Ground Truth: [0 2 3 0 2 2 2 2 0 1 4 4 4 1 2 2]\n",
      "  Prediction  : [2 2 2 2 2 2 2 2 0 1 4 4 4 1 3 2]\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_unseen_samples = 5\n",
    "unseen_data = np.random.randint(0, q, size=(num_unseen_samples, n))\n",
    "\n",
    "padded_unseen_data = np.hstack((unseen_data, np.zeros((num_unseen_samples, n_padded - n))))\n",
    "\n",
    "unseen_data_normalized = unseen_data.astype(np.float32) / (q - 1)\n",
    "\n",
    "# print(\"Unseen Input Data (Before Encoding):\")\n",
    "# print(unseen_data)\n",
    "\n",
    "unseen_encoded = np.array([np.dot(M_tilde, x) for x in padded_unseen_data])\n",
    "unseen_encoded[np.abs(unseen_encoded) < 1e-10] = 0\n",
    "unseen_encoded = np.round(unseen_encoded, decimals=10)\n",
    "\n",
    "X_real_unseen = np.real(unseen_encoded).astype(np.float32)\n",
    "X_imag_unseen = np.imag(unseen_encoded).astype(np.float32)\n",
    "\n",
    "print(\"\\nEncoded Unseen Data (Real and Imaginary Parts):\")\n",
    "print(\"Real Part Shape:\", X_real_unseen.shape)\n",
    "print(\"Imaginary Part Shape:\", X_imag_unseen.shape)\n",
    "\n",
    "y_pred_unseen = model.predict([X_real_unseen, X_imag_unseen])\n",
    "\n",
    "y_pred_unseen_rescaled = y_pred_unseen * (q - 1)\n",
    "\n",
    "y_pred_unseen_final = np.mod(np.round(y_pred_unseen_rescaled), q)\n",
    "\n",
    "print(\"\\nComparison of Unseen Data and Predictions:\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(num_unseen_samples):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  Ground Truth: {padded_unseen_data[i].astype(int)}\")\n",
    "    print(f\"  Prediction  : {y_pred_unseen_final[i].astype(int)}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check gradient flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real_input has no trainable variables.\n",
      "imag_input has no trainable variables.\n",
      "real_layer1 Gradient Mean: 0.2382715\n",
      "imag_layer1 Gradient Mean: 0.33378655\n",
      "leaky_re_lu has no trainable variables.\n",
      "leaky_re_lu_3 has no trainable variables.\n",
      "real_support_layer_1 Gradient Mean: 0.38555908\n",
      "imag_support_layer_1 Gradient Mean: 0.472827\n",
      "leaky_re_lu_1 has no trainable variables.\n",
      "leaky_re_lu_4 has no trainable variables.\n",
      "real_layer2 Gradient Mean: 0.5938346\n",
      "imag_layer2 Gradient Mean: 0.56571734\n",
      "leaky_re_lu_2 has no trainable variables.\n",
      "leaky_re_lu_5 has no trainable variables.\n",
      "merge_real_imag has no trainable variables.\n",
      "diagonal_scaling_layer Gradient Mean: 0.40031403\n",
      "leaky_re_lu_6 has no trainable variables.\n",
      "output_layer Gradient Mean: 1.1929548\n",
      "activation has no trainable variables.\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    if layer.trainable_variables:\n",
    "        grads = tf.reduce_mean(tf.abs(layer.trainable_variables[0]))\n",
    "        print(layer.name, \"Gradient Mean:\", grads.numpy())\n",
    "    else:\n",
    "        print(layer.name, \"has no trainable variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear layer input: (4, 32)\n",
      "linear layer output: (4, 16)\n",
      " Gradient OK for real_layer1/kernel_b1:0, mean: 0.0019327610498294234\n",
      " Gradient OK for real_layer1/kernel_b2:0, mean: 0.00017200794536620378\n",
      " Gradient OK for real_layer1/kernel_d1:0, mean: 0.0004911997239105403\n",
      " Gradient OK for real_layer1/kernel_d2:0, mean: -0.0006354645011015236\n",
      " Gradient OK for real_layer1/bias:0, mean: 0.0009436324471607804\n",
      " Gradient OK for imag_layer1/kernel_b1:0, mean: -0.0003010322107002139\n",
      " Gradient OK for imag_layer1/kernel_b2:0, mean: -0.0002690846740733832\n",
      " Gradient OK for imag_layer1/kernel_d1:0, mean: -0.00022621317475568503\n",
      " Gradient OK for imag_layer1/kernel_d2:0, mean: 0.00020413317542988807\n",
      " Gradient OK for imag_layer1/bias:0, mean: -0.0006644867826253176\n",
      " Gradient OK for real_support_layer_1/kernel_m:0, mean: -0.0006778326351195574\n",
      " Gradient OK for real_support_layer_1/bias:0, mean: -0.005381305702030659\n",
      " Gradient OK for imag_support_layer_1/kernel_m:0, mean: -0.0011643213219940662\n",
      " Gradient OK for imag_support_layer_1/bias:0, mean: -0.0031910687685012817\n",
      " Gradient OK for real_layer2/kernel_d1:0, mean: 0.004191596992313862\n",
      " Gradient OK for real_layer2/bias:0, mean: -0.007987899705767632\n",
      " Gradient OK for imag_layer2/kernel_d1:0, mean: 0.005029289051890373\n",
      " Gradient OK for imag_layer2/bias:0, mean: -0.002647761721163988\n",
      " Gradient OK for diagonal_scaling_layer/kernel_m:0, mean: 0.002551817800849676\n",
      " Gradient OK for diagonal_scaling_layer/bias:0, mean: 0.007639055605977774\n",
      " Gradient OK for output_layer/kernel_m_1:0, mean: -0.005014162044972181\n",
      " Gradient OK for output_layer/bias:0, mean: -0.14645454287528992\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4 \n",
    "X_real_dummy = np.random.rand(batch_size, X_real_train.shape[1]).astype(np.float32)\n",
    "X_imag_dummy = np.random.rand(batch_size, X_imag_train.shape[1]).astype(np.float32)\n",
    "y_dummy = np.random.rand(batch_size, y_train.shape[1]).astype(np.float32)\n",
    "\n",
    "def check_gradients(model, X_real_dummy, X_imag_dummy, y_dummy):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(model.trainable_variables)\n",
    "        predictions = model([X_real_dummy, X_imag_dummy], training=True)\n",
    "        loss = tf.keras.losses.mean_squared_error(y_dummy, predictions)\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    for i, (grad, var) in enumerate(zip(grads, model.trainable_variables)):\n",
    "        if grad is None:\n",
    "            print(f\" Gradient is None for {var.name} at index {i}\")\n",
    "        else:\n",
    "            print(f\" Gradient OK for {var.name}, mean: {tf.reduce_mean(grad).numpy()}\")\n",
    "\n",
    "check_gradients(model, X_real_dummy, X_imag_dummy, y_dummy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
