{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DFTSNN - Generator Matrix Encoded dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is encoded using the padded generator matrix. the imaginary and real values are parellel processed until concatenation. This mimics the IDFT SNN. One additional layer is included to account for the scaling diagonal matrix $\\hat{D}_n$ in the classical algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (r + 1) | (q - 1)\n",
    "# (r + 1) | n\n",
    "# q is a prime number\n",
    "# n = 2^t\n",
    "# r < n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset -f\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original n: 16\n",
      "Padded n: 16\n",
      "Generated dataset shape: (1000, 16)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n = 16\n",
    "q = 5\n",
    "num_samples = 1000\n",
    "\n",
    "def next_power_of_two(x):\n",
    "    return 1 if x == 0 else 2**(x - 1).bit_length()\n",
    "\n",
    "n_padded = next_power_of_two(n)\n",
    "\n",
    "dataset = np.random.randint(0, q, size=(num_samples, n))\n",
    "\n",
    "if n_padded > n:\n",
    "    pad_width = n_padded - n\n",
    "    dataset = np.pad(dataset, ((0, 0), (0, pad_width)), mode='constant', constant_values=0)\n",
    "\n",
    "print(\"Original n:\", n)\n",
    "print(\"Padded n:\", n_padded)\n",
    "print(\"Generated dataset shape:\", dataset.shape)\n",
    "\n",
    "n = n_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 4 2 ... 1 3 1]\n",
      " [3 4 0 ... 3 2 3]\n",
      " [3 0 2 ... 0 1 4]\n",
      " ...\n",
      " [4 4 4 ... 0 2 2]\n",
      " [4 2 4 ... 4 0 0]\n",
      " [2 2 2 ... 0 4 2]]\n",
      "(1000, 16)\n",
      "(16,)\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(dataset.shape)\n",
    "print(dataset[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\tilde{M}_{kj} = \\left[ \\left( \\frac{w_0}{z_0} \\right)^j \\zeta^{kj} \\right]_{k,j=0}^{n-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padded_generator_matrix(n, w0, z0):\n",
    "    zeta = np.exp(-2j * np.pi / n)\n",
    "    M_tilde = np.array([[(w0 / z0) ** j * zeta**(k * j) for j in range(n)] for k in range(n)], dtype=complex)\n",
    "    return M_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 16)\n"
     ]
    }
   ],
   "source": [
    "# (x, y, z, w) --> (1, 2, 3, 4)\n",
    "w0 = 4\n",
    "z0 = 3\n",
    "\n",
    "M_tilde = padded_generator_matrix(n, w0, z0)\n",
    "print(M_tilde.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 649.89449698  +0.j           69.97998169+371.13335899j\n",
      "   -83.92481787+137.40000024j ...  -88.37954911-103.22398682j\n",
      "   -83.92481787-137.40000024j   69.97998169-371.13335899j]\n",
      " [ 695.87003951  +0.j          268.59479107+394.57072111j\n",
      "   -36.50274968+309.16102475j ...  -47.36788425-179.88976988j\n",
      "   -36.50274968-309.16102475j  268.59479107-394.57072111j]\n",
      " [ 540.63797786  +0.j          235.29809241+209.31041098j\n",
      "   176.66145652+204.71795948j ...  108.87153914-320.22828928j\n",
      "   176.66145652-204.71795948j  235.29809241-209.31041098j]\n",
      " ...\n",
      " [ 460.45123158  +0.j          162.64910275+168.11969395j\n",
      "    54.45039471+219.81284778j ...  -26.91067254-124.72256855j\n",
      "    54.45039471-219.81284778j  162.64910275-168.11969395j]\n",
      " [ 558.17613119  +0.j          -92.41727316+362.85897707j\n",
      "  -228.64175513 +13.85662093j ...  -76.51498185+176.39278765j\n",
      "  -228.64175513 -13.85662093j  -92.41727316-362.85897707j]\n",
      " [ 671.99178181  +0.j          161.97417378+352.04621278j\n",
      "    43.09272997+244.01831359j ...  -56.20864468-237.97753496j\n",
      "    43.09272997-244.01831359j  161.97417378-352.04621278j]]\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset = np.array([np.dot(M_tilde, x) for x in dataset])\n",
    "encoded_dataset[np.abs(encoded_dataset) < 1e-10] = 0\n",
    "encoded_dataset = np.round(encoded_dataset, decimals=10)\n",
    "print(encoded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 16)\n"
     ]
    }
   ],
   "source": [
    "print(encoded_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 2 4 4 1 2 2 2 4 3 2 4 1 3 1]\n",
      "[ 649.89449698  +0.j           69.97998169+371.13335899j\n",
      "  -83.92481787+137.40000024j  -88.37954911+103.22398682j\n",
      "  -74.53901813 +41.73652084j  -97.48254893 +37.96448017j\n",
      " -147.95858217-108.15626408j   47.97234649-148.66691513j\n",
      "  146.76987906  -0.j           47.97234649+148.66691513j\n",
      " -147.95858217+108.15626408j  -97.48254893 -37.96448017j\n",
      "  -74.53901813 -41.73652084j  -88.37954911-103.22398682j\n",
      "  -83.92481787-137.40000024j   69.97998169-371.13335899j]\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])\n",
    "print(encoded_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to zero mean and unit variance - output results of normalizing made unseen data converge to q-1\n",
    "# encoded_dataset = (encoded_dataset - np.mean(encoded_dataset, axis=0)) / np.std(encoded_dataset, axis=0)\n",
    "# print(encoded_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train_real : (1000, 16)\n",
      "[ 649.8945     69.97998   -83.92482   -88.37955   -74.53902   -97.48255\n",
      " -147.95859    47.972347  146.76988    47.972347 -147.95859   -97.48255\n",
      "  -74.53902   -88.37955   -83.92482    69.97998 ]\n",
      "\n",
      "X_train_imag : (1000, 16)\n",
      "[   0.        371.13336   137.4       103.22398    41.736523   37.96448\n",
      " -108.156265 -148.66692    -0.        148.66692   108.156265  -37.96448\n",
      "  -41.736523 -103.22398  -137.4      -371.13336 ]\n"
     ]
    }
   ],
   "source": [
    "X_real = np.real(encoded_dataset).astype(np.float32)\n",
    "X_imag = np.imag(encoded_dataset).astype(np.float32)\n",
    "\n",
    "print(\"\\nX_train_real :\", X_real.shape)\n",
    "print(X_real[0])\n",
    "print(\"\\nX_train_imag :\", X_imag.shape)\n",
    "print(X_imag[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: \n",
      "[3. 4. 2. 4. 4. 1. 2. 2. 2. 4. 3. 2. 4. 1. 3. 1.]\n",
      "\n",
      "y_normalized: \n",
      "[0.75 1.   0.5  1.   1.   0.25 0.5  0.5  0.5  1.   0.75 0.5  1.   0.25\n",
      " 0.75 0.25]\n"
     ]
    }
   ],
   "source": [
    "# normalize target data (integers 0-q to [0, 1])\n",
    "y_normalized = dataset.astype(np.float32) / (q - 1)  # Scale to [0, 1]\n",
    "print(\"y: \")\n",
    "print(dataset.astype(np.float32)[0])\n",
    "print(\"\\ny_normalized: \")\n",
    "print(y_normalized[0])\n",
    "\n",
    "labels = y_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shapes: X_real: (800, 16) X_imag: (800, 16) y: (800, 16)\n",
      "Testing data shapes: X_real: (200, 16) X_imag: (200, 16) y: (200, 16)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_real_train, X_real_test, y_train, y_test = train_test_split(\n",
    "    X_real, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_imag_train, X_imag_test, _, _ = train_test_split(\n",
    "    X_imag, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training data shapes: X_real:\", X_real_train.shape, \"X_imag:\", X_imag_train.shape, \"y:\", y_train.shape)\n",
    "print(\"Testing data shapes: X_real:\", X_real_test.shape, \"X_imag:\", X_imag_test.shape, \"y:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "print(X_real_train.shape[1])\n",
    "print(X_real_test.shape[1])\n",
    "print(X_imag_train.shape[1])\n",
    "print(X_imag_test.shape[1])\n",
    "print(y_train.shape[1])\n",
    "print(y_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_real_train.shape[1]\n",
    "output_dim = y_train.shape[1]\n",
    "print(input_dim)\n",
    "print(output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDFT - Structure Imposed Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, kernel_initializer='he_normal', bias_initializer='zeros', use_bias=True, **kwargs):\n",
    "        super(FirstLayer, self).__init__(**kwargs)\n",
    "        self.units = units  # Features/neurons\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n = self.units\n",
    "        n1 = n // 2\n",
    "        num_blocks = n1 // 2\n",
    "\n",
    "        self.b_1 = self.add_weight(name=\"kernel_b1\",\n",
    "                                   shape=(num_blocks, 2, 2),\n",
    "                                   initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                   trainable=True)\n",
    "        self.b_2 = self.add_weight(name=\"kernel_b2\",\n",
    "                                   shape=(num_blocks, 2, 2),\n",
    "                                   initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                   trainable=True)\n",
    "        self.d_1 = self.add_weight(name=\"kernel_d1\",\n",
    "                                   shape=(n1 - 2,),\n",
    "                                   initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                   trainable=True)\n",
    "        self.d_2 = self.add_weight(name=\"kernel_d2\",\n",
    "                                   shape=(n1 - 2,),\n",
    "                                   initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                   trainable=True)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name=\"bias\",\n",
    "                                        shape=(self.units,),\n",
    "                                        initializer=tf.keras.initializers.get(self.bias_initializer),\n",
    "                                        trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        def recursiveIDFT(inputVector, B, d, level):\n",
    "            n = inputVector.shape[1]\n",
    "            n1 = n // 2\n",
    "\n",
    "            if n == 2:\n",
    "                out = tf.matmul(inputVector, B[level])\n",
    "                return out\n",
    "            else:\n",
    "                q = tf.concat([inputVector[:, ::2], inputVector[:, 1::2]], axis=1)\n",
    "\n",
    "                B1 = recursiveIDFT(q[:, :n1], B, d[n1:], level + 1)\n",
    "                B2 = recursiveIDFT(q[:, n1:], B, d[n1:], level + 1)\n",
    "\n",
    "                d_n = tf.reshape(d[:n1], (1, -1))\n",
    "                z1 = tf.concat([(B1 + tf.multiply(B2, d_n)), (B1 - tf.multiply(B2, d_n))], axis=1)\n",
    "\n",
    "                return z1 / tf.sqrt(tf.constant(2.0, dtype=tf.float32))\n",
    "\n",
    "        n = self.units\n",
    "        n1 = n // 2\n",
    "\n",
    "        q = tf.concat([inputs[:, ::2], inputs[:, 1::2]], axis=1)\n",
    "\n",
    "        B1 = recursiveIDFT(q[:, :n1], self.b_1, self.d_1, level=0)\n",
    "        B2 = recursiveIDFT(q[:, n1:], self.b_2, self.d_2, level=0)\n",
    "\n",
    "        out = tf.concat([B1, B2], axis=1)\n",
    "\n",
    "        if self.use_bias:\n",
    "            out += self.bias\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, kernel_initializer='he_normal', bias_initializer='zeros', use_bias=True, **kwargs):\n",
    "        super(SecondLayer, self).__init__(**kwargs)\n",
    "        self.units = units  # Features/neurons\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n = self.units\n",
    "        n1 = n // 2\n",
    "\n",
    "        self.d_1 = self.add_weight(name=\"kernel_d1\",\n",
    "                                   shape=(n1,),\n",
    "                                   initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                   trainable=True)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name=\"bias\",\n",
    "                                        shape=(n,),\n",
    "                                        initializer=tf.keras.initializers.get(self.bias_initializer),\n",
    "                                        trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        out1 = inputs[:, :int(inputs.shape[1] / 2)]\n",
    "        out2 = inputs[:, int(inputs.shape[1] / 2):]\n",
    "\n",
    "        z1 = tf.concat([(out1 + tf.multiply(out2, self.d_1)), (out1 - tf.multiply(out2, self.d_1))], axis=1)\n",
    "        out = z1 / tf.sqrt(tf.constant(2.0, dtype=tf.float32))\n",
    "\n",
    "        if self.use_bias:\n",
    "            out += self.bias\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, kernel_initializer='he_normal', bias_initializer='zeros', use_bias=True, **kwargs):\n",
    "        super(CustomLayer, self).__init__(**kwargs)\n",
    "        self.units = units  # features/neurons\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n = self.units\n",
    "\n",
    "        self.m = self.add_weight(name=\"kernel_m\",\n",
    "                                 shape=(n,),\n",
    "                                 initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                 trainable=True)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name=\"bias\",\n",
    "                                        shape=(self.units,),\n",
    "                                        initializer=tf.keras.initializers.get(self.bias_initializer),\n",
    "                                        trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        out = tf.multiply(inputs, self.m)\n",
    "        if self.use_bias:\n",
    "            out += self.bias\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, kernel_initializer='ones', bias_initializer='zeros', use_bias=True, **kwargs):\n",
    "        super(LinearLayer, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n = self.units\n",
    "\n",
    "        self.m = self.add_weight(name=\"kernel_m_1\",\n",
    "                                 shape=(n,),\n",
    "                                 initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                 trainable=True)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name=\"bias\",\n",
    "                                        shape=(self.units,),\n",
    "                                        initializer=tf.keras.initializers.get(self.bias_initializer),\n",
    "                                        trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        print(\"linear layer input:\", inputs.shape)\n",
    "\n",
    "        inputs1 = inputs[:, :int(inputs.shape[1] / 2)]\n",
    "        inputs2 = inputs[:, int(inputs.shape[1] / 2):]\n",
    "\n",
    "        out = tf.math.sqrt(inputs1**2 + inputs2**2)\n",
    "        out = tf.multiply(out, self.m)\n",
    "\n",
    "        if self.use_bias:\n",
    "            out += self.bias\n",
    "\n",
    "        print(\"linear layer output:\", out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import Initializer\n",
    "\n",
    "class CustomScalingInitializer(Initializer):\n",
    "    def __init__(self, n, w0, z0):\n",
    "        self.n = n\n",
    "        self.w0 = w0\n",
    "        self.z0 = z0\n",
    "\n",
    "    def __call__(self, shape, dtype=None):\n",
    "        return tf.convert_to_tensor([(self.z0 / self.w0) ** k for k in range(self.n)], dtype=dtype)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"n\": self.n, \"w0\": self.w0, \"z0\": self.z0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "linear layer input: (None, 32)\n",
      "linear layer output: (None, 16)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " real_input (InputLayer)     [(None, 16)]                 0         []                            \n",
      "                                                                                                  \n",
      " imag_input (InputLayer)     [(None, 16)]                 0         []                            \n",
      "                                                                                                  \n",
      " real_layer1 (FirstLayer)    (None, 16)                   60        ['real_input[0][0]']          \n",
      "                                                                                                  \n",
      " imag_layer1 (FirstLayer)    (None, 16)                   60        ['imag_input[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)     (None, 16)                   0         ['real_layer1[0][0]']         \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 16)                   0         ['imag_layer1[0][0]']         \n",
      "                                                                                                  \n",
      " real_support_layer_1 (Cust  (None, 16)                   32        ['leaky_re_lu[0][0]']         \n",
      " omLayer)                                                                                         \n",
      "                                                                                                  \n",
      " imag_support_layer_1 (Cust  (None, 16)                   32        ['leaky_re_lu_3[0][0]']       \n",
      " omLayer)                                                                                         \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 16)                   0         ['real_support_layer_1[0][0]']\n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 16)                   0         ['imag_support_layer_1[0][0]']\n",
      "                                                                                                  \n",
      " real_layer2 (SecondLayer)   (None, 16)                   24        ['leaky_re_lu_1[0][0]']       \n",
      "                                                                                                  \n",
      " imag_layer2 (SecondLayer)   (None, 16)                   24        ['leaky_re_lu_4[0][0]']       \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 16)                   0         ['real_layer2[0][0]']         \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 16)                   0         ['imag_layer2[0][0]']         \n",
      "                                                                                                  \n",
      " merge_real_imag (Concatena  (None, 32)                   0         ['leaky_re_lu_2[0][0]',       \n",
      " te)                                                                 'leaky_re_lu_5[0][0]']       \n",
      "                                                                                                  \n",
      " diagonal_scaling_layer (Cu  (None, 32)                   64        ['merge_real_imag[0][0]']     \n",
      " stomLayer)                                                                                       \n",
      "                                                                                                  \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 32)                   0         ['diagonal_scaling_layer[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " output_layer (LinearLayer)  (None, 16)                   32        ['leaky_re_lu_6[0][0]']       \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 16)                   0         ['output_layer[0][0]']        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 328 (1.28 KB)\n",
      "Trainable params: 328 (1.28 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Concatenate, LeakyReLU, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def cosine_loss(y_true, y_pred):\n",
    "    y_true_norm = tf.nn.l2_normalize(y_true, axis=-1)\n",
    "    y_pred_norm = tf.nn.l2_normalize(y_pred, axis=-1)\n",
    "    return 1 - tf.reduce_mean(tf.reduce_sum(y_true_norm * y_pred_norm, axis=-1))\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    return mse\n",
    "\n",
    "def hybrid_loss(y_true, y_pred):\n",
    "    mse_part = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    cos_part = cosine_loss(y_true, y_pred)\n",
    "    return 0.4* mse_part + 0.6 * cos_part\n",
    "\n",
    "\n",
    "def structured_NN(input_dim, output_dim):\n",
    "    real_input = Input(shape=(input_dim,), name=\"real_input\")\n",
    "    imag_input = Input(shape=(input_dim,), name=\"imag_input\")\n",
    "    \n",
    "    # he_normal\n",
    "    # glorot_normal\n",
    "    # glorot_uniform\n",
    "    \n",
    "    real_x = FirstLayer(units=input_dim, kernel_initializer='glorot_normal', bias_initializer='glorot_uniform', name=\"real_layer1\")(real_input)\n",
    "    real_x = LeakyReLU(alpha=0.1)(real_x)\n",
    "    real_x = CustomLayer(units=input_dim, kernel_initializer='glorot_normal', bias_initializer='glorot_uniform', name=\"real_support_layer_1\")(real_x)\n",
    "    real_x = LeakyReLU(alpha=0.1)(real_x)\n",
    "    real_x = SecondLayer(units=input_dim, kernel_initializer='glorot_normal', bias_initializer='glorot_uniform', name=\"real_layer2\")(real_x)\n",
    "    real_x = LeakyReLU(alpha=0.1)(real_x)\n",
    "\n",
    "    imag_x = FirstLayer(units=input_dim, kernel_initializer='glorot_normal', bias_initializer='glorot_uniform', name=\"imag_layer1\")(imag_input)\n",
    "    imag_x = LeakyReLU(alpha=0.1)(imag_x)\n",
    "    imag_x = CustomLayer(units=input_dim, kernel_initializer='glorot_normal', bias_initializer='glorot_uniform', name=\"imag_support_layer_1\")(imag_x)\n",
    "    imag_x = LeakyReLU(alpha=0.1)(imag_x)\n",
    "    imag_x = SecondLayer(units=input_dim, kernel_initializer='glorot_normal', bias_initializer='glorot_uniform', name=\"imag_layer2\")(imag_x)\n",
    "    imag_x = LeakyReLU(alpha=0.1)(imag_x)\n",
    "\n",
    "    merged = Concatenate(name=\"merge_real_imag\")([real_x, imag_x])\n",
    "    \n",
    "    # output = CustomLayer(units=input_dim * 2, kernel_initializer=CustomScalingInitializer(n=input_dim * 2, w0=w0, z0=z0), \n",
    "    #                      bias_initializer='zeros', name=\"diagonal_scaling_layer\")(merged)\n",
    "    output = CustomLayer(units=input_dim * 2, kernel_initializer='glorot_normal', bias_initializer='glorot_uniform', name=\"diagonal_scaling_layer\")(merged)\n",
    "    output = LeakyReLU(alpha=0.1)(output)\n",
    "    output = LinearLayer(units=output_dim, kernel_initializer='ones', bias_initializer='zeros', name=\"output_layer\")(output)\n",
    "    output = Activation('linear')(output) # sigmoid # tanh\n",
    "\n",
    "    model = Model(inputs=[real_input, imag_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss=hybrid_loss, metrics=['mse', 'mae'])\n",
    "\n",
    "    return model\n",
    "\n",
    "input_dim = X_real_train.shape[1]\n",
    "output_dim = y_train.shape[1]\n",
    "model = structured_NN(input_dim, output_dim)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real_input [(None, 16)]\n",
      "imag_input [(None, 16)]\n",
      "real_layer1 (None, 16)\n",
      "imag_layer1 (None, 16)\n",
      "leaky_re_lu (None, 16)\n",
      "leaky_re_lu_3 (None, 16)\n",
      "real_support_layer_1 (None, 16)\n",
      "imag_support_layer_1 (None, 16)\n",
      "leaky_re_lu_1 (None, 16)\n",
      "leaky_re_lu_4 (None, 16)\n",
      "real_layer2 (None, 16)\n",
      "imag_layer2 (None, 16)\n",
      "leaky_re_lu_2 (None, 16)\n",
      "leaky_re_lu_5 (None, 16)\n",
      "merge_real_imag (None, 32)\n",
      "diagonal_scaling_layer (None, 32)\n",
      "leaky_re_lu_6 (None, 32)\n",
      "output_layer (None, 16)\n",
      "activation (None, 16)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name, layer.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "adjust_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "linear layer input: (16, 32)\n",
      "linear layer output: (16, 16)\n",
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "linear layer input: (16, 32)\n",
      "linear layer output: (16, 16)\n",
      "35/50 [====================>.........] - ETA: 0s - loss: 0.2784 - mse: 0.2290 - mae: 0.3910 linear layer input: (None, 32)\n",
      "linear layer output: (None, 16)\n",
      "50/50 [==============================] - 6s 27ms/step - loss: 0.2632 - mse: 0.2213 - mae: 0.3848 - val_loss: 0.2141 - val_mse: 0.1939 - val_mae: 0.3638 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1991 - mse: 0.1893 - mae: 0.3589 - val_loss: 0.1879 - val_mse: 0.1775 - val_mae: 0.3487 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1823 - mse: 0.1749 - mae: 0.3462 - val_loss: 0.1764 - val_mse: 0.1642 - val_mae: 0.3380 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1721 - mse: 0.1615 - mae: 0.3358 - val_loss: 0.1670 - val_mse: 0.1516 - val_mae: 0.3276 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1630 - mse: 0.1490 - mae: 0.3253 - val_loss: 0.1581 - val_mse: 0.1394 - val_mae: 0.3162 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1541 - mse: 0.1373 - mae: 0.3138 - val_loss: 0.1489 - val_mse: 0.1285 - val_mae: 0.3045 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1446 - mse: 0.1261 - mae: 0.3011 - val_loss: 0.1391 - val_mse: 0.1173 - val_mae: 0.2908 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1359 - mse: 0.1157 - mae: 0.2878 - val_loss: 0.1315 - val_mse: 0.1087 - val_mae: 0.2785 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1298 - mse: 0.1085 - mae: 0.2779 - val_loss: 0.1265 - val_mse: 0.1031 - val_mae: 0.2702 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1250 - mse: 0.1032 - mae: 0.2699 - val_loss: 0.1220 - val_mse: 0.0984 - val_mae: 0.2626 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1208 - mse: 0.0989 - mae: 0.2628 - val_loss: 0.1181 - val_mse: 0.0946 - val_mae: 0.2554 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1170 - mse: 0.0952 - mae: 0.2559 - val_loss: 0.1144 - val_mse: 0.0911 - val_mae: 0.2484 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1137 - mse: 0.0920 - mae: 0.2494 - val_loss: 0.1112 - val_mse: 0.0882 - val_mae: 0.2417 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1109 - mse: 0.0894 - mae: 0.2435 - val_loss: 0.1090 - val_mse: 0.0864 - val_mae: 0.2373 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1088 - mse: 0.0877 - mae: 0.2393 - val_loss: 0.1070 - val_mse: 0.0848 - val_mae: 0.2335 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1071 - mse: 0.0862 - mae: 0.2357 - val_loss: 0.1054 - val_mse: 0.0836 - val_mae: 0.2305 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1057 - mse: 0.0851 - mae: 0.2331 - val_loss: 0.1041 - val_mse: 0.0826 - val_mae: 0.2283 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1044 - mse: 0.0841 - mae: 0.2308 - val_loss: 0.1026 - val_mse: 0.0815 - val_mae: 0.2260 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1033 - mse: 0.0832 - mae: 0.2286 - val_loss: 0.1014 - val_mse: 0.0805 - val_mae: 0.2239 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1020 - mse: 0.0822 - mae: 0.2267 - val_loss: 0.1001 - val_mse: 0.0796 - val_mae: 0.2222 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1007 - mse: 0.0812 - mae: 0.2245 - val_loss: 0.0984 - val_mse: 0.0783 - val_mae: 0.2188 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0996 - mse: 0.0803 - mae: 0.2222 - val_loss: 0.0976 - val_mse: 0.0777 - val_mae: 0.2178 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0984 - mse: 0.0794 - mae: 0.2201 - val_loss: 0.0962 - val_mse: 0.0765 - val_mae: 0.2149 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0971 - mse: 0.0784 - mae: 0.2176 - val_loss: 0.0949 - val_mse: 0.0756 - val_mae: 0.2124 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0960 - mse: 0.0776 - mae: 0.2154 - val_loss: 0.0937 - val_mse: 0.0747 - val_mae: 0.2105 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0948 - mse: 0.0767 - mae: 0.2131 - val_loss: 0.0923 - val_mse: 0.0736 - val_mae: 0.2077 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0938 - mse: 0.0759 - mae: 0.2108 - val_loss: 0.0913 - val_mse: 0.0728 - val_mae: 0.2052 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0928 - mse: 0.0751 - mae: 0.2086 - val_loss: 0.0902 - val_mse: 0.0720 - val_mae: 0.2031 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0919 - mse: 0.0745 - mae: 0.2063 - val_loss: 0.0897 - val_mse: 0.0716 - val_mae: 0.2007 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0913 - mse: 0.0739 - mae: 0.2043 - val_loss: 0.0887 - val_mse: 0.0708 - val_mae: 0.1983 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0906 - mse: 0.0734 - mae: 0.2025 - val_loss: 0.0880 - val_mse: 0.0703 - val_mae: 0.1967 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0898 - mse: 0.0729 - mae: 0.2014 - val_loss: 0.0871 - val_mse: 0.0697 - val_mae: 0.1954 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0890 - mse: 0.0723 - mae: 0.1996 - val_loss: 0.0863 - val_mse: 0.0691 - val_mae: 0.1935 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0886 - mse: 0.0720 - mae: 0.1987 - val_loss: 0.0858 - val_mse: 0.0687 - val_mae: 0.1924 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0882 - mse: 0.0717 - mae: 0.1981 - val_loss: 0.0856 - val_mse: 0.0686 - val_mae: 0.1919 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0878 - mse: 0.0714 - mae: 0.1971 - val_loss: 0.0850 - val_mse: 0.0681 - val_mae: 0.1908 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0874 - mse: 0.0711 - mae: 0.1964 - val_loss: 0.0850 - val_mse: 0.0680 - val_mae: 0.1903 - lr: 0.0010\n",
      "Epoch 38/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0871 - mse: 0.0708 - mae: 0.1957 - val_loss: 0.0844 - val_mse: 0.0676 - val_mae: 0.1894 - lr: 0.0010\n",
      "Epoch 39/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0868 - mse: 0.0706 - mae: 0.1951 - val_loss: 0.0839 - val_mse: 0.0673 - val_mae: 0.1887 - lr: 0.0010\n",
      "Epoch 40/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0865 - mse: 0.0704 - mae: 0.1944 - val_loss: 0.0835 - val_mse: 0.0670 - val_mae: 0.1878 - lr: 0.0010\n",
      "Epoch 41/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0861 - mse: 0.0701 - mae: 0.1938 - val_loss: 0.0832 - val_mse: 0.0668 - val_mae: 0.1874 - lr: 0.0010\n",
      "Epoch 42/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0857 - mse: 0.0698 - mae: 0.1931 - val_loss: 0.0832 - val_mse: 0.0668 - val_mae: 0.1873 - lr: 0.0010\n",
      "Epoch 43/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0854 - mse: 0.0696 - mae: 0.1924 - val_loss: 0.0828 - val_mse: 0.0665 - val_mae: 0.1862 - lr: 0.0010\n",
      "Epoch 44/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0852 - mse: 0.0694 - mae: 0.1920 - val_loss: 0.0826 - val_mse: 0.0663 - val_mae: 0.1860 - lr: 0.0010\n",
      "Epoch 45/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0850 - mse: 0.0693 - mae: 0.1914 - val_loss: 0.0825 - val_mse: 0.0663 - val_mae: 0.1857 - lr: 0.0010\n",
      "Epoch 46/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0847 - mse: 0.0690 - mae: 0.1911 - val_loss: 0.0823 - val_mse: 0.0661 - val_mae: 0.1854 - lr: 0.0010\n",
      "Epoch 47/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0845 - mse: 0.0689 - mae: 0.1907 - val_loss: 0.0821 - val_mse: 0.0660 - val_mae: 0.1851 - lr: 0.0010\n",
      "Epoch 48/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0843 - mse: 0.0688 - mae: 0.1903 - val_loss: 0.0817 - val_mse: 0.0657 - val_mae: 0.1842 - lr: 0.0010\n",
      "Epoch 49/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0842 - mse: 0.0686 - mae: 0.1901 - val_loss: 0.0817 - val_mse: 0.0657 - val_mae: 0.1839 - lr: 0.0010\n",
      "Epoch 50/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0840 - mse: 0.0685 - mae: 0.1895 - val_loss: 0.0815 - val_mse: 0.0655 - val_mae: 0.1837 - lr: 0.0010\n",
      "Epoch 51/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0839 - mse: 0.0684 - mae: 0.1894 - val_loss: 0.0812 - val_mse: 0.0653 - val_mae: 0.1832 - lr: 0.0010\n",
      "Epoch 52/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0837 - mse: 0.0683 - mae: 0.1891 - val_loss: 0.0812 - val_mse: 0.0653 - val_mae: 0.1832 - lr: 0.0010\n",
      "Epoch 53/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0835 - mse: 0.0681 - mae: 0.1887 - val_loss: 0.0811 - val_mse: 0.0652 - val_mae: 0.1828 - lr: 0.0010\n",
      "Epoch 54/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0834 - mse: 0.0681 - mae: 0.1886 - val_loss: 0.0810 - val_mse: 0.0651 - val_mae: 0.1827 - lr: 0.0010\n",
      "Epoch 55/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0833 - mse: 0.0680 - mae: 0.1883 - val_loss: 0.0807 - val_mse: 0.0649 - val_mae: 0.1821 - lr: 0.0010\n",
      "Epoch 56/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0832 - mse: 0.0679 - mae: 0.1880 - val_loss: 0.0805 - val_mse: 0.0648 - val_mae: 0.1818 - lr: 0.0010\n",
      "Epoch 57/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0830 - mse: 0.0678 - mae: 0.1875 - val_loss: 0.0805 - val_mse: 0.0647 - val_mae: 0.1815 - lr: 0.0010\n",
      "Epoch 58/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0829 - mse: 0.0677 - mae: 0.1874 - val_loss: 0.0804 - val_mse: 0.0647 - val_mae: 0.1816 - lr: 0.0010\n",
      "Epoch 59/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0828 - mse: 0.0676 - mae: 0.1872 - val_loss: 0.0804 - val_mse: 0.0646 - val_mae: 0.1811 - lr: 0.0010\n",
      "Epoch 60/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0827 - mse: 0.0675 - mae: 0.1870 - val_loss: 0.0801 - val_mse: 0.0645 - val_mae: 0.1810 - lr: 0.0010\n",
      "Epoch 61/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0826 - mse: 0.0674 - mae: 0.1867 - val_loss: 0.0799 - val_mse: 0.0642 - val_mae: 0.1803 - lr: 0.0010\n",
      "Epoch 62/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0825 - mse: 0.0673 - mae: 0.1864 - val_loss: 0.0802 - val_mse: 0.0645 - val_mae: 0.1810 - lr: 0.0010\n",
      "Epoch 63/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0824 - mse: 0.0673 - mae: 0.1863 - val_loss: 0.0797 - val_mse: 0.0641 - val_mae: 0.1799 - lr: 0.0010\n",
      "Epoch 64/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0824 - mse: 0.0673 - mae: 0.1860 - val_loss: 0.0801 - val_mse: 0.0644 - val_mae: 0.1806 - lr: 0.0010\n",
      "Epoch 65/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0823 - mse: 0.0672 - mae: 0.1859 - val_loss: 0.0797 - val_mse: 0.0641 - val_mae: 0.1799 - lr: 0.0010\n",
      "Epoch 66/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0822 - mse: 0.0671 - mae: 0.1856 - val_loss: 0.0794 - val_mse: 0.0639 - val_mae: 0.1793 - lr: 0.0010\n",
      "Epoch 67/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0820 - mse: 0.0670 - mae: 0.1854 - val_loss: 0.0794 - val_mse: 0.0638 - val_mae: 0.1793 - lr: 0.0010\n",
      "Epoch 68/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0819 - mse: 0.0669 - mae: 0.1851 - val_loss: 0.0795 - val_mse: 0.0639 - val_mae: 0.1794 - lr: 0.0010\n",
      "Epoch 69/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0818 - mse: 0.0668 - mae: 0.1850 - val_loss: 0.0791 - val_mse: 0.0637 - val_mae: 0.1787 - lr: 0.0010\n",
      "Epoch 70/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0818 - mse: 0.0668 - mae: 0.1848 - val_loss: 0.0792 - val_mse: 0.0637 - val_mae: 0.1787 - lr: 0.0010\n",
      "Epoch 71/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0818 - mse: 0.0667 - mae: 0.1851 - val_loss: 0.0791 - val_mse: 0.0637 - val_mae: 0.1788 - lr: 0.0010\n",
      "Epoch 72/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0816 - mse: 0.0667 - mae: 0.1847 - val_loss: 0.0788 - val_mse: 0.0634 - val_mae: 0.1781 - lr: 0.0010\n",
      "Epoch 73/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0815 - mse: 0.0665 - mae: 0.1841 - val_loss: 0.0793 - val_mse: 0.0638 - val_mae: 0.1785 - lr: 0.0010\n",
      "Epoch 74/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0814 - mse: 0.0665 - mae: 0.1841 - val_loss: 0.0786 - val_mse: 0.0633 - val_mae: 0.1776 - lr: 0.0010\n",
      "Epoch 75/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0813 - mse: 0.0664 - mae: 0.1838 - val_loss: 0.0788 - val_mse: 0.0634 - val_mae: 0.1775 - lr: 0.0010\n",
      "Epoch 76/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0813 - mse: 0.0664 - mae: 0.1839 - val_loss: 0.0787 - val_mse: 0.0633 - val_mae: 0.1777 - lr: 0.0010\n",
      "Epoch 77/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0811 - mse: 0.0662 - mae: 0.1833 - val_loss: 0.0785 - val_mse: 0.0631 - val_mae: 0.1772 - lr: 0.0010\n",
      "Epoch 78/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0810 - mse: 0.0661 - mae: 0.1832 - val_loss: 0.0782 - val_mse: 0.0629 - val_mae: 0.1767 - lr: 0.0010\n",
      "Epoch 79/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0809 - mse: 0.0661 - mae: 0.1829 - val_loss: 0.0783 - val_mse: 0.0630 - val_mae: 0.1766 - lr: 0.0010\n",
      "Epoch 80/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0807 - mse: 0.0659 - mae: 0.1826 - val_loss: 0.0783 - val_mse: 0.0630 - val_mae: 0.1770 - lr: 0.0010\n",
      "Epoch 81/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0806 - mse: 0.0658 - mae: 0.1826 - val_loss: 0.0782 - val_mse: 0.0628 - val_mae: 0.1765 - lr: 0.0010\n",
      "Epoch 82/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0805 - mse: 0.0658 - mae: 0.1825 - val_loss: 0.0781 - val_mse: 0.0628 - val_mae: 0.1765 - lr: 0.0010\n",
      "Epoch 83/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0805 - mse: 0.0658 - mae: 0.1823 - val_loss: 0.0780 - val_mse: 0.0627 - val_mae: 0.1757 - lr: 0.0010\n",
      "Epoch 84/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0803 - mse: 0.0656 - mae: 0.1821 - val_loss: 0.0778 - val_mse: 0.0626 - val_mae: 0.1762 - lr: 0.0010\n",
      "Epoch 85/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0801 - mse: 0.0654 - mae: 0.1817 - val_loss: 0.0778 - val_mse: 0.0625 - val_mae: 0.1756 - lr: 0.0010\n",
      "Epoch 86/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0798 - mse: 0.0652 - mae: 0.1811 - val_loss: 0.0777 - val_mse: 0.0624 - val_mae: 0.1754 - lr: 0.0010\n",
      "Epoch 87/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0797 - mse: 0.0651 - mae: 0.1809 - val_loss: 0.0772 - val_mse: 0.0621 - val_mae: 0.1751 - lr: 0.0010\n",
      "Epoch 88/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0795 - mse: 0.0649 - mae: 0.1807 - val_loss: 0.0770 - val_mse: 0.0619 - val_mae: 0.1746 - lr: 0.0010\n",
      "Epoch 89/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0794 - mse: 0.0648 - mae: 0.1801 - val_loss: 0.0768 - val_mse: 0.0618 - val_mae: 0.1740 - lr: 0.0010\n",
      "Epoch 90/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0793 - mse: 0.0647 - mae: 0.1801 - val_loss: 0.0768 - val_mse: 0.0618 - val_mae: 0.1742 - lr: 0.0010\n",
      "Epoch 91/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0790 - mse: 0.0645 - mae: 0.1796 - val_loss: 0.0766 - val_mse: 0.0616 - val_mae: 0.1737 - lr: 0.0010\n",
      "Epoch 92/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0788 - mse: 0.0643 - mae: 0.1793 - val_loss: 0.0764 - val_mse: 0.0615 - val_mae: 0.1736 - lr: 0.0010\n",
      "Epoch 93/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0786 - mse: 0.0642 - mae: 0.1789 - val_loss: 0.0761 - val_mse: 0.0612 - val_mae: 0.1730 - lr: 0.0010\n",
      "Epoch 94/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0783 - mse: 0.0639 - mae: 0.1786 - val_loss: 0.0758 - val_mse: 0.0610 - val_mae: 0.1725 - lr: 0.0010\n",
      "Epoch 95/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0781 - mse: 0.0637 - mae: 0.1781 - val_loss: 0.0757 - val_mse: 0.0608 - val_mae: 0.1724 - lr: 0.0010\n",
      "Epoch 96/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0778 - mse: 0.0635 - mae: 0.1778 - val_loss: 0.0752 - val_mse: 0.0605 - val_mae: 0.1717 - lr: 0.0010\n",
      "Epoch 97/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0776 - mse: 0.0633 - mae: 0.1772 - val_loss: 0.0751 - val_mse: 0.0603 - val_mae: 0.1715 - lr: 0.0010\n",
      "Epoch 98/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0773 - mse: 0.0631 - mae: 0.1768 - val_loss: 0.0747 - val_mse: 0.0600 - val_mae: 0.1711 - lr: 0.0010\n",
      "Epoch 99/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0769 - mse: 0.0628 - mae: 0.1763 - val_loss: 0.0744 - val_mse: 0.0599 - val_mae: 0.1704 - lr: 0.0010\n",
      "Epoch 100/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0767 - mse: 0.0626 - mae: 0.1759 - val_loss: 0.0742 - val_mse: 0.0596 - val_mae: 0.1704 - lr: 0.0010\n",
      "Epoch 101/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0763 - mse: 0.0622 - mae: 0.1753 - val_loss: 0.0737 - val_mse: 0.0593 - val_mae: 0.1695 - lr: 0.0010\n",
      "Epoch 102/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0758 - mse: 0.0619 - mae: 0.1746 - val_loss: 0.0733 - val_mse: 0.0590 - val_mae: 0.1686 - lr: 0.0010\n",
      "Epoch 103/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0757 - mse: 0.0617 - mae: 0.1741 - val_loss: 0.0733 - val_mse: 0.0589 - val_mae: 0.1690 - lr: 0.0010\n",
      "Epoch 104/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0752 - mse: 0.0614 - mae: 0.1738 - val_loss: 0.0727 - val_mse: 0.0584 - val_mae: 0.1674 - lr: 0.0010\n",
      "Epoch 105/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0749 - mse: 0.0611 - mae: 0.1729 - val_loss: 0.0724 - val_mse: 0.0582 - val_mae: 0.1671 - lr: 0.0010\n",
      "Epoch 106/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0745 - mse: 0.0608 - mae: 0.1722 - val_loss: 0.0721 - val_mse: 0.0580 - val_mae: 0.1666 - lr: 0.0010\n",
      "Epoch 107/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0743 - mse: 0.0606 - mae: 0.1718 - val_loss: 0.0720 - val_mse: 0.0579 - val_mae: 0.1663 - lr: 0.0010\n",
      "Epoch 108/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0741 - mse: 0.0605 - mae: 0.1715 - val_loss: 0.0716 - val_mse: 0.0575 - val_mae: 0.1657 - lr: 0.0010\n",
      "Epoch 109/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0737 - mse: 0.0601 - mae: 0.1707 - val_loss: 0.0713 - val_mse: 0.0574 - val_mae: 0.1654 - lr: 0.0010\n",
      "Epoch 110/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0734 - mse: 0.0599 - mae: 0.1706 - val_loss: 0.0711 - val_mse: 0.0572 - val_mae: 0.1653 - lr: 0.0010\n",
      "Epoch 111/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0731 - mse: 0.0597 - mae: 0.1698 - val_loss: 0.0709 - val_mse: 0.0570 - val_mae: 0.1646 - lr: 0.0010\n",
      "Epoch 112/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0730 - mse: 0.0596 - mae: 0.1702 - val_loss: 0.0708 - val_mse: 0.0569 - val_mae: 0.1645 - lr: 0.0010\n",
      "Epoch 113/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0727 - mse: 0.0593 - mae: 0.1694 - val_loss: 0.0703 - val_mse: 0.0566 - val_mae: 0.1638 - lr: 0.0010\n",
      "Epoch 114/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0726 - mse: 0.0592 - mae: 0.1692 - val_loss: 0.0703 - val_mse: 0.0565 - val_mae: 0.1636 - lr: 0.0010\n",
      "Epoch 115/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0725 - mse: 0.0592 - mae: 0.1692 - val_loss: 0.0704 - val_mse: 0.0566 - val_mae: 0.1638 - lr: 0.0010\n",
      "Epoch 116/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0722 - mse: 0.0590 - mae: 0.1688 - val_loss: 0.0704 - val_mse: 0.0565 - val_mae: 0.1636 - lr: 0.0010\n",
      "Epoch 117/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0722 - mse: 0.0589 - mae: 0.1685 - val_loss: 0.0702 - val_mse: 0.0564 - val_mae: 0.1640 - lr: 0.0010\n",
      "Epoch 118/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0721 - mse: 0.0588 - mae: 0.1688 - val_loss: 0.0705 - val_mse: 0.0567 - val_mae: 0.1652 - lr: 0.0010\n",
      "Epoch 119/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0720 - mse: 0.0588 - mae: 0.1686 - val_loss: 0.0699 - val_mse: 0.0562 - val_mae: 0.1630 - lr: 0.0010\n",
      "Epoch 120/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0718 - mse: 0.0586 - mae: 0.1682 - val_loss: 0.0702 - val_mse: 0.0564 - val_mae: 0.1636 - lr: 0.0010\n",
      "Epoch 121/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0717 - mse: 0.0585 - mae: 0.1675 - val_loss: 0.0696 - val_mse: 0.0559 - val_mae: 0.1626 - lr: 0.0010\n",
      "Epoch 122/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0715 - mse: 0.0583 - mae: 0.1675 - val_loss: 0.0698 - val_mse: 0.0560 - val_mae: 0.1626 - lr: 0.0010\n",
      "Epoch 123/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0715 - mse: 0.0584 - mae: 0.1674 - val_loss: 0.0699 - val_mse: 0.0561 - val_mae: 0.1633 - lr: 0.0010\n",
      "Epoch 124/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0714 - mse: 0.0583 - mae: 0.1674 - val_loss: 0.0692 - val_mse: 0.0556 - val_mae: 0.1620 - lr: 0.0010\n",
      "Epoch 125/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0711 - mse: 0.0581 - mae: 0.1670 - val_loss: 0.0693 - val_mse: 0.0556 - val_mae: 0.1617 - lr: 0.0010\n",
      "Epoch 126/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0710 - mse: 0.0580 - mae: 0.1666 - val_loss: 0.0693 - val_mse: 0.0556 - val_mae: 0.1613 - lr: 0.0010\n",
      "Epoch 127/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0709 - mse: 0.0579 - mae: 0.1664 - val_loss: 0.0692 - val_mse: 0.0555 - val_mae: 0.1612 - lr: 0.0010\n",
      "Epoch 128/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0708 - mse: 0.0577 - mae: 0.1660 - val_loss: 0.0688 - val_mse: 0.0554 - val_mae: 0.1613 - lr: 0.0010\n",
      "Epoch 129/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0707 - mse: 0.0578 - mae: 0.1662 - val_loss: 0.0686 - val_mse: 0.0551 - val_mae: 0.1607 - lr: 0.0010\n",
      "Epoch 130/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0706 - mse: 0.0577 - mae: 0.1660 - val_loss: 0.0685 - val_mse: 0.0550 - val_mae: 0.1605 - lr: 0.0010\n",
      "Epoch 131/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0705 - mse: 0.0575 - mae: 0.1656 - val_loss: 0.0685 - val_mse: 0.0550 - val_mae: 0.1605 - lr: 0.0010\n",
      "Epoch 132/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0703 - mse: 0.0574 - mae: 0.1653 - val_loss: 0.0685 - val_mse: 0.0551 - val_mae: 0.1605 - lr: 0.0010\n",
      "Epoch 133/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0704 - mse: 0.0575 - mae: 0.1656 - val_loss: 0.0685 - val_mse: 0.0550 - val_mae: 0.1609 - lr: 0.0010\n",
      "Epoch 134/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0702 - mse: 0.0574 - mae: 0.1654 - val_loss: 0.0681 - val_mse: 0.0547 - val_mae: 0.1597 - lr: 0.0010\n",
      "Epoch 135/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0700 - mse: 0.0572 - mae: 0.1648 - val_loss: 0.0685 - val_mse: 0.0550 - val_mae: 0.1601 - lr: 0.0010\n",
      "Epoch 136/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0700 - mse: 0.0572 - mae: 0.1646 - val_loss: 0.0682 - val_mse: 0.0548 - val_mae: 0.1601 - lr: 0.0010\n",
      "Epoch 137/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0699 - mse: 0.0571 - mae: 0.1645 - val_loss: 0.0683 - val_mse: 0.0548 - val_mae: 0.1597 - lr: 0.0010\n",
      "Epoch 138/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0698 - mse: 0.0571 - mae: 0.1645 - val_loss: 0.0680 - val_mse: 0.0546 - val_mae: 0.1592 - lr: 0.0010\n",
      "Epoch 139/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0696 - mse: 0.0569 - mae: 0.1642 - val_loss: 0.0676 - val_mse: 0.0544 - val_mae: 0.1590 - lr: 0.0010\n",
      "Epoch 140/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0696 - mse: 0.0569 - mae: 0.1638 - val_loss: 0.0678 - val_mse: 0.0544 - val_mae: 0.1591 - lr: 0.0010\n",
      "Epoch 141/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0696 - mse: 0.0569 - mae: 0.1641 - val_loss: 0.0680 - val_mse: 0.0545 - val_mae: 0.1594 - lr: 0.0010\n",
      "Epoch 142/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0695 - mse: 0.0568 - mae: 0.1641 - val_loss: 0.0679 - val_mse: 0.0546 - val_mae: 0.1591 - lr: 0.0010\n",
      "Epoch 143/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0695 - mse: 0.0568 - mae: 0.1637 - val_loss: 0.0678 - val_mse: 0.0544 - val_mae: 0.1591 - lr: 0.0010\n",
      "Epoch 144/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0692 - mse: 0.0566 - mae: 0.1633 - val_loss: 0.0672 - val_mse: 0.0541 - val_mae: 0.1586 - lr: 0.0010\n",
      "Epoch 145/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0692 - mse: 0.0566 - mae: 0.1635 - val_loss: 0.0677 - val_mse: 0.0544 - val_mae: 0.1588 - lr: 0.0010\n",
      "Epoch 146/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0691 - mse: 0.0565 - mae: 0.1630 - val_loss: 0.0674 - val_mse: 0.0542 - val_mae: 0.1583 - lr: 0.0010\n",
      "Epoch 147/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0690 - mse: 0.0564 - mae: 0.1631 - val_loss: 0.0672 - val_mse: 0.0540 - val_mae: 0.1579 - lr: 0.0010\n",
      "Epoch 148/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0688 - mse: 0.0563 - mae: 0.1627 - val_loss: 0.0671 - val_mse: 0.0540 - val_mae: 0.1581 - lr: 0.0010\n",
      "Epoch 149/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0689 - mse: 0.0563 - mae: 0.1628 - val_loss: 0.0668 - val_mse: 0.0537 - val_mae: 0.1576 - lr: 0.0010\n",
      "Epoch 150/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0691 - mse: 0.0565 - mae: 0.1630 - val_loss: 0.0672 - val_mse: 0.0539 - val_mae: 0.1584 - lr: 0.0010\n",
      "Epoch 151/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0689 - mse: 0.0563 - mae: 0.1631 - val_loss: 0.0672 - val_mse: 0.0540 - val_mae: 0.1584 - lr: 0.0010\n",
      "Epoch 152/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0687 - mse: 0.0562 - mae: 0.1626 - val_loss: 0.0668 - val_mse: 0.0537 - val_mae: 0.1575 - lr: 0.0010\n",
      "Epoch 153/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0685 - mse: 0.0561 - mae: 0.1623 - val_loss: 0.0666 - val_mse: 0.0536 - val_mae: 0.1570 - lr: 0.0010\n",
      "Epoch 154/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0685 - mse: 0.0561 - mae: 0.1621 - val_loss: 0.0668 - val_mse: 0.0537 - val_mae: 0.1567 - lr: 0.0010\n",
      "Epoch 155/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0684 - mse: 0.0560 - mae: 0.1620 - val_loss: 0.0663 - val_mse: 0.0534 - val_mae: 0.1564 - lr: 0.0010\n",
      "Epoch 156/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0683 - mse: 0.0559 - mae: 0.1620 - val_loss: 0.0665 - val_mse: 0.0535 - val_mae: 0.1566 - lr: 0.0010\n",
      "Epoch 157/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0682 - mse: 0.0558 - mae: 0.1617 - val_loss: 0.0663 - val_mse: 0.0533 - val_mae: 0.1564 - lr: 0.0010\n",
      "Epoch 158/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0681 - mse: 0.0558 - mae: 0.1614 - val_loss: 0.0659 - val_mse: 0.0531 - val_mae: 0.1559 - lr: 0.0010\n",
      "Epoch 159/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0681 - mse: 0.0557 - mae: 0.1613 - val_loss: 0.0663 - val_mse: 0.0533 - val_mae: 0.1563 - lr: 0.0010\n",
      "Epoch 160/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0685 - mse: 0.0560 - mae: 0.1622 - val_loss: 0.0667 - val_mse: 0.0536 - val_mae: 0.1572 - lr: 0.0010\n",
      "Epoch 161/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0682 - mse: 0.0558 - mae: 0.1617 - val_loss: 0.0661 - val_mse: 0.0532 - val_mae: 0.1566 - lr: 0.0010\n",
      "Epoch 162/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0680 - mse: 0.0557 - mae: 0.1615 - val_loss: 0.0664 - val_mse: 0.0534 - val_mae: 0.1564 - lr: 0.0010\n",
      "Epoch 163/500\n",
      "36/50 [====================>.........] - ETA: 0s - loss: 0.0680 - mse: 0.0556 - mae: 0.1616\n",
      "Epoch 163: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0678 - mse: 0.0555 - mae: 0.1607 - val_loss: 0.0659 - val_mse: 0.0530 - val_mae: 0.1554 - lr: 0.0010\n",
      "Epoch 164/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0677 - mse: 0.0554 - mae: 0.1606 - val_loss: 0.0658 - val_mse: 0.0530 - val_mae: 0.1557 - lr: 5.0000e-04\n",
      "Epoch 165/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0675 - mse: 0.0553 - mae: 0.1605 - val_loss: 0.0658 - val_mse: 0.0529 - val_mae: 0.1555 - lr: 5.0000e-04\n",
      "Epoch 166/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0674 - mse: 0.0552 - mae: 0.1602 - val_loss: 0.0658 - val_mse: 0.0529 - val_mae: 0.1554 - lr: 5.0000e-04\n",
      "Epoch 167/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0674 - mse: 0.0552 - mae: 0.1603 - val_loss: 0.0657 - val_mse: 0.0528 - val_mae: 0.1552 - lr: 5.0000e-04\n",
      "Epoch 168/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0673 - mse: 0.0552 - mae: 0.1600 - val_loss: 0.0658 - val_mse: 0.0529 - val_mae: 0.1555 - lr: 5.0000e-04\n",
      "Epoch 169/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0673 - mse: 0.0551 - mae: 0.1601 - val_loss: 0.0655 - val_mse: 0.0527 - val_mae: 0.1549 - lr: 5.0000e-04\n",
      "Epoch 170/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0673 - mse: 0.0551 - mae: 0.1599 - val_loss: 0.0658 - val_mse: 0.0529 - val_mae: 0.1551 - lr: 5.0000e-04\n",
      "Epoch 171/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0672 - mse: 0.0550 - mae: 0.1599 - val_loss: 0.0657 - val_mse: 0.0528 - val_mae: 0.1550 - lr: 5.0000e-04\n",
      "Epoch 172/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0672 - mse: 0.0550 - mae: 0.1598 - val_loss: 0.0656 - val_mse: 0.0528 - val_mae: 0.1547 - lr: 5.0000e-04\n",
      "Epoch 173/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0671 - mse: 0.0549 - mae: 0.1596 - val_loss: 0.0654 - val_mse: 0.0526 - val_mae: 0.1545 - lr: 5.0000e-04\n",
      "Epoch 174/500\n",
      "39/50 [======================>.......] - ETA: 0s - loss: 0.0675 - mse: 0.0550 - mae: 0.1596\n",
      "Epoch 174: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0670 - mse: 0.0549 - mae: 0.1595 - val_loss: 0.0656 - val_mse: 0.0527 - val_mae: 0.1547 - lr: 5.0000e-04\n",
      "Epoch 175/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0670 - mse: 0.0549 - mae: 0.1594 - val_loss: 0.0654 - val_mse: 0.0526 - val_mae: 0.1544 - lr: 2.5000e-04\n",
      "Epoch 176/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0669 - mse: 0.0548 - mae: 0.1593 - val_loss: 0.0654 - val_mse: 0.0526 - val_mae: 0.1545 - lr: 2.5000e-04\n",
      "Epoch 177/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0669 - mse: 0.0548 - mae: 0.1593 - val_loss: 0.0654 - val_mse: 0.0526 - val_mae: 0.1545 - lr: 2.5000e-04\n",
      "Epoch 178/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0669 - mse: 0.0548 - mae: 0.1592 - val_loss: 0.0654 - val_mse: 0.0526 - val_mae: 0.1545 - lr: 2.5000e-04\n",
      "Epoch 179/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0669 - mse: 0.0548 - mae: 0.1593 - val_loss: 0.0654 - val_mse: 0.0526 - val_mae: 0.1545 - lr: 2.5000e-04\n",
      "Epoch 180/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0669 - mse: 0.0548 - mae: 0.1593 - val_loss: 0.0652 - val_mse: 0.0525 - val_mae: 0.1542 - lr: 2.5000e-04\n",
      "Epoch 181/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0668 - mse: 0.0548 - mae: 0.1593 - val_loss: 0.0653 - val_mse: 0.0525 - val_mae: 0.1543 - lr: 2.5000e-04\n",
      "Epoch 182/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0668 - mse: 0.0547 - mae: 0.1591 - val_loss: 0.0653 - val_mse: 0.0525 - val_mae: 0.1543 - lr: 2.5000e-04\n",
      "Epoch 183/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0668 - mse: 0.0548 - mae: 0.1591 - val_loss: 0.0653 - val_mse: 0.0525 - val_mae: 0.1543 - lr: 2.5000e-04\n",
      "Epoch 184/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0668 - mse: 0.0547 - mae: 0.1591 - val_loss: 0.0653 - val_mse: 0.0526 - val_mae: 0.1544 - lr: 2.5000e-04\n",
      "Epoch 185/500\n",
      "36/50 [====================>.........] - ETA: 0s - loss: 0.0665 - mse: 0.0543 - mae: 0.1582\n",
      "Epoch 185: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0667 - mse: 0.0547 - mae: 0.1591 - val_loss: 0.0652 - val_mse: 0.0525 - val_mae: 0.1543 - lr: 2.5000e-04\n",
      "Epoch 186/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0667 - mse: 0.0547 - mae: 0.1591 - val_loss: 0.0651 - val_mse: 0.0524 - val_mae: 0.1541 - lr: 1.2500e-04\n",
      "Epoch 187/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0667 - mse: 0.0546 - mae: 0.1590 - val_loss: 0.0651 - val_mse: 0.0524 - val_mae: 0.1540 - lr: 1.2500e-04\n",
      "Epoch 188/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0667 - mse: 0.0546 - mae: 0.1589 - val_loss: 0.0652 - val_mse: 0.0525 - val_mae: 0.1542 - lr: 1.2500e-04\n",
      "Epoch 189/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0667 - mse: 0.0546 - mae: 0.1589 - val_loss: 0.0652 - val_mse: 0.0525 - val_mae: 0.1541 - lr: 1.2500e-04\n",
      "Epoch 190/500\n",
      "37/50 [=====================>........] - ETA: 0s - loss: 0.0664 - mse: 0.0545 - mae: 0.1588\n",
      "Epoch 190: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0667 - mse: 0.0546 - mae: 0.1589 - val_loss: 0.0652 - val_mse: 0.0525 - val_mae: 0.1541 - lr: 1.2500e-04\n",
      "Epoch 191/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0666 - mse: 0.0546 - mae: 0.1589 - val_loss: 0.0652 - val_mse: 0.0524 - val_mae: 0.1541 - lr: 6.2500e-05\n",
      "Epoch 192/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0666 - mse: 0.0546 - mae: 0.1589 - val_loss: 0.0652 - val_mse: 0.0524 - val_mae: 0.1541 - lr: 6.2500e-05\n",
      "Epoch 193/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0666 - mse: 0.0546 - mae: 0.1588 - val_loss: 0.0652 - val_mse: 0.0524 - val_mae: 0.1541 - lr: 6.2500e-05\n",
      "Epoch 194/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0666 - mse: 0.0546 - mae: 0.1588 - val_loss: 0.0652 - val_mse: 0.0525 - val_mae: 0.1541 - lr: 6.2500e-05\n",
      "Epoch 195/500\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0667 - mse: 0.0547 - mae: 0.1590\n",
      "Epoch 195: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0666 - mse: 0.0546 - mae: 0.1588 - val_loss: 0.0652 - val_mse: 0.0525 - val_mae: 0.1541 - lr: 6.2500e-05\n",
      "Epoch 196/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0666 - mse: 0.0546 - mae: 0.1588 - val_loss: 0.0652 - val_mse: 0.0525 - val_mae: 0.1541 - lr: 3.1250e-05\n",
      "Epoch 197/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0666 - mse: 0.0546 - mae: 0.1588 - val_loss: 0.0652 - val_mse: 0.0525 - val_mae: 0.1541 - lr: 3.1250e-05\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [X_real_train, X_imag_train],\n",
    "    y_train,\n",
    "    epochs=500,\n",
    "    batch_size=16,\n",
    "    validation_data=([X_real_test, X_imag_test], y_test),\n",
    "    callbacks=[adjust_lr, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0651 - mse: 0.0524 - mae: 0.1540\n",
      "Test MSE: 0.0524, Test MAE: 0.1540\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_mse, test_mae = model.evaluate([X_real_test, X_imag_test], y_test)\n",
    "print(f\"Test MSE: {test_mse:.4f}, Test MAE: {test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear layer input: (None, 32)\n",
      "linear layer output: (None, 16)\n",
      "7/7 [==============================] - 1s 3ms/step\n",
      "Comparison of predictions and ground truth:\n",
      "Sample 1:\n",
      "predicted as is:  [0.3081977  0.43762112 0.5222944  0.44948462 0.51243275 0.3850971\n",
      " 0.49161944 0.19546765 0.22147128 0.46213377 0.33397958 0.6160693\n",
      " 0.27570754 0.2619928  0.98199797 0.4707925 ]\n",
      "  Predicted:    [1. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 1. 1. 4. 2.]\n",
      "  Ground Truth: [1. 0. 1. 1. 1. 1. 3. 0. 1. 3. 1. 2. 1. 1. 4. 2.]\n",
      "----------------------------------------\n",
      "Sample 2:\n",
      "predicted as is:  [0.48487183 0.4757967  0.4518394  0.4962391  0.53668135 0.4186362\n",
      " 0.46533835 0.3263958  0.21120843 0.48772413 0.57240283 0.45246238\n",
      " 0.7621021  0.0230394  0.2492784  0.00881121]\n",
      "  Predicted:    [2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 3. 0. 1. 0.]\n",
      "  Ground Truth: [4. 0. 2. 0. 2. 0. 3. 2. 1. 2. 2. 2. 3. 0. 1. 0.]\n",
      "----------------------------------------\n",
      "Sample 3:\n",
      "predicted as is:  [ 0.54759455  0.5126716   0.45242238  0.4887147   0.5485109   0.52236694\n",
      "  0.6026671   0.7253652   0.7677634   0.9094783   0.15311757  0.8724402\n",
      "  1.0016887   0.77926207 -0.00620758  0.27115798]\n",
      "  Predicted:    [ 2.  2.  2.  2.  2.  2.  2.  3.  3.  4.  1.  3.  4.  3. -0.  1.]\n",
      "  Ground Truth: [3. 2. 0. 0. 2. 0. 4. 4. 3. 3. 1. 3. 4. 3. 0. 1.]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = model.predict([X_real_test, X_imag_test])\n",
    "y_pred_test_rescaled = y_pred_test * (q - 1)\n",
    "# y_pred_test_rescaled = (y_pred_test * y_std) + y_mean\n",
    "y_test_rescaled = y_test * (q - 1)\n",
    "\n",
    "print(\"Comparison of predictions and ground truth:\")\n",
    "for i in range(3):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"predicted as is: \", y_pred_test[i])\n",
    "    print(f\"  Predicted:    {np.round(y_pred_test_rescaled[i])}\")\n",
    "    print(f\"  Ground Truth: {np.round(y_test_rescaled[i])}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoded Unseen Data (Real and Imaginary Parts):\n",
      "Real Part Shape: (5, 16)\n",
      "Imaginary Part Shape: (5, 16)\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "\n",
      "Comparison of Unseen Data and Predictions:\n",
      "============================================================\n",
      "Sample 1:\n",
      "  Prediction  : [3 2 2 2 2 1 1 2 4 2 2 4 2 2 4 4]\n",
      "  Ground Truth: [2 3 2 1 0 0 0 4 4 1 2 4 2 2 4 4]\n",
      "------------------------------------------------------------\n",
      "Sample 2:\n",
      "  Prediction  : [2 2 2 2 2 2 2 3 3 1 2 1 0 3 3 0]\n",
      "  Ground Truth: [1 3 0 4 4 2 3 4 3 0 2 1 0 3 3 0]\n",
      "------------------------------------------------------------\n",
      "Sample 3:\n",
      "  Prediction  : [2 2 2 2 2 2 2 4 2 4 3 3 2 3 2 1]\n",
      "  Ground Truth: [3 1 1 3 3 1 3 4 2 4 3 3 2 3 2 1]\n",
      "------------------------------------------------------------\n",
      "Sample 4:\n",
      "  Prediction  : [2 2 2 2 1 2 3 0 3 1 0 4 0 4 1 3]\n",
      "  Ground Truth: [2 2 4 3 1 0 2 0 3 2 0 4 0 4 1 3]\n",
      "------------------------------------------------------------\n",
      "Sample 5:\n",
      "  Prediction  : [1 2 2 2 2 2 2 1 0 1 4 4 4 1 2 2]\n",
      "  Ground Truth: [0 2 3 0 2 2 2 2 0 1 4 4 4 1 2 2]\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_unseen_samples = 5\n",
    "unseen_data = np.random.randint(0, q, size=(num_unseen_samples, n))\n",
    "padded_unseen_data = np.hstack((unseen_data, np.zeros((num_unseen_samples, n_padded - n))))",
    "\n",
    "unseen_data_normalized = unseen_data.astype(np.float32) / (q - 1)\n",
    "\n",
    "# print(\"Unseen Input Data (Before Encoding):\")\n",
    "# print(unseen_data)\n",
    "\n",
    "unseen_encoded = np.array([np.dot(M_tilde, x) for x in padded_unseen_data])\n",
    "unseen_encoded[np.abs(unseen_encoded) < 1e-10] = 0\n",
    "unseen_encoded = np.round(unseen_encoded, decimals=10)\n",
    "\n",
    "X_real_unseen = np.real(unseen_encoded).astype(np.float32)\n",
    "X_imag_unseen = np.imag(unseen_encoded).astype(np.float32)\n",
    "\n",
    "print(\"\\nEncoded Unseen Data (Real and Imaginary Parts):\")\n",
    "print(\"Real Part Shape:\", X_real_unseen.shape)\n",
    "print(\"Imaginary Part Shape:\", X_imag_unseen.shape)\n",
    "\n",
    "y_pred_unseen = model.predict([X_real_unseen, X_imag_unseen])\n",
    "\n",
    "y_pred_unseen_rescaled = y_pred_unseen * (q - 1)\n",
    "\n",
    "y_pred_unseen_final = np.mod(np.round(y_pred_unseen_rescaled), q)\n",
    "\n",
    "print(\"\\nComparison of Unseen Data and Predictions:\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(num_unseen_samples):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  Prediction  : {y_pred_unseen_final[i].astype(int)}\")\n",
    "    print(f\"  Ground Truth: {unseen_data[i]}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check gradient flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real_input has no trainable variables.\n",
      "imag_input has no trainable variables.\n",
      "real_layer1 Gradient Mean: 0.31840104\n",
      "imag_layer1 Gradient Mean: 0.29495806\n",
      "leaky_re_lu has no trainable variables.\n",
      "leaky_re_lu_3 has no trainable variables.\n",
      "real_support_layer_1 Gradient Mean: 0.48576808\n",
      "imag_support_layer_1 Gradient Mean: 0.3677332\n",
      "leaky_re_lu_1 has no trainable variables.\n",
      "leaky_re_lu_4 has no trainable variables.\n",
      "real_layer2 Gradient Mean: 0.6274698\n",
      "imag_layer2 Gradient Mean: 0.5567539\n",
      "leaky_re_lu_2 has no trainable variables.\n",
      "leaky_re_lu_5 has no trainable variables.\n",
      "merge_real_imag has no trainable variables.\n",
      "diagonal_scaling_layer Gradient Mean: 0.43636677\n",
      "leaky_re_lu_6 has no trainable variables.\n",
      "output_layer Gradient Mean: 1.3726172\n",
      "activation has no trainable variables.\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    if layer.trainable_variables:\n",
    "        grads = tf.reduce_mean(tf.abs(layer.trainable_variables[0]))\n",
    "        print(layer.name, \"Gradient Mean:\", grads.numpy())\n",
    "    else:\n",
    "        print(layer.name, \"has no trainable variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear layer input: (4, 32)\n",
      "linear layer output: (4, 16)\n",
      " Gradient OK for real_layer1/kernel_b1:0, mean: 5.071519990451634e-05\n",
      " Gradient OK for real_layer1/kernel_b2:0, mean: 0.00010039917106041685\n",
      " Gradient OK for real_layer1/kernel_d1:0, mean: 0.003307051258161664\n",
      " Gradient OK for real_layer1/kernel_d2:0, mean: 5.9373942349338904e-05\n",
      " Gradient OK for real_layer1/bias:0, mean: 0.0015068321954458952\n",
      " Gradient OK for imag_layer1/kernel_b1:0, mean: 0.00029691256349906325\n",
      " Gradient OK for imag_layer1/kernel_b2:0, mean: 0.00023999929544515908\n",
      " Gradient OK for imag_layer1/kernel_d1:0, mean: 6.740874232491478e-05\n",
      " Gradient OK for imag_layer1/kernel_d2:0, mean: 6.896765989949927e-05\n",
      " Gradient OK for imag_layer1/bias:0, mean: -0.0015458695124834776\n",
      " Gradient OK for real_support_layer_1/kernel_m:0, mean: 0.006629172246903181\n",
      " Gradient OK for real_support_layer_1/bias:0, mean: 0.0055859703570604324\n",
      " Gradient OK for imag_support_layer_1/kernel_m:0, mean: 0.0184511449187994\n",
      " Gradient OK for imag_support_layer_1/bias:0, mean: 0.007196296006441116\n",
      " Gradient OK for real_layer2/kernel_d1:0, mean: 0.005360353272408247\n",
      " Gradient OK for real_layer2/bias:0, mean: 0.007381669245660305\n",
      " Gradient OK for imag_layer2/kernel_d1:0, mean: -0.0003102544869761914\n",
      " Gradient OK for imag_layer2/bias:0, mean: 0.011204415000975132\n",
      " Gradient OK for diagonal_scaling_layer/kernel_m:0, mean: -0.00978630967438221\n",
      " Gradient OK for diagonal_scaling_layer/bias:0, mean: -0.06559659540653229\n",
      " Gradient OK for output_layer/kernel_m_1:0, mean: -0.016709081828594208\n",
      " Gradient OK for output_layer/bias:0, mean: -0.1513599306344986\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4 \n",
    "X_real_dummy = np.random.rand(batch_size, X_real_train.shape[1]).astype(np.float32)\n",
    "X_imag_dummy = np.random.rand(batch_size, X_imag_train.shape[1]).astype(np.float32)\n",
    "y_dummy = np.random.rand(batch_size, y_train.shape[1]).astype(np.float32)\n",
    "\n",
    "def check_gradients(model, X_real_dummy, X_imag_dummy, y_dummy):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(model.trainable_variables)\n",
    "        predictions = model([X_real_dummy, X_imag_dummy], training=True)\n",
    "        loss = tf.keras.losses.mean_squared_error(y_dummy, predictions)\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    for i, (grad, var) in enumerate(zip(grads, model.trainable_variables)):\n",
    "        if grad is None:\n",
    "            print(f\" Gradient is None for {var.name} at index {i}\")\n",
    "        else:\n",
    "            print(f\" Gradient OK for {var.name}, mean: {tf.reduce_mean(grad).numpy()}\")\n",
    "\n",
    "check_gradients(model, X_real_dummy, X_imag_dummy, y_dummy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
