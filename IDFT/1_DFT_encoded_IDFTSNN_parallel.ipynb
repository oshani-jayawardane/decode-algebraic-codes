{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DFTSNN - DFT Encoded dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is encoded using the recursive radix-2 DFT algorithm. The imaginary and real values are parellel processed until concatenation. Results demonstrate perfect reconstruction of the original codeword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (r + 1) | (q - 1)\n",
    "# (r + 1) | n\n",
    "# q is a prime number\n",
    "# n = 2^t\n",
    "# r < n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset -f\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original n: 16\n",
      "Padded n: 16\n",
      "Generated dataset shape: (1000, 16)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n = 16\n",
    "q = 5\n",
    "num_samples = 1000\n",
    "\n",
    "def next_power_of_two(x):\n",
    "    return 1 if x == 0 else 2**(x - 1).bit_length()\n",
    "\n",
    "n_padded = next_power_of_two(n)\n",
    "\n",
    "dataset = np.random.randint(0, q, size=(num_samples, n))\n",
    "\n",
    "if n_padded > n:\n",
    "    pad_width = n_padded - n\n",
    "    dataset = np.pad(dataset, ((0, 0), (0, pad_width)), mode='constant', constant_values=0)\n",
    "\n",
    "print(\"Original n:\", n)\n",
    "print(\"Padded n:\", n_padded)\n",
    "print(\"Generated dataset shape:\", dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 4 2 ... 1 3 1]\n",
      " [3 4 0 ... 3 2 3]\n",
      " [3 0 2 ... 0 1 4]\n",
      " ...\n",
      " [4 4 4 ... 0 2 2]\n",
      " [4 2 4 ... 4 0 0]\n",
      " [2 2 2 ... 0 4 2]]\n",
      "(1000, 16)\n",
      "(16,)\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(dataset.shape)\n",
    "print(dataset[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dft(u, n):\n",
    "    n1 = n // 2\n",
    "    if n == 2:  \n",
    "        return np.array([u[0] + u[1], u[0] - u[1]])\n",
    "    \n",
    "    zeta = np.exp(-2j * np.pi / n)\n",
    "    Dn = np.diag([zeta**k for k in range(n1)])\n",
    "    Hn = np.block([[np.eye(n1), np.eye(n1)],\n",
    "                   [Dn, -Dn]])\n",
    "    \n",
    "    p = np.dot(Hn, u)\n",
    "    \n",
    "    s1 = dft(p[:n1], n1)\n",
    "    s2 = dft(p[n1:], n1)\n",
    "    \n",
    "    interleaved = np.empty((len(s1) + len(s2)), dtype=complex)\n",
    "    interleaved[0::2] = s1\n",
    "    interleaved[1::2] = s2\n",
    "    \n",
    "    y = interleaved\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42.         +0.j          0.84148733 -0.81622894j\n",
      "  -0.87867966 -6.36396103j ... -1.2304425  -1.25570089j\n",
      "  -0.87867966 +6.36396103j  0.84148733 +0.81622894j]\n",
      " [34.         +0.j          5.88076075 +0.31815162j\n",
      "  -2.41421356 +3.j         ...  6.11085797 -0.77903661j\n",
      "  -2.41421356 -3.j          5.88076075 -0.31815162j]\n",
      " [29.         +0.j          2.38895517 -4.90489287j\n",
      "   0.17157288 -1.17157288j ...  2.07192983 -7.86463619j\n",
      "   0.17157288 +1.17157288j  2.38895517 +4.90489287j]\n",
      " ...\n",
      " [37.         +0.j          6.60006572-11.28512148j\n",
      "   2.70710678 -0.70710678j ... -0.68037683 +4.15172419j\n",
      "   2.70710678 +0.70710678j  6.60006572+11.28512148j]\n",
      " [40.         +0.j         -4.28191411 +3.31849689j\n",
      "   0.70710678 -3.70710678j ...  1.90134022+10.29671013j\n",
      "   0.70710678 +3.70710678j -4.28191411 -3.31849689j]\n",
      " [35.         +0.j         -2.11652017 +0.96427923j\n",
      "   2.41421356 +0.34314575j ...  0.28130457 -2.5159377j\n",
      "   2.41421356 -0.34314575j -2.11652017 -0.96427923j]]\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset = np.array([dft(message, n_padded) for message in dataset])\n",
    "encoded_dataset[np.abs(encoded_dataset) < 1e-10] = 0\n",
    "encoded_dataset = np.round(encoded_dataset, decimals=10)\n",
    "print(encoded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 16)\n"
     ]
    }
   ],
   "source": [
    "print(encoded_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 2 4 4 1 2 2 2 4 3 2 4 1 3 1]\n",
      "[42.        +0.j          0.84148733-0.81622894j -0.87867966-6.36396103j\n",
      " -1.2304425 +1.25570089j  3.        -1.j          3.2304425 -1.57272623j\n",
      " -5.12132034-6.36396103j  1.15851267-3.64465606j  4.        +0.j\n",
      "  1.15851267+3.64465606j -5.12132034+6.36396103j  3.2304425 +1.57272623j\n",
      "  3.        +1.j         -1.2304425 -1.25570089j -0.87867966+6.36396103j\n",
      "  0.84148733+0.81622894j]\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])\n",
    "print(encoded_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to zero mean and unit variance - output results of normalizing made unseen data converge to q-1\n",
    "# encoded_dataset = (encoded_dataset - np.mean(encoded_dataset, axis=0)) / np.std(encoded_dataset, axis=0)\n",
    "# print(encoded_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train_real : (1000, 16)\n",
      "[42.          0.84148735 -0.87867963 -1.2304425   3.          3.2304425\n",
      " -5.1213202   1.1585127   4.          1.1585127  -5.1213202   3.2304425\n",
      "  3.         -1.2304425  -0.87867963  0.84148735]\n",
      "\n",
      "X_train_imag : (1000, 16)\n",
      "[ 0.        -0.8162289 -6.363961   1.255701  -1.        -1.5727262\n",
      " -6.363961  -3.644656   0.         3.644656   6.363961   1.5727262\n",
      "  1.        -1.255701   6.363961   0.8162289]\n"
     ]
    }
   ],
   "source": [
    "X_real = np.real(encoded_dataset).astype(np.float32)\n",
    "X_imag = np.imag(encoded_dataset).astype(np.float32)\n",
    "\n",
    "print(\"\\nX_train_real :\", X_real.shape)\n",
    "print(X_real[0])\n",
    "print(\"\\nX_train_imag :\", X_imag.shape)\n",
    "print(X_imag[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: \n",
      "[3. 4. 2. 4. 4. 1. 2. 2. 2. 4. 3. 2. 4. 1. 3. 1.]\n",
      "\n",
      "y_normalized: \n",
      "[0.75 1.   0.5  1.   1.   0.25 0.5  0.5  0.5  1.   0.75 0.5  1.   0.25\n",
      " 0.75 0.25]\n"
     ]
    }
   ],
   "source": [
    "# normalize target data (integers 0-q to [0, 1])\n",
    "y_normalized = dataset.astype(np.float32) / (q - 1)  # Scale to [0, 1]\n",
    "print(\"y: \")\n",
    "print(dataset.astype(np.float32)[0])\n",
    "print(\"\\ny_normalized: \")\n",
    "print(y_normalized[0])\n",
    "\n",
    "labels = y_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shapes: X_real: (800, 16) X_imag: (800, 16) y: (800, 16)\n",
      "Testing data shapes: X_real: (200, 16) X_imag: (200, 16) y: (200, 16)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_real_train, X_real_test, y_train, y_test = train_test_split(\n",
    "    X_real, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_imag_train, X_imag_test, _, _ = train_test_split(\n",
    "    X_imag, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training data shapes: X_real:\", X_real_train.shape, \"X_imag:\", X_imag_train.shape, \"y:\", y_train.shape)\n",
    "print(\"Testing data shapes: X_real:\", X_real_test.shape, \"X_imag:\", X_imag_test.shape, \"y:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "print(X_real_train.shape[1])\n",
    "print(X_real_test.shape[1])\n",
    "print(X_imag_train.shape[1])\n",
    "print(X_imag_test.shape[1])\n",
    "print(y_train.shape[1])\n",
    "print(y_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_real_train.shape[1]\n",
    "output_dim = y_train.shape[1]\n",
    "print(input_dim)\n",
    "print(output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDFT - Structure Imposed Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, kernel_initializer='he_normal', bias_initializer='zeros', use_bias=True, **kwargs):\n",
    "        super(FirstLayer, self).__init__(**kwargs)\n",
    "        self.units = units  # Features/neurons\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n = self.units\n",
    "        n1 = n // 2\n",
    "        num_blocks = n1 // 2\n",
    "\n",
    "        self.b_1 = self.add_weight(name=\"kernel_b1\",\n",
    "                                   shape=(num_blocks, 2, 2),\n",
    "                                   initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                   trainable=True)\n",
    "        self.b_2 = self.add_weight(name=\"kernel_b2\",\n",
    "                                   shape=(num_blocks, 2, 2),\n",
    "                                   initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                   trainable=True)\n",
    "        self.d_1 = self.add_weight(name=\"kernel_d1\",\n",
    "                                   shape=(n1 - 2,),\n",
    "                                   initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                   trainable=True)\n",
    "        self.d_2 = self.add_weight(name=\"kernel_d2\",\n",
    "                                   shape=(n1 - 2,),\n",
    "                                   initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                   trainable=True)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name=\"bias\",\n",
    "                                        shape=(self.units,),\n",
    "                                        initializer=tf.keras.initializers.get(self.bias_initializer),\n",
    "                                        trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        def recursiveIDFT(inputVector, B, d, level):\n",
    "            n = inputVector.shape[1]\n",
    "            n1 = n // 2\n",
    "\n",
    "            if n == 2:\n",
    "                out = tf.matmul(inputVector, B[level])\n",
    "                return out\n",
    "            else:\n",
    "                q = tf.concat([inputVector[:, ::2], inputVector[:, 1::2]], axis=1)\n",
    "\n",
    "                B1 = recursiveIDFT(q[:, :n1], B, d[n1:], level + 1)\n",
    "                B2 = recursiveIDFT(q[:, n1:], B, d[n1:], level + 1)\n",
    "\n",
    "                d_n = tf.reshape(d[:n1], (1, -1))\n",
    "                z1 = tf.concat([(B1 + tf.multiply(B2, d_n)), (B1 - tf.multiply(B2, d_n))], axis=1)\n",
    "\n",
    "                return z1 / tf.sqrt(tf.constant(2.0, dtype=tf.float32))\n",
    "\n",
    "        n = self.units\n",
    "        n1 = n // 2\n",
    "\n",
    "        q = tf.concat([inputs[:, ::2], inputs[:, 1::2]], axis=1)\n",
    "\n",
    "        B1 = recursiveIDFT(q[:, :n1], self.b_1, self.d_1, level=0)\n",
    "        B2 = recursiveIDFT(q[:, n1:], self.b_2, self.d_2, level=0)\n",
    "\n",
    "        out = tf.concat([B1, B2], axis=1)\n",
    "\n",
    "        if self.use_bias:\n",
    "            out += self.bias\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, kernel_initializer='he_normal', bias_initializer='zeros', use_bias=True, **kwargs):\n",
    "        super(SecondLayer, self).__init__(**kwargs)\n",
    "        self.units = units  # Features/neurons\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n = self.units\n",
    "        n1 = n // 2\n",
    "\n",
    "        self.d_1 = self.add_weight(name=\"kernel_d1\",\n",
    "                                   shape=(n1,),\n",
    "                                   initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                   trainable=True)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name=\"bias\",\n",
    "                                        shape=(n,),\n",
    "                                        initializer=tf.keras.initializers.get(self.bias_initializer),\n",
    "                                        trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        out1 = inputs[:, :int(inputs.shape[1] / 2)]\n",
    "        out2 = inputs[:, int(inputs.shape[1] / 2):]\n",
    "\n",
    "        z1 = tf.concat([(out1 + tf.multiply(out2, self.d_1)), (out1 - tf.multiply(out2, self.d_1))], axis=1)\n",
    "        out = z1 / tf.sqrt(tf.constant(2.0, dtype=tf.float32))\n",
    "\n",
    "        if self.use_bias:\n",
    "            out += self.bias\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, kernel_initializer='he_normal', bias_initializer='zeros', use_bias=True, **kwargs):\n",
    "        super(CustomLayer, self).__init__(**kwargs)\n",
    "        self.units = units  # features/neurons\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n = self.units\n",
    "\n",
    "        self.m = self.add_weight(name=\"kernel_m\",\n",
    "                                 shape=(n,),\n",
    "                                 initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                 trainable=True)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name=\"bias\",\n",
    "                                        shape=(self.units,),\n",
    "                                        initializer=tf.keras.initializers.get(self.bias_initializer),\n",
    "                                        trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        out = tf.multiply(inputs, self.m)\n",
    "        if self.use_bias:\n",
    "            out += self.bias\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, kernel_initializer='ones', bias_initializer='zeros', use_bias=True, **kwargs):\n",
    "        super(LinearLayer, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n = self.units\n",
    "\n",
    "        self.m = self.add_weight(name=\"kernel_m_1\",\n",
    "                                 shape=(n,),\n",
    "                                 initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                 trainable=True)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name=\"bias\",\n",
    "                                        shape=(self.units,),\n",
    "                                        initializer=tf.keras.initializers.get(self.bias_initializer),\n",
    "                                        trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        print(\"linear layer input:\", inputs.shape)\n",
    "\n",
    "        inputs1 = inputs[:, :int(inputs.shape[1] / 2)]\n",
    "        inputs2 = inputs[:, int(inputs.shape[1] / 2):]\n",
    "\n",
    "        out = tf.math.sqrt(inputs1**2 + inputs2**2)\n",
    "        out = tf.multiply(out, self.m)\n",
    "\n",
    "        if self.use_bias:\n",
    "            out += self.bias\n",
    "\n",
    "        print(\"linear layer output:\", out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "linear layer input: (None, 32)\n",
      "linear layer output: (None, 16)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " real_input (InputLayer)     [(None, 16)]                 0         []                            \n",
      "                                                                                                  \n",
      " imag_input (InputLayer)     [(None, 16)]                 0         []                            \n",
      "                                                                                                  \n",
      " real_layer1 (FirstLayer)    (None, 16)                   60        ['real_input[0][0]']          \n",
      "                                                                                                  \n",
      " imag_layer1 (FirstLayer)    (None, 16)                   60        ['imag_input[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)     (None, 16)                   0         ['real_layer1[0][0]']         \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 16)                   0         ['imag_layer1[0][0]']         \n",
      "                                                                                                  \n",
      " real_support_layer_1 (Cust  (None, 16)                   32        ['leaky_re_lu[0][0]']         \n",
      " omLayer)                                                                                         \n",
      "                                                                                                  \n",
      " imag_support_layer_1 (Cust  (None, 16)                   32        ['leaky_re_lu_3[0][0]']       \n",
      " omLayer)                                                                                         \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 16)                   0         ['real_support_layer_1[0][0]']\n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 16)                   0         ['imag_support_layer_1[0][0]']\n",
      "                                                                                                  \n",
      " real_layer2 (SecondLayer)   (None, 16)                   24        ['leaky_re_lu_1[0][0]']       \n",
      "                                                                                                  \n",
      " imag_layer2 (SecondLayer)   (None, 16)                   24        ['leaky_re_lu_4[0][0]']       \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 16)                   0         ['real_layer2[0][0]']         \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 16)                   0         ['imag_layer2[0][0]']         \n",
      "                                                                                                  \n",
      " merge_real_imag (Concatena  (None, 32)                   0         ['leaky_re_lu_2[0][0]',       \n",
      " te)                                                                 'leaky_re_lu_5[0][0]']       \n",
      "                                                                                                  \n",
      " output_layer (LinearLayer)  (None, 16)                   32        ['merge_real_imag[0][0]']     \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 16)                   0         ['output_layer[0][0]']        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 264 (1.03 KB)\n",
      "Trainable params: 264 (1.03 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Concatenate, LeakyReLU, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def cosine_loss(y_true, y_pred):\n",
    "    y_true_norm = tf.nn.l2_normalize(y_true, axis=-1)\n",
    "    y_pred_norm = tf.nn.l2_normalize(y_pred, axis=-1)\n",
    "    return 1 - tf.reduce_mean(tf.reduce_sum(y_true_norm * y_pred_norm, axis=-1))\n",
    "\n",
    "def hybrid_loss(y_true, y_pred):\n",
    "    mse_part = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    cos_part = cosine_loss(y_true, y_pred)\n",
    "    return mse_part * 0.5 + cos_part * 0.5\n",
    "\n",
    "def structured_NN(input_dim, output_dim):\n",
    "    real_input = Input(shape=(input_dim,), name=\"real_input\")\n",
    "    imag_input = Input(shape=(input_dim,), name=\"imag_input\")\n",
    "\n",
    "    real_x = FirstLayer(units=input_dim, kernel_initializer='he_normal', bias_initializer='zeros', name=\"real_layer1\")(real_input)\n",
    "    real_x = LeakyReLU(alpha=0.1)(real_x)\n",
    "    real_x = CustomLayer(units=input_dim, kernel_initializer='he_normal', bias_initializer='zeros', name=\"real_support_layer_1\")(real_x)\n",
    "    real_x = LeakyReLU(alpha=0.1)(real_x)\n",
    "    real_x = SecondLayer(units=input_dim, kernel_initializer='he_normal', bias_initializer='zeros', name=\"real_layer2\")(real_x)\n",
    "    real_x = LeakyReLU(alpha=0.1)(real_x)\n",
    "\n",
    "    imag_x = FirstLayer(units=input_dim, kernel_initializer='he_normal', bias_initializer='zeros', name=\"imag_layer1\")(imag_input)\n",
    "    imag_x = LeakyReLU(alpha=0.1)(imag_x)\n",
    "    imag_x = CustomLayer(units=input_dim, kernel_initializer='he_normal', bias_initializer='zeros', name=\"imag_support_layer_1\")(imag_x)\n",
    "    imag_x = LeakyReLU(alpha=0.1)(imag_x)\n",
    "    imag_x = SecondLayer(units=input_dim, kernel_initializer='he_normal', bias_initializer='zeros', name=\"imag_layer2\")(imag_x)\n",
    "    imag_x = LeakyReLU(alpha=0.1)(imag_x)\n",
    "\n",
    "    merged = Concatenate(name=\"merge_real_imag\")([real_x, imag_x])\n",
    "    \n",
    "    output = LinearLayer(units=output_dim, kernel_initializer='ones', bias_initializer='zeros', name=\"output_layer\")(merged)\n",
    "    output = Activation('sigmoid')(output)\n",
    "\n",
    "    model = Model(inputs=[real_input, imag_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss=hybrid_loss, metrics=['mse', 'mae'])\n",
    "\n",
    "    return model\n",
    "\n",
    "input_dim = X_real_train.shape[1]\n",
    "output_dim = y_train.shape[1]\n",
    "model = structured_NN(input_dim, output_dim)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real_input [(None, 16)]\n",
      "imag_input [(None, 16)]\n",
      "real_layer1 (None, 16)\n",
      "imag_layer1 (None, 16)\n",
      "leaky_re_lu (None, 16)\n",
      "leaky_re_lu_3 (None, 16)\n",
      "real_support_layer_1 (None, 16)\n",
      "imag_support_layer_1 (None, 16)\n",
      "leaky_re_lu_1 (None, 16)\n",
      "leaky_re_lu_4 (None, 16)\n",
      "real_layer2 (None, 16)\n",
      "imag_layer2 (None, 16)\n",
      "leaky_re_lu_2 (None, 16)\n",
      "leaky_re_lu_5 (None, 16)\n",
      "merge_real_imag (None, 32)\n",
      "output_layer (None, 16)\n",
      "activation (None, 16)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name, layer.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "adjust_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "linear layer input: (16, 32)\n",
      "linear layer output: (16, 16)\n",
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "linear layer input: (16, 32)\n",
      "linear layer output: (16, 16)\n",
      "37/50 [=====================>........] - ETA: 0s - loss: 0.1600 - mse: 0.1351 - mae: 0.3135 linear layer input: (None, 32)\n",
      "linear layer output: (None, 16)\n",
      "50/50 [==============================] - 6s 27ms/step - loss: 0.1593 - mse: 0.1340 - mae: 0.3126 - val_loss: 0.1584 - val_mse: 0.1307 - val_mae: 0.3079 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1557 - mse: 0.1297 - mae: 0.3092 - val_loss: 0.1554 - val_mse: 0.1273 - val_mae: 0.3052 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1534 - mse: 0.1272 - mae: 0.3073 - val_loss: 0.1535 - val_mse: 0.1253 - val_mae: 0.3033 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1521 - mse: 0.1258 - mae: 0.3058 - val_loss: 0.1523 - val_mse: 0.1241 - val_mae: 0.3019 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1510 - mse: 0.1247 - mae: 0.3047 - val_loss: 0.1513 - val_mse: 0.1232 - val_mae: 0.3009 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1501 - mse: 0.1240 - mae: 0.3036 - val_loss: 0.1503 - val_mse: 0.1223 - val_mae: 0.2998 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1493 - mse: 0.1232 - mae: 0.3027 - val_loss: 0.1494 - val_mse: 0.1216 - val_mae: 0.2989 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1484 - mse: 0.1225 - mae: 0.3018 - val_loss: 0.1484 - val_mse: 0.1208 - val_mae: 0.2979 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1475 - mse: 0.1218 - mae: 0.3008 - val_loss: 0.1475 - val_mse: 0.1202 - val_mae: 0.2969 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1465 - mse: 0.1212 - mae: 0.2998 - val_loss: 0.1466 - val_mse: 0.1196 - val_mae: 0.2961 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1457 - mse: 0.1205 - mae: 0.2989 - val_loss: 0.1456 - val_mse: 0.1189 - val_mae: 0.2951 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1447 - mse: 0.1198 - mae: 0.2980 - val_loss: 0.1447 - val_mse: 0.1182 - val_mae: 0.2944 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1437 - mse: 0.1190 - mae: 0.2970 - val_loss: 0.1436 - val_mse: 0.1175 - val_mae: 0.2934 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1425 - mse: 0.1181 - mae: 0.2958 - val_loss: 0.1424 - val_mse: 0.1165 - val_mae: 0.2923 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1411 - mse: 0.1171 - mae: 0.2945 - val_loss: 0.1410 - val_mse: 0.1155 - val_mae: 0.2909 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1396 - mse: 0.1159 - mae: 0.2930 - val_loss: 0.1395 - val_mse: 0.1143 - val_mae: 0.2894 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1380 - mse: 0.1146 - mae: 0.2914 - val_loss: 0.1380 - val_mse: 0.1132 - val_mae: 0.2878 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1364 - mse: 0.1135 - mae: 0.2896 - val_loss: 0.1364 - val_mse: 0.1121 - val_mae: 0.2863 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1348 - mse: 0.1123 - mae: 0.2881 - val_loss: 0.1349 - val_mse: 0.1111 - val_mae: 0.2847 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1331 - mse: 0.1110 - mae: 0.2863 - val_loss: 0.1330 - val_mse: 0.1097 - val_mae: 0.2829 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1312 - mse: 0.1096 - mae: 0.2842 - val_loss: 0.1311 - val_mse: 0.1083 - val_mae: 0.2809 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1294 - mse: 0.1082 - mae: 0.2821 - val_loss: 0.1291 - val_mse: 0.1068 - val_mae: 0.2786 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1276 - mse: 0.1068 - mae: 0.2799 - val_loss: 0.1281 - val_mse: 0.1062 - val_mae: 0.2771 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1263 - mse: 0.1058 - mae: 0.2782 - val_loss: 0.1267 - val_mse: 0.1051 - val_mae: 0.2754 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1250 - mse: 0.1047 - mae: 0.2767 - val_loss: 0.1250 - val_mse: 0.1037 - val_mae: 0.2736 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1237 - mse: 0.1037 - mae: 0.2751 - val_loss: 0.1239 - val_mse: 0.1028 - val_mae: 0.2722 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1224 - mse: 0.1025 - mae: 0.2735 - val_loss: 0.1227 - val_mse: 0.1019 - val_mae: 0.2708 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1211 - mse: 0.1014 - mae: 0.2719 - val_loss: 0.1211 - val_mse: 0.1005 - val_mae: 0.2691 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1198 - mse: 0.1003 - mae: 0.2703 - val_loss: 0.1200 - val_mse: 0.0996 - val_mae: 0.2677 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1185 - mse: 0.0991 - mae: 0.2687 - val_loss: 0.1188 - val_mse: 0.0985 - val_mae: 0.2662 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1171 - mse: 0.0980 - mae: 0.2671 - val_loss: 0.1176 - val_mse: 0.0974 - val_mae: 0.2650 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1159 - mse: 0.0969 - mae: 0.2656 - val_loss: 0.1164 - val_mse: 0.0965 - val_mae: 0.2638 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1148 - mse: 0.0959 - mae: 0.2641 - val_loss: 0.1155 - val_mse: 0.0957 - val_mae: 0.2627 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1138 - mse: 0.0951 - mae: 0.2628 - val_loss: 0.1146 - val_mse: 0.0950 - val_mae: 0.2617 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1127 - mse: 0.0941 - mae: 0.2614 - val_loss: 0.1133 - val_mse: 0.0939 - val_mae: 0.2600 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1114 - mse: 0.0931 - mae: 0.2598 - val_loss: 0.1120 - val_mse: 0.0928 - val_mae: 0.2583 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1101 - mse: 0.0919 - mae: 0.2580 - val_loss: 0.1108 - val_mse: 0.0917 - val_mae: 0.2565 - lr: 0.0010\n",
      "Epoch 38/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1086 - mse: 0.0907 - mae: 0.2561 - val_loss: 0.1094 - val_mse: 0.0905 - val_mae: 0.2546 - lr: 0.0010\n",
      "Epoch 39/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1073 - mse: 0.0896 - mae: 0.2542 - val_loss: 0.1080 - val_mse: 0.0893 - val_mae: 0.2529 - lr: 0.0010\n",
      "Epoch 40/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1061 - mse: 0.0885 - mae: 0.2526 - val_loss: 0.1068 - val_mse: 0.0884 - val_mae: 0.2512 - lr: 0.0010\n",
      "Epoch 41/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1050 - mse: 0.0876 - mae: 0.2512 - val_loss: 0.1057 - val_mse: 0.0874 - val_mae: 0.2498 - lr: 0.0010\n",
      "Epoch 42/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1040 - mse: 0.0867 - mae: 0.2499 - val_loss: 0.1047 - val_mse: 0.0866 - val_mae: 0.2485 - lr: 0.0010\n",
      "Epoch 43/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1031 - mse: 0.0860 - mae: 0.2485 - val_loss: 0.1036 - val_mse: 0.0855 - val_mae: 0.2471 - lr: 0.0010\n",
      "Epoch 44/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1021 - mse: 0.0851 - mae: 0.2472 - val_loss: 0.1026 - val_mse: 0.0846 - val_mae: 0.2459 - lr: 0.0010\n",
      "Epoch 45/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1010 - mse: 0.0841 - mae: 0.2458 - val_loss: 0.1013 - val_mse: 0.0836 - val_mae: 0.2443 - lr: 0.0010\n",
      "Epoch 46/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0997 - mse: 0.0830 - mae: 0.2443 - val_loss: 0.1000 - val_mse: 0.0824 - val_mae: 0.2427 - lr: 0.0010\n",
      "Epoch 47/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0983 - mse: 0.0819 - mae: 0.2426 - val_loss: 0.0988 - val_mse: 0.0815 - val_mae: 0.2410 - lr: 0.0010\n",
      "Epoch 48/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0971 - mse: 0.0810 - mae: 0.2411 - val_loss: 0.0976 - val_mse: 0.0805 - val_mae: 0.2395 - lr: 0.0010\n",
      "Epoch 49/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0961 - mse: 0.0801 - mae: 0.2397 - val_loss: 0.0967 - val_mse: 0.0797 - val_mae: 0.2382 - lr: 0.0010\n",
      "Epoch 50/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0953 - mse: 0.0794 - mae: 0.2385 - val_loss: 0.0959 - val_mse: 0.0790 - val_mae: 0.2372 - lr: 0.0010\n",
      "Epoch 51/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0945 - mse: 0.0787 - mae: 0.2374 - val_loss: 0.0952 - val_mse: 0.0785 - val_mae: 0.2360 - lr: 0.0010\n",
      "Epoch 52/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0938 - mse: 0.0782 - mae: 0.2364 - val_loss: 0.0945 - val_mse: 0.0779 - val_mae: 0.2351 - lr: 0.0010\n",
      "Epoch 53/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0931 - mse: 0.0777 - mae: 0.2354 - val_loss: 0.0939 - val_mse: 0.0774 - val_mae: 0.2343 - lr: 0.0010\n",
      "Epoch 54/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0926 - mse: 0.0771 - mae: 0.2345 - val_loss: 0.0935 - val_mse: 0.0771 - val_mae: 0.2335 - lr: 0.0010\n",
      "Epoch 55/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0920 - mse: 0.0767 - mae: 0.2336 - val_loss: 0.0928 - val_mse: 0.0765 - val_mae: 0.2326 - lr: 0.0010\n",
      "Epoch 56/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0913 - mse: 0.0761 - mae: 0.2327 - val_loss: 0.0922 - val_mse: 0.0761 - val_mae: 0.2319 - lr: 0.0010\n",
      "Epoch 57/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0908 - mse: 0.0757 - mae: 0.2319 - val_loss: 0.0918 - val_mse: 0.0757 - val_mae: 0.2313 - lr: 0.0010\n",
      "Epoch 58/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0903 - mse: 0.0753 - mae: 0.2311 - val_loss: 0.0912 - val_mse: 0.0752 - val_mae: 0.2307 - lr: 0.0010\n",
      "Epoch 59/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0898 - mse: 0.0748 - mae: 0.2304 - val_loss: 0.0908 - val_mse: 0.0748 - val_mae: 0.2300 - lr: 0.0010\n",
      "Epoch 60/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0893 - mse: 0.0744 - mae: 0.2296 - val_loss: 0.0902 - val_mse: 0.0743 - val_mae: 0.2292 - lr: 0.0010\n",
      "Epoch 61/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0888 - mse: 0.0740 - mae: 0.2289 - val_loss: 0.0898 - val_mse: 0.0740 - val_mae: 0.2287 - lr: 0.0010\n",
      "Epoch 62/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0883 - mse: 0.0737 - mae: 0.2281 - val_loss: 0.0894 - val_mse: 0.0737 - val_mae: 0.2280 - lr: 0.0010\n",
      "Epoch 63/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0879 - mse: 0.0733 - mae: 0.2275 - val_loss: 0.0889 - val_mse: 0.0732 - val_mae: 0.2274 - lr: 0.0010\n",
      "Epoch 64/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0874 - mse: 0.0729 - mae: 0.2268 - val_loss: 0.0885 - val_mse: 0.0729 - val_mae: 0.2268 - lr: 0.0010\n",
      "Epoch 65/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0870 - mse: 0.0725 - mae: 0.2262 - val_loss: 0.0881 - val_mse: 0.0725 - val_mae: 0.2263 - lr: 0.0010\n",
      "Epoch 66/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0866 - mse: 0.0722 - mae: 0.2256 - val_loss: 0.0877 - val_mse: 0.0723 - val_mae: 0.2256 - lr: 0.0010\n",
      "Epoch 67/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0862 - mse: 0.0719 - mae: 0.2250 - val_loss: 0.0874 - val_mse: 0.0720 - val_mae: 0.2251 - lr: 0.0010\n",
      "Epoch 68/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0858 - mse: 0.0716 - mae: 0.2245 - val_loss: 0.0867 - val_mse: 0.0715 - val_mae: 0.2243 - lr: 0.0010\n",
      "Epoch 69/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0854 - mse: 0.0713 - mae: 0.2239 - val_loss: 0.0864 - val_mse: 0.0712 - val_mae: 0.2240 - lr: 0.0010\n",
      "Epoch 70/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0851 - mse: 0.0710 - mae: 0.2234 - val_loss: 0.0862 - val_mse: 0.0710 - val_mae: 0.2236 - lr: 0.0010\n",
      "Epoch 71/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0847 - mse: 0.0707 - mae: 0.2229 - val_loss: 0.0858 - val_mse: 0.0706 - val_mae: 0.2230 - lr: 0.0010\n",
      "Epoch 72/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0844 - mse: 0.0704 - mae: 0.2224 - val_loss: 0.0855 - val_mse: 0.0704 - val_mae: 0.2226 - lr: 0.0010\n",
      "Epoch 73/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0841 - mse: 0.0701 - mae: 0.2219 - val_loss: 0.0852 - val_mse: 0.0702 - val_mae: 0.2221 - lr: 0.0010\n",
      "Epoch 74/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0838 - mse: 0.0699 - mae: 0.2214 - val_loss: 0.0847 - val_mse: 0.0697 - val_mae: 0.2216 - lr: 0.0010\n",
      "Epoch 75/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0834 - mse: 0.0696 - mae: 0.2211 - val_loss: 0.0845 - val_mse: 0.0696 - val_mae: 0.2214 - lr: 0.0010\n",
      "Epoch 76/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0831 - mse: 0.0693 - mae: 0.2206 - val_loss: 0.0843 - val_mse: 0.0694 - val_mae: 0.2209 - lr: 0.0010\n",
      "Epoch 77/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0828 - mse: 0.0691 - mae: 0.2203 - val_loss: 0.0839 - val_mse: 0.0691 - val_mae: 0.2204 - lr: 0.0010\n",
      "Epoch 78/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0825 - mse: 0.0688 - mae: 0.2198 - val_loss: 0.0837 - val_mse: 0.0689 - val_mae: 0.2203 - lr: 0.0010\n",
      "Epoch 79/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0822 - mse: 0.0686 - mae: 0.2195 - val_loss: 0.0833 - val_mse: 0.0686 - val_mae: 0.2197 - lr: 0.0010\n",
      "Epoch 80/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0819 - mse: 0.0683 - mae: 0.2191 - val_loss: 0.0830 - val_mse: 0.0684 - val_mae: 0.2193 - lr: 0.0010\n",
      "Epoch 81/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0816 - mse: 0.0681 - mae: 0.2187 - val_loss: 0.0827 - val_mse: 0.0681 - val_mae: 0.2189 - lr: 0.0010\n",
      "Epoch 82/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0814 - mse: 0.0679 - mae: 0.2183 - val_loss: 0.0823 - val_mse: 0.0678 - val_mae: 0.2185 - lr: 0.0010\n",
      "Epoch 83/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0811 - mse: 0.0676 - mae: 0.2179 - val_loss: 0.0822 - val_mse: 0.0676 - val_mae: 0.2182 - lr: 0.0010\n",
      "Epoch 84/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0807 - mse: 0.0674 - mae: 0.2176 - val_loss: 0.0819 - val_mse: 0.0674 - val_mae: 0.2178 - lr: 0.0010\n",
      "Epoch 85/500\n",
      "50/50 [==============================] - 1s 14ms/step - loss: 0.0805 - mse: 0.0672 - mae: 0.2171 - val_loss: 0.0814 - val_mse: 0.0670 - val_mae: 0.2175 - lr: 0.0010\n",
      "Epoch 86/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0801 - mse: 0.0669 - mae: 0.2168 - val_loss: 0.0809 - val_mse: 0.0666 - val_mae: 0.2168 - lr: 0.0010\n",
      "Epoch 87/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0795 - mse: 0.0664 - mae: 0.2160 - val_loss: 0.0802 - val_mse: 0.0660 - val_mae: 0.2158 - lr: 0.0010\n",
      "Epoch 88/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0785 - mse: 0.0656 - mae: 0.2147 - val_loss: 0.0791 - val_mse: 0.0652 - val_mae: 0.2141 - lr: 0.0010\n",
      "Epoch 89/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0771 - mse: 0.0645 - mae: 0.2126 - val_loss: 0.0777 - val_mse: 0.0641 - val_mae: 0.2120 - lr: 0.0010\n",
      "Epoch 90/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0759 - mse: 0.0636 - mae: 0.2107 - val_loss: 0.0768 - val_mse: 0.0635 - val_mae: 0.2109 - lr: 0.0010\n",
      "Epoch 91/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0751 - mse: 0.0630 - mae: 0.2095 - val_loss: 0.0760 - val_mse: 0.0628 - val_mae: 0.2099 - lr: 0.0010\n",
      "Epoch 92/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0745 - mse: 0.0624 - mae: 0.2086 - val_loss: 0.0759 - val_mse: 0.0627 - val_mae: 0.2095 - lr: 0.0010\n",
      "Epoch 93/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0740 - mse: 0.0621 - mae: 0.2079 - val_loss: 0.0753 - val_mse: 0.0623 - val_mae: 0.2084 - lr: 0.0010\n",
      "Epoch 94/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0736 - mse: 0.0617 - mae: 0.2070 - val_loss: 0.0745 - val_mse: 0.0616 - val_mae: 0.2075 - lr: 0.0010\n",
      "Epoch 95/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0732 - mse: 0.0614 - mae: 0.2064 - val_loss: 0.0741 - val_mse: 0.0612 - val_mae: 0.2069 - lr: 0.0010\n",
      "Epoch 96/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0728 - mse: 0.0611 - mae: 0.2060 - val_loss: 0.0739 - val_mse: 0.0611 - val_mae: 0.2064 - lr: 0.0010\n",
      "Epoch 97/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0724 - mse: 0.0607 - mae: 0.2053 - val_loss: 0.0736 - val_mse: 0.0608 - val_mae: 0.2062 - lr: 0.0010\n",
      "Epoch 98/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0721 - mse: 0.0605 - mae: 0.2050 - val_loss: 0.0730 - val_mse: 0.0604 - val_mae: 0.2053 - lr: 0.0010\n",
      "Epoch 99/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0719 - mse: 0.0603 - mae: 0.2045 - val_loss: 0.0725 - val_mse: 0.0599 - val_mae: 0.2047 - lr: 0.0010\n",
      "Epoch 100/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0716 - mse: 0.0601 - mae: 0.2040 - val_loss: 0.0723 - val_mse: 0.0598 - val_mae: 0.2043 - lr: 0.0010\n",
      "Epoch 101/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0712 - mse: 0.0598 - mae: 0.2037 - val_loss: 0.0720 - val_mse: 0.0595 - val_mae: 0.2038 - lr: 0.0010\n",
      "Epoch 102/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0710 - mse: 0.0596 - mae: 0.2032 - val_loss: 0.0721 - val_mse: 0.0596 - val_mae: 0.2038 - lr: 0.0010\n",
      "Epoch 103/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0708 - mse: 0.0594 - mae: 0.2028 - val_loss: 0.0716 - val_mse: 0.0592 - val_mae: 0.2032 - lr: 0.0010\n",
      "Epoch 104/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0704 - mse: 0.0592 - mae: 0.2024 - val_loss: 0.0712 - val_mse: 0.0589 - val_mae: 0.2024 - lr: 0.0010\n",
      "Epoch 105/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0702 - mse: 0.0590 - mae: 0.2019 - val_loss: 0.0714 - val_mse: 0.0591 - val_mae: 0.2024 - lr: 0.0010\n",
      "Epoch 106/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0699 - mse: 0.0588 - mae: 0.2015 - val_loss: 0.0706 - val_mse: 0.0584 - val_mae: 0.2015 - lr: 0.0010\n",
      "Epoch 107/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0697 - mse: 0.0586 - mae: 0.2011 - val_loss: 0.0704 - val_mse: 0.0582 - val_mae: 0.2013 - lr: 0.0010\n",
      "Epoch 108/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0695 - mse: 0.0584 - mae: 0.2008 - val_loss: 0.0702 - val_mse: 0.0580 - val_mae: 0.2008 - lr: 0.0010\n",
      "Epoch 109/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0693 - mse: 0.0582 - mae: 0.2004 - val_loss: 0.0700 - val_mse: 0.0578 - val_mae: 0.2003 - lr: 0.0010\n",
      "Epoch 110/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0691 - mse: 0.0580 - mae: 0.2000 - val_loss: 0.0698 - val_mse: 0.0577 - val_mae: 0.2001 - lr: 0.0010\n",
      "Epoch 111/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0689 - mse: 0.0579 - mae: 0.1997 - val_loss: 0.0695 - val_mse: 0.0574 - val_mae: 0.1996 - lr: 0.0010\n",
      "Epoch 112/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0686 - mse: 0.0577 - mae: 0.1994 - val_loss: 0.0692 - val_mse: 0.0571 - val_mae: 0.1992 - lr: 0.0010\n",
      "Epoch 113/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0683 - mse: 0.0575 - mae: 0.1989 - val_loss: 0.0689 - val_mse: 0.0569 - val_mae: 0.1987 - lr: 0.0010\n",
      "Epoch 114/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0681 - mse: 0.0573 - mae: 0.1983 - val_loss: 0.0688 - val_mse: 0.0569 - val_mae: 0.1986 - lr: 0.0010\n",
      "Epoch 115/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0679 - mse: 0.0571 - mae: 0.1982 - val_loss: 0.0686 - val_mse: 0.0567 - val_mae: 0.1981 - lr: 0.0010\n",
      "Epoch 116/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0677 - mse: 0.0569 - mae: 0.1977 - val_loss: 0.0683 - val_mse: 0.0564 - val_mae: 0.1976 - lr: 0.0010\n",
      "Epoch 117/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0674 - mse: 0.0567 - mae: 0.1973 - val_loss: 0.0681 - val_mse: 0.0562 - val_mae: 0.1971 - lr: 0.0010\n",
      "Epoch 118/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0673 - mse: 0.0566 - mae: 0.1970 - val_loss: 0.0678 - val_mse: 0.0560 - val_mae: 0.1966 - lr: 0.0010\n",
      "Epoch 119/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0671 - mse: 0.0564 - mae: 0.1966 - val_loss: 0.0677 - val_mse: 0.0560 - val_mae: 0.1965 - lr: 0.0010\n",
      "Epoch 120/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0669 - mse: 0.0563 - mae: 0.1964 - val_loss: 0.0674 - val_mse: 0.0557 - val_mae: 0.1960 - lr: 0.0010\n",
      "Epoch 121/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0667 - mse: 0.0561 - mae: 0.1960 - val_loss: 0.0672 - val_mse: 0.0554 - val_mae: 0.1955 - lr: 0.0010\n",
      "Epoch 122/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0665 - mse: 0.0559 - mae: 0.1956 - val_loss: 0.0672 - val_mse: 0.0555 - val_mae: 0.1957 - lr: 0.0010\n",
      "Epoch 123/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0664 - mse: 0.0559 - mae: 0.1955 - val_loss: 0.0668 - val_mse: 0.0552 - val_mae: 0.1951 - lr: 0.0010\n",
      "Epoch 124/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0662 - mse: 0.0557 - mae: 0.1952 - val_loss: 0.0668 - val_mse: 0.0552 - val_mae: 0.1949 - lr: 0.0010\n",
      "Epoch 125/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0660 - mse: 0.0555 - mae: 0.1947 - val_loss: 0.0667 - val_mse: 0.0551 - val_mae: 0.1946 - lr: 0.0010\n",
      "Epoch 126/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0659 - mse: 0.0555 - mae: 0.1947 - val_loss: 0.0663 - val_mse: 0.0548 - val_mae: 0.1942 - lr: 0.0010\n",
      "Epoch 127/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0657 - mse: 0.0553 - mae: 0.1943 - val_loss: 0.0661 - val_mse: 0.0546 - val_mae: 0.1939 - lr: 0.0010\n",
      "Epoch 128/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0655 - mse: 0.0552 - mae: 0.1940 - val_loss: 0.0661 - val_mse: 0.0546 - val_mae: 0.1935 - lr: 0.0010\n",
      "Epoch 129/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0654 - mse: 0.0551 - mae: 0.1937 - val_loss: 0.0659 - val_mse: 0.0544 - val_mae: 0.1935 - lr: 0.0010\n",
      "Epoch 130/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0652 - mse: 0.0549 - mae: 0.1936 - val_loss: 0.0658 - val_mse: 0.0544 - val_mae: 0.1931 - lr: 0.0010\n",
      "Epoch 131/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0651 - mse: 0.0548 - mae: 0.1931 - val_loss: 0.0657 - val_mse: 0.0543 - val_mae: 0.1931 - lr: 0.0010\n",
      "Epoch 132/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0649 - mse: 0.0547 - mae: 0.1929 - val_loss: 0.0653 - val_mse: 0.0539 - val_mae: 0.1923 - lr: 0.0010\n",
      "Epoch 133/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0649 - mse: 0.0546 - mae: 0.1928 - val_loss: 0.0654 - val_mse: 0.0541 - val_mae: 0.1926 - lr: 0.0010\n",
      "Epoch 134/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0647 - mse: 0.0545 - mae: 0.1926 - val_loss: 0.0652 - val_mse: 0.0539 - val_mae: 0.1920 - lr: 0.0010\n",
      "Epoch 135/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0645 - mse: 0.0544 - mae: 0.1922 - val_loss: 0.0649 - val_mse: 0.0536 - val_mae: 0.1917 - lr: 0.0010\n",
      "Epoch 136/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0644 - mse: 0.0543 - mae: 0.1921 - val_loss: 0.0651 - val_mse: 0.0538 - val_mae: 0.1918 - lr: 0.0010\n",
      "Epoch 137/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0643 - mse: 0.0542 - mae: 0.1918 - val_loss: 0.0647 - val_mse: 0.0534 - val_mae: 0.1913 - lr: 0.0010\n",
      "Epoch 138/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0641 - mse: 0.0540 - mae: 0.1915 - val_loss: 0.0648 - val_mse: 0.0535 - val_mae: 0.1913 - lr: 0.0010\n",
      "Epoch 139/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0640 - mse: 0.0539 - mae: 0.1914 - val_loss: 0.0643 - val_mse: 0.0531 - val_mae: 0.1906 - lr: 0.0010\n",
      "Epoch 140/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0639 - mse: 0.0539 - mae: 0.1911 - val_loss: 0.0644 - val_mse: 0.0532 - val_mae: 0.1905 - lr: 0.0010\n",
      "Epoch 141/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0638 - mse: 0.0537 - mae: 0.1909 - val_loss: 0.0643 - val_mse: 0.0531 - val_mae: 0.1901 - lr: 0.0010\n",
      "Epoch 142/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0636 - mse: 0.0536 - mae: 0.1905 - val_loss: 0.0640 - val_mse: 0.0529 - val_mae: 0.1899 - lr: 0.0010\n",
      "Epoch 143/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0635 - mse: 0.0536 - mae: 0.1904 - val_loss: 0.0641 - val_mse: 0.0529 - val_mae: 0.1898 - lr: 0.0010\n",
      "Epoch 144/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0634 - mse: 0.0535 - mae: 0.1902 - val_loss: 0.0637 - val_mse: 0.0526 - val_mae: 0.1896 - lr: 0.0010\n",
      "Epoch 145/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0633 - mse: 0.0533 - mae: 0.1899 - val_loss: 0.0638 - val_mse: 0.0527 - val_mae: 0.1895 - lr: 0.0010\n",
      "Epoch 146/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0631 - mse: 0.0532 - mae: 0.1897 - val_loss: 0.0638 - val_mse: 0.0527 - val_mae: 0.1894 - lr: 0.0010\n",
      "Epoch 147/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0631 - mse: 0.0531 - mae: 0.1895 - val_loss: 0.0635 - val_mse: 0.0524 - val_mae: 0.1888 - lr: 0.0010\n",
      "Epoch 148/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0630 - mse: 0.0531 - mae: 0.1894 - val_loss: 0.0634 - val_mse: 0.0523 - val_mae: 0.1888 - lr: 0.0010\n",
      "Epoch 149/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0629 - mse: 0.0530 - mae: 0.1892 - val_loss: 0.0633 - val_mse: 0.0522 - val_mae: 0.1884 - lr: 0.0010\n",
      "Epoch 150/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0627 - mse: 0.0529 - mae: 0.1887 - val_loss: 0.0630 - val_mse: 0.0521 - val_mae: 0.1883 - lr: 0.0010\n",
      "Epoch 151/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0626 - mse: 0.0528 - mae: 0.1888 - val_loss: 0.0631 - val_mse: 0.0521 - val_mae: 0.1880 - lr: 0.0010\n",
      "Epoch 152/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0624 - mse: 0.0526 - mae: 0.1884 - val_loss: 0.0634 - val_mse: 0.0524 - val_mae: 0.1882 - lr: 0.0010\n",
      "Epoch 153/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0624 - mse: 0.0526 - mae: 0.1882 - val_loss: 0.0628 - val_mse: 0.0519 - val_mae: 0.1877 - lr: 0.0010\n",
      "Epoch 154/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0622 - mse: 0.0525 - mae: 0.1881 - val_loss: 0.0629 - val_mse: 0.0519 - val_mae: 0.1874 - lr: 0.0010\n",
      "Epoch 155/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0622 - mse: 0.0524 - mae: 0.1879 - val_loss: 0.0627 - val_mse: 0.0518 - val_mae: 0.1874 - lr: 0.0010\n",
      "Epoch 156/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0620 - mse: 0.0523 - mae: 0.1875 - val_loss: 0.0626 - val_mse: 0.0517 - val_mae: 0.1872 - lr: 0.0010\n",
      "Epoch 157/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0619 - mse: 0.0522 - mae: 0.1874 - val_loss: 0.0624 - val_mse: 0.0516 - val_mae: 0.1868 - lr: 0.0010\n",
      "Epoch 158/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0618 - mse: 0.0521 - mae: 0.1872 - val_loss: 0.0621 - val_mse: 0.0513 - val_mae: 0.1864 - lr: 0.0010\n",
      "Epoch 159/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0617 - mse: 0.0520 - mae: 0.1869 - val_loss: 0.0620 - val_mse: 0.0512 - val_mae: 0.1862 - lr: 0.0010\n",
      "Epoch 160/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0616 - mse: 0.0520 - mae: 0.1868 - val_loss: 0.0621 - val_mse: 0.0513 - val_mae: 0.1861 - lr: 0.0010\n",
      "Epoch 161/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0614 - mse: 0.0518 - mae: 0.1864 - val_loss: 0.0624 - val_mse: 0.0516 - val_mae: 0.1864 - lr: 0.0010\n",
      "Epoch 162/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0613 - mse: 0.0518 - mae: 0.1863 - val_loss: 0.0616 - val_mse: 0.0509 - val_mae: 0.1854 - lr: 0.0010\n",
      "Epoch 163/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0612 - mse: 0.0517 - mae: 0.1861 - val_loss: 0.0616 - val_mse: 0.0509 - val_mae: 0.1855 - lr: 0.0010\n",
      "Epoch 164/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0611 - mse: 0.0516 - mae: 0.1859 - val_loss: 0.0618 - val_mse: 0.0511 - val_mae: 0.1852 - lr: 0.0010\n",
      "Epoch 165/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0610 - mse: 0.0515 - mae: 0.1856 - val_loss: 0.0614 - val_mse: 0.0508 - val_mae: 0.1849 - lr: 0.0010\n",
      "Epoch 166/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0608 - mse: 0.0514 - mae: 0.1855 - val_loss: 0.0611 - val_mse: 0.0505 - val_mae: 0.1842 - lr: 0.0010\n",
      "Epoch 167/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0607 - mse: 0.0512 - mae: 0.1850 - val_loss: 0.0616 - val_mse: 0.0510 - val_mae: 0.1848 - lr: 0.0010\n",
      "Epoch 168/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0606 - mse: 0.0511 - mae: 0.1849 - val_loss: 0.0610 - val_mse: 0.0504 - val_mae: 0.1841 - lr: 0.0010\n",
      "Epoch 169/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0605 - mse: 0.0511 - mae: 0.1847 - val_loss: 0.0607 - val_mse: 0.0502 - val_mae: 0.1838 - lr: 0.0010\n",
      "Epoch 170/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0604 - mse: 0.0510 - mae: 0.1845 - val_loss: 0.0612 - val_mse: 0.0505 - val_mae: 0.1838 - lr: 0.0010\n",
      "Epoch 171/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0602 - mse: 0.0509 - mae: 0.1843 - val_loss: 0.0609 - val_mse: 0.0503 - val_mae: 0.1837 - lr: 0.0010\n",
      "Epoch 172/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0601 - mse: 0.0507 - mae: 0.1840 - val_loss: 0.0610 - val_mse: 0.0505 - val_mae: 0.1837 - lr: 0.0010\n",
      "Epoch 173/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0600 - mse: 0.0507 - mae: 0.1838 - val_loss: 0.0604 - val_mse: 0.0499 - val_mae: 0.1830 - lr: 0.0010\n",
      "Epoch 174/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0599 - mse: 0.0506 - mae: 0.1837 - val_loss: 0.0601 - val_mse: 0.0497 - val_mae: 0.1825 - lr: 0.0010\n",
      "Epoch 175/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0598 - mse: 0.0505 - mae: 0.1834 - val_loss: 0.0601 - val_mse: 0.0496 - val_mae: 0.1825 - lr: 0.0010\n",
      "Epoch 176/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0596 - mse: 0.0504 - mae: 0.1831 - val_loss: 0.0603 - val_mse: 0.0498 - val_mae: 0.1821 - lr: 0.0010\n",
      "Epoch 177/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0594 - mse: 0.0502 - mae: 0.1827 - val_loss: 0.0603 - val_mse: 0.0498 - val_mae: 0.1824 - lr: 0.0010\n",
      "Epoch 178/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0593 - mse: 0.0501 - mae: 0.1826 - val_loss: 0.0605 - val_mse: 0.0501 - val_mae: 0.1822 - lr: 0.0010\n",
      "Epoch 179/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0592 - mse: 0.0500 - mae: 0.1823 - val_loss: 0.0594 - val_mse: 0.0491 - val_mae: 0.1815 - lr: 0.0010\n",
      "Epoch 180/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0591 - mse: 0.0499 - mae: 0.1821 - val_loss: 0.0595 - val_mse: 0.0492 - val_mae: 0.1811 - lr: 0.0010\n",
      "Epoch 181/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0589 - mse: 0.0498 - mae: 0.1818 - val_loss: 0.0599 - val_mse: 0.0496 - val_mae: 0.1814 - lr: 0.0010\n",
      "Epoch 182/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0588 - mse: 0.0497 - mae: 0.1817 - val_loss: 0.0592 - val_mse: 0.0489 - val_mae: 0.1808 - lr: 0.0010\n",
      "Epoch 183/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0587 - mse: 0.0496 - mae: 0.1812 - val_loss: 0.0589 - val_mse: 0.0487 - val_mae: 0.1804 - lr: 0.0010\n",
      "Epoch 184/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0585 - mse: 0.0495 - mae: 0.1813 - val_loss: 0.0588 - val_mse: 0.0486 - val_mae: 0.1801 - lr: 0.0010\n",
      "Epoch 185/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0584 - mse: 0.0494 - mae: 0.1808 - val_loss: 0.0594 - val_mse: 0.0491 - val_mae: 0.1804 - lr: 0.0010\n",
      "Epoch 186/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0582 - mse: 0.0492 - mae: 0.1806 - val_loss: 0.0589 - val_mse: 0.0487 - val_mae: 0.1795 - lr: 0.0010\n",
      "Epoch 187/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0581 - mse: 0.0491 - mae: 0.1803 - val_loss: 0.0593 - val_mse: 0.0491 - val_mae: 0.1799 - lr: 0.0010\n",
      "Epoch 188/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0580 - mse: 0.0491 - mae: 0.1801 - val_loss: 0.0582 - val_mse: 0.0482 - val_mae: 0.1793 - lr: 0.0010\n",
      "Epoch 189/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0579 - mse: 0.0490 - mae: 0.1799 - val_loss: 0.0580 - val_mse: 0.0480 - val_mae: 0.1787 - lr: 0.0010\n",
      "Epoch 190/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0578 - mse: 0.0489 - mae: 0.1797 - val_loss: 0.0579 - val_mse: 0.0479 - val_mae: 0.1786 - lr: 0.0010\n",
      "Epoch 191/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0576 - mse: 0.0488 - mae: 0.1795 - val_loss: 0.0583 - val_mse: 0.0483 - val_mae: 0.1787 - lr: 0.0010\n",
      "Epoch 192/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0574 - mse: 0.0486 - mae: 0.1790 - val_loss: 0.0585 - val_mse: 0.0484 - val_mae: 0.1785 - lr: 0.0010\n",
      "Epoch 193/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0573 - mse: 0.0485 - mae: 0.1789 - val_loss: 0.0575 - val_mse: 0.0476 - val_mae: 0.1779 - lr: 0.0010\n",
      "Epoch 194/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0572 - mse: 0.0484 - mae: 0.1786 - val_loss: 0.0573 - val_mse: 0.0474 - val_mae: 0.1774 - lr: 0.0010\n",
      "Epoch 195/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0571 - mse: 0.0483 - mae: 0.1785 - val_loss: 0.0571 - val_mse: 0.0473 - val_mae: 0.1771 - lr: 0.0010\n",
      "Epoch 196/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0569 - mse: 0.0482 - mae: 0.1781 - val_loss: 0.0580 - val_mse: 0.0480 - val_mae: 0.1778 - lr: 0.0010\n",
      "Epoch 197/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0567 - mse: 0.0480 - mae: 0.1778 - val_loss: 0.0569 - val_mse: 0.0471 - val_mae: 0.1766 - lr: 0.0010\n",
      "Epoch 198/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0566 - mse: 0.0479 - mae: 0.1776 - val_loss: 0.0577 - val_mse: 0.0478 - val_mae: 0.1770 - lr: 0.0010\n",
      "Epoch 199/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0565 - mse: 0.0478 - mae: 0.1774 - val_loss: 0.0571 - val_mse: 0.0473 - val_mae: 0.1765 - lr: 0.0010\n",
      "Epoch 200/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0563 - mse: 0.0477 - mae: 0.1771 - val_loss: 0.0576 - val_mse: 0.0477 - val_mae: 0.1766 - lr: 0.0010\n",
      "Epoch 201/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0562 - mse: 0.0477 - mae: 0.1769 - val_loss: 0.0562 - val_mse: 0.0465 - val_mae: 0.1753 - lr: 0.0010\n",
      "Epoch 202/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0561 - mse: 0.0475 - mae: 0.1767 - val_loss: 0.0561 - val_mse: 0.0464 - val_mae: 0.1752 - lr: 0.0010\n",
      "Epoch 203/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0560 - mse: 0.0474 - mae: 0.1764 - val_loss: 0.0560 - val_mse: 0.0464 - val_mae: 0.1750 - lr: 0.0010\n",
      "Epoch 204/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0558 - mse: 0.0472 - mae: 0.1762 - val_loss: 0.0558 - val_mse: 0.0462 - val_mae: 0.1746 - lr: 0.0010\n",
      "Epoch 205/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0556 - mse: 0.0471 - mae: 0.1758 - val_loss: 0.0559 - val_mse: 0.0463 - val_mae: 0.1746 - lr: 0.0010\n",
      "Epoch 206/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0554 - mse: 0.0470 - mae: 0.1756 - val_loss: 0.0567 - val_mse: 0.0470 - val_mae: 0.1748 - lr: 0.0010\n",
      "Epoch 207/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0553 - mse: 0.0469 - mae: 0.1752 - val_loss: 0.0555 - val_mse: 0.0460 - val_mae: 0.1741 - lr: 0.0010\n",
      "Epoch 208/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0552 - mse: 0.0468 - mae: 0.1750 - val_loss: 0.0562 - val_mse: 0.0466 - val_mae: 0.1744 - lr: 0.0010\n",
      "Epoch 209/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0550 - mse: 0.0466 - mae: 0.1746 - val_loss: 0.0563 - val_mse: 0.0468 - val_mae: 0.1741 - lr: 0.0010\n",
      "Epoch 210/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0550 - mse: 0.0466 - mae: 0.1746 - val_loss: 0.0550 - val_mse: 0.0456 - val_mae: 0.1733 - lr: 0.0010\n",
      "Epoch 211/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0549 - mse: 0.0465 - mae: 0.1744 - val_loss: 0.0548 - val_mse: 0.0454 - val_mae: 0.1729 - lr: 0.0010\n",
      "Epoch 212/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0547 - mse: 0.0464 - mae: 0.1741 - val_loss: 0.0547 - val_mse: 0.0454 - val_mae: 0.1727 - lr: 0.0010\n",
      "Epoch 213/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0545 - mse: 0.0462 - mae: 0.1736 - val_loss: 0.0545 - val_mse: 0.0452 - val_mae: 0.1724 - lr: 0.0010\n",
      "Epoch 214/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0544 - mse: 0.0461 - mae: 0.1735 - val_loss: 0.0544 - val_mse: 0.0451 - val_mae: 0.1719 - lr: 0.0010\n",
      "Epoch 215/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0543 - mse: 0.0460 - mae: 0.1731 - val_loss: 0.0545 - val_mse: 0.0451 - val_mae: 0.1719 - lr: 0.0010\n",
      "Epoch 216/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0541 - mse: 0.0459 - mae: 0.1729 - val_loss: 0.0552 - val_mse: 0.0458 - val_mae: 0.1722 - lr: 0.0010\n",
      "Epoch 217/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0540 - mse: 0.0458 - mae: 0.1726 - val_loss: 0.0552 - val_mse: 0.0459 - val_mae: 0.1719 - lr: 0.0010\n",
      "Epoch 218/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0539 - mse: 0.0457 - mae: 0.1723 - val_loss: 0.0540 - val_mse: 0.0447 - val_mae: 0.1710 - lr: 0.0010\n",
      "Epoch 219/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0538 - mse: 0.0456 - mae: 0.1721 - val_loss: 0.0538 - val_mse: 0.0446 - val_mae: 0.1708 - lr: 0.0010\n",
      "Epoch 220/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0536 - mse: 0.0455 - mae: 0.1718 - val_loss: 0.0536 - val_mse: 0.0444 - val_mae: 0.1701 - lr: 0.0010\n",
      "Epoch 221/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0535 - mse: 0.0453 - mae: 0.1715 - val_loss: 0.0535 - val_mse: 0.0443 - val_mae: 0.1699 - lr: 0.0010\n",
      "Epoch 222/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0534 - mse: 0.0453 - mae: 0.1713 - val_loss: 0.0533 - val_mse: 0.0442 - val_mae: 0.1695 - lr: 0.0010\n",
      "Epoch 223/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0533 - mse: 0.0451 - mae: 0.1709 - val_loss: 0.0532 - val_mse: 0.0441 - val_mae: 0.1692 - lr: 0.0010\n",
      "Epoch 224/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0531 - mse: 0.0450 - mae: 0.1706 - val_loss: 0.0538 - val_mse: 0.0446 - val_mae: 0.1696 - lr: 0.0010\n",
      "Epoch 225/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0530 - mse: 0.0450 - mae: 0.1704 - val_loss: 0.0543 - val_mse: 0.0451 - val_mae: 0.1700 - lr: 0.0010\n",
      "Epoch 226/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0528 - mse: 0.0448 - mae: 0.1700 - val_loss: 0.0541 - val_mse: 0.0449 - val_mae: 0.1694 - lr: 0.0010\n",
      "Epoch 227/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0530 - mse: 0.0449 - mae: 0.1700 - val_loss: 0.0527 - val_mse: 0.0437 - val_mae: 0.1683 - lr: 0.0010\n",
      "Epoch 228/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0527 - mse: 0.0447 - mae: 0.1696 - val_loss: 0.0525 - val_mse: 0.0435 - val_mae: 0.1677 - lr: 0.0010\n",
      "Epoch 229/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0525 - mse: 0.0445 - mae: 0.1693 - val_loss: 0.0531 - val_mse: 0.0440 - val_mae: 0.1682 - lr: 0.0010\n",
      "Epoch 230/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0524 - mse: 0.0444 - mae: 0.1690 - val_loss: 0.0536 - val_mse: 0.0445 - val_mae: 0.1684 - lr: 0.0010\n",
      "Epoch 231/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0523 - mse: 0.0443 - mae: 0.1687 - val_loss: 0.0521 - val_mse: 0.0432 - val_mae: 0.1669 - lr: 0.0010\n",
      "Epoch 232/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0522 - mse: 0.0443 - mae: 0.1685 - val_loss: 0.0528 - val_mse: 0.0437 - val_mae: 0.1673 - lr: 0.0010\n",
      "Epoch 233/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0521 - mse: 0.0441 - mae: 0.1681 - val_loss: 0.0533 - val_mse: 0.0443 - val_mae: 0.1674 - lr: 0.0010\n",
      "Epoch 234/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0520 - mse: 0.0441 - mae: 0.1679 - val_loss: 0.0519 - val_mse: 0.0431 - val_mae: 0.1665 - lr: 0.0010\n",
      "Epoch 235/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0519 - mse: 0.0440 - mae: 0.1677 - val_loss: 0.0517 - val_mse: 0.0429 - val_mae: 0.1660 - lr: 0.0010\n",
      "Epoch 236/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0517 - mse: 0.0439 - mae: 0.1674 - val_loss: 0.0529 - val_mse: 0.0439 - val_mae: 0.1669 - lr: 0.0010\n",
      "Epoch 237/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0516 - mse: 0.0438 - mae: 0.1671 - val_loss: 0.0516 - val_mse: 0.0428 - val_mae: 0.1657 - lr: 0.0010\n",
      "Epoch 238/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0516 - mse: 0.0437 - mae: 0.1669 - val_loss: 0.0514 - val_mse: 0.0427 - val_mae: 0.1655 - lr: 0.0010\n",
      "Epoch 239/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0515 - mse: 0.0437 - mae: 0.1667 - val_loss: 0.0513 - val_mse: 0.0425 - val_mae: 0.1652 - lr: 0.0010\n",
      "Epoch 240/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0513 - mse: 0.0435 - mae: 0.1664 - val_loss: 0.0511 - val_mse: 0.0424 - val_mae: 0.1647 - lr: 0.0010\n",
      "Epoch 241/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0512 - mse: 0.0434 - mae: 0.1661 - val_loss: 0.0517 - val_mse: 0.0429 - val_mae: 0.1652 - lr: 0.0010\n",
      "Epoch 242/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0511 - mse: 0.0434 - mae: 0.1659 - val_loss: 0.0522 - val_mse: 0.0434 - val_mae: 0.1652 - lr: 0.0010\n",
      "Epoch 243/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0510 - mse: 0.0433 - mae: 0.1656 - val_loss: 0.0521 - val_mse: 0.0432 - val_mae: 0.1649 - lr: 0.0010\n",
      "Epoch 244/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0508 - mse: 0.0431 - mae: 0.1652 - val_loss: 0.0520 - val_mse: 0.0432 - val_mae: 0.1647 - lr: 0.0010\n",
      "Epoch 245/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0507 - mse: 0.0430 - mae: 0.1650 - val_loss: 0.0505 - val_mse: 0.0419 - val_mae: 0.1634 - lr: 0.0010\n",
      "Epoch 246/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0505 - mse: 0.0429 - mae: 0.1647 - val_loss: 0.0504 - val_mse: 0.0418 - val_mae: 0.1631 - lr: 0.0010\n",
      "Epoch 247/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0503 - mse: 0.0427 - mae: 0.1644 - val_loss: 0.0502 - val_mse: 0.0417 - val_mae: 0.1630 - lr: 0.0010\n",
      "Epoch 248/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0502 - mse: 0.0426 - mae: 0.1641 - val_loss: 0.0501 - val_mse: 0.0416 - val_mae: 0.1625 - lr: 0.0010\n",
      "Epoch 249/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0500 - mse: 0.0424 - mae: 0.1637 - val_loss: 0.0498 - val_mse: 0.0413 - val_mae: 0.1619 - lr: 0.0010\n",
      "Epoch 250/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0498 - mse: 0.0423 - mae: 0.1635 - val_loss: 0.0495 - val_mse: 0.0411 - val_mae: 0.1615 - lr: 0.0010\n",
      "Epoch 251/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0496 - mse: 0.0421 - mae: 0.1629 - val_loss: 0.0494 - val_mse: 0.0410 - val_mae: 0.1613 - lr: 0.0010\n",
      "Epoch 252/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0494 - mse: 0.0420 - mae: 0.1627 - val_loss: 0.0494 - val_mse: 0.0410 - val_mae: 0.1612 - lr: 0.0010\n",
      "Epoch 253/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0492 - mse: 0.0418 - mae: 0.1623 - val_loss: 0.0503 - val_mse: 0.0418 - val_mae: 0.1614 - lr: 0.0010\n",
      "Epoch 254/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0492 - mse: 0.0418 - mae: 0.1620 - val_loss: 0.0489 - val_mse: 0.0406 - val_mae: 0.1603 - lr: 0.0010\n",
      "Epoch 255/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0490 - mse: 0.0416 - mae: 0.1619 - val_loss: 0.0488 - val_mse: 0.0405 - val_mae: 0.1601 - lr: 0.0010\n",
      "Epoch 256/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0489 - mse: 0.0415 - mae: 0.1616 - val_loss: 0.0486 - val_mse: 0.0404 - val_mae: 0.1598 - lr: 0.0010\n",
      "Epoch 257/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0487 - mse: 0.0414 - mae: 0.1614 - val_loss: 0.0484 - val_mse: 0.0403 - val_mae: 0.1596 - lr: 0.0010\n",
      "Epoch 258/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0485 - mse: 0.0413 - mae: 0.1611 - val_loss: 0.0482 - val_mse: 0.0401 - val_mae: 0.1590 - lr: 0.0010\n",
      "Epoch 259/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0484 - mse: 0.0412 - mae: 0.1608 - val_loss: 0.0481 - val_mse: 0.0400 - val_mae: 0.1588 - lr: 0.0010\n",
      "Epoch 260/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0483 - mse: 0.0410 - mae: 0.1604 - val_loss: 0.0480 - val_mse: 0.0399 - val_mae: 0.1587 - lr: 0.0010\n",
      "Epoch 261/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0481 - mse: 0.0410 - mae: 0.1602 - val_loss: 0.0478 - val_mse: 0.0398 - val_mae: 0.1582 - lr: 0.0010\n",
      "Epoch 262/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0480 - mse: 0.0408 - mae: 0.1599 - val_loss: 0.0476 - val_mse: 0.0396 - val_mae: 0.1580 - lr: 0.0010\n",
      "Epoch 263/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0478 - mse: 0.0407 - mae: 0.1596 - val_loss: 0.0476 - val_mse: 0.0395 - val_mae: 0.1577 - lr: 0.0010\n",
      "Epoch 264/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0476 - mse: 0.0405 - mae: 0.1592 - val_loss: 0.0474 - val_mse: 0.0394 - val_mae: 0.1573 - lr: 0.0010\n",
      "Epoch 265/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0475 - mse: 0.0404 - mae: 0.1589 - val_loss: 0.0471 - val_mse: 0.0392 - val_mae: 0.1568 - lr: 0.0010\n",
      "Epoch 266/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0473 - mse: 0.0403 - mae: 0.1586 - val_loss: 0.0471 - val_mse: 0.0392 - val_mae: 0.1567 - lr: 0.0010\n",
      "Epoch 267/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0472 - mse: 0.0402 - mae: 0.1583 - val_loss: 0.0470 - val_mse: 0.0390 - val_mae: 0.1562 - lr: 0.0010\n",
      "Epoch 268/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0471 - mse: 0.0401 - mae: 0.1580 - val_loss: 0.0468 - val_mse: 0.0389 - val_mae: 0.1560 - lr: 0.0010\n",
      "Epoch 269/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0469 - mse: 0.0400 - mae: 0.1577 - val_loss: 0.0466 - val_mse: 0.0388 - val_mae: 0.1555 - lr: 0.0010\n",
      "Epoch 270/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0468 - mse: 0.0399 - mae: 0.1573 - val_loss: 0.0471 - val_mse: 0.0392 - val_mae: 0.1558 - lr: 0.0010\n",
      "Epoch 271/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0466 - mse: 0.0397 - mae: 0.1570 - val_loss: 0.0472 - val_mse: 0.0393 - val_mae: 0.1558 - lr: 0.0010\n",
      "Epoch 272/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0465 - mse: 0.0396 - mae: 0.1568 - val_loss: 0.0463 - val_mse: 0.0385 - val_mae: 0.1547 - lr: 0.0010\n",
      "Epoch 273/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0465 - mse: 0.0396 - mae: 0.1566 - val_loss: 0.0461 - val_mse: 0.0384 - val_mae: 0.1545 - lr: 0.0010\n",
      "Epoch 274/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0462 - mse: 0.0394 - mae: 0.1561 - val_loss: 0.0457 - val_mse: 0.0380 - val_mae: 0.1537 - lr: 0.0010\n",
      "Epoch 275/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0453 - mse: 0.0387 - mae: 0.1549 - val_loss: 0.0453 - val_mse: 0.0378 - val_mae: 0.1529 - lr: 0.0010\n",
      "Epoch 276/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0444 - mse: 0.0379 - mae: 0.1536 - val_loss: 0.0446 - val_mse: 0.0372 - val_mae: 0.1517 - lr: 0.0010\n",
      "Epoch 277/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0437 - mse: 0.0373 - mae: 0.1525 - val_loss: 0.0431 - val_mse: 0.0360 - val_mae: 0.1499 - lr: 0.0010\n",
      "Epoch 278/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0431 - mse: 0.0368 - mae: 0.1515 - val_loss: 0.0427 - val_mse: 0.0356 - val_mae: 0.1493 - lr: 0.0010\n",
      "Epoch 279/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0425 - mse: 0.0364 - mae: 0.1506 - val_loss: 0.0423 - val_mse: 0.0353 - val_mae: 0.1486 - lr: 0.0010\n",
      "Epoch 280/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0420 - mse: 0.0361 - mae: 0.1498 - val_loss: 0.0417 - val_mse: 0.0349 - val_mae: 0.1478 - lr: 0.0010\n",
      "Epoch 281/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0416 - mse: 0.0357 - mae: 0.1488 - val_loss: 0.0414 - val_mse: 0.0346 - val_mae: 0.1471 - lr: 0.0010\n",
      "Epoch 282/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0413 - mse: 0.0354 - mae: 0.1482 - val_loss: 0.0410 - val_mse: 0.0343 - val_mae: 0.1463 - lr: 0.0010\n",
      "Epoch 283/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0409 - mse: 0.0351 - mae: 0.1474 - val_loss: 0.0405 - val_mse: 0.0339 - val_mae: 0.1455 - lr: 0.0010\n",
      "Epoch 284/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0405 - mse: 0.0349 - mae: 0.1467 - val_loss: 0.0402 - val_mse: 0.0337 - val_mae: 0.1448 - lr: 0.0010\n",
      "Epoch 285/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0402 - mse: 0.0347 - mae: 0.1462 - val_loss: 0.0398 - val_mse: 0.0334 - val_mae: 0.1440 - lr: 0.0010\n",
      "Epoch 286/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0399 - mse: 0.0344 - mae: 0.1454 - val_loss: 0.0395 - val_mse: 0.0332 - val_mae: 0.1435 - lr: 0.0010\n",
      "Epoch 287/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0396 - mse: 0.0342 - mae: 0.1449 - val_loss: 0.0393 - val_mse: 0.0330 - val_mae: 0.1429 - lr: 0.0010\n",
      "Epoch 288/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0393 - mse: 0.0339 - mae: 0.1442 - val_loss: 0.0390 - val_mse: 0.0327 - val_mae: 0.1421 - lr: 0.0010\n",
      "Epoch 289/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0391 - mse: 0.0337 - mae: 0.1436 - val_loss: 0.0388 - val_mse: 0.0326 - val_mae: 0.1419 - lr: 0.0010\n",
      "Epoch 290/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0389 - mse: 0.0336 - mae: 0.1433 - val_loss: 0.0386 - val_mse: 0.0324 - val_mae: 0.1413 - lr: 0.0010\n",
      "Epoch 291/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0387 - mse: 0.0334 - mae: 0.1427 - val_loss: 0.0384 - val_mse: 0.0323 - val_mae: 0.1406 - lr: 0.0010\n",
      "Epoch 292/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0386 - mse: 0.0333 - mae: 0.1423 - val_loss: 0.0383 - val_mse: 0.0322 - val_mae: 0.1404 - lr: 0.0010\n",
      "Epoch 293/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0384 - mse: 0.0332 - mae: 0.1420 - val_loss: 0.0381 - val_mse: 0.0320 - val_mae: 0.1399 - lr: 0.0010\n",
      "Epoch 294/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0382 - mse: 0.0330 - mae: 0.1415 - val_loss: 0.0379 - val_mse: 0.0318 - val_mae: 0.1394 - lr: 0.0010\n",
      "Epoch 295/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0381 - mse: 0.0329 - mae: 0.1413 - val_loss: 0.0377 - val_mse: 0.0317 - val_mae: 0.1391 - lr: 0.0010\n",
      "Epoch 296/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0379 - mse: 0.0327 - mae: 0.1409 - val_loss: 0.0375 - val_mse: 0.0316 - val_mae: 0.1386 - lr: 0.0010\n",
      "Epoch 297/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0377 - mse: 0.0326 - mae: 0.1406 - val_loss: 0.0374 - val_mse: 0.0314 - val_mae: 0.1383 - lr: 0.0010\n",
      "Epoch 298/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0376 - mse: 0.0325 - mae: 0.1401 - val_loss: 0.0372 - val_mse: 0.0313 - val_mae: 0.1380 - lr: 0.0010\n",
      "Epoch 299/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0375 - mse: 0.0324 - mae: 0.1399 - val_loss: 0.0371 - val_mse: 0.0313 - val_mae: 0.1379 - lr: 0.0010\n",
      "Epoch 300/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0374 - mse: 0.0323 - mae: 0.1396 - val_loss: 0.0374 - val_mse: 0.0315 - val_mae: 0.1377 - lr: 0.0010\n",
      "Epoch 301/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0372 - mse: 0.0321 - mae: 0.1391 - val_loss: 0.0375 - val_mse: 0.0316 - val_mae: 0.1373 - lr: 0.0010\n",
      "Epoch 302/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0371 - mse: 0.0321 - mae: 0.1389 - val_loss: 0.0367 - val_mse: 0.0310 - val_mae: 0.1371 - lr: 0.0010\n",
      "Epoch 303/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0370 - mse: 0.0320 - mae: 0.1387 - val_loss: 0.0365 - val_mse: 0.0308 - val_mae: 0.1364 - lr: 0.0010\n",
      "Epoch 304/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0369 - mse: 0.0319 - mae: 0.1385 - val_loss: 0.0364 - val_mse: 0.0307 - val_mae: 0.1361 - lr: 0.0010\n",
      "Epoch 305/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0368 - mse: 0.0318 - mae: 0.1380 - val_loss: 0.0362 - val_mse: 0.0306 - val_mae: 0.1359 - lr: 0.0010\n",
      "Epoch 306/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0367 - mse: 0.0317 - mae: 0.1378 - val_loss: 0.0361 - val_mse: 0.0304 - val_mae: 0.1354 - lr: 0.0010\n",
      "Epoch 307/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0365 - mse: 0.0316 - mae: 0.1375 - val_loss: 0.0360 - val_mse: 0.0304 - val_mae: 0.1353 - lr: 0.0010\n",
      "Epoch 308/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0364 - mse: 0.0315 - mae: 0.1373 - val_loss: 0.0358 - val_mse: 0.0302 - val_mae: 0.1348 - lr: 0.0010\n",
      "Epoch 309/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0363 - mse: 0.0314 - mae: 0.1370 - val_loss: 0.0357 - val_mse: 0.0301 - val_mae: 0.1347 - lr: 0.0010\n",
      "Epoch 310/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0362 - mse: 0.0313 - mae: 0.1368 - val_loss: 0.0357 - val_mse: 0.0301 - val_mae: 0.1344 - lr: 0.0010\n",
      "Epoch 311/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0361 - mse: 0.0312 - mae: 0.1364 - val_loss: 0.0355 - val_mse: 0.0300 - val_mae: 0.1340 - lr: 0.0010\n",
      "Epoch 312/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0360 - mse: 0.0311 - mae: 0.1362 - val_loss: 0.0354 - val_mse: 0.0298 - val_mae: 0.1337 - lr: 0.0010\n",
      "Epoch 313/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0359 - mse: 0.0311 - mae: 0.1359 - val_loss: 0.0358 - val_mse: 0.0302 - val_mae: 0.1338 - lr: 0.0010\n",
      "Epoch 314/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0358 - mse: 0.0310 - mae: 0.1356 - val_loss: 0.0358 - val_mse: 0.0303 - val_mae: 0.1337 - lr: 0.0010\n",
      "Epoch 315/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0358 - mse: 0.0310 - mae: 0.1355 - val_loss: 0.0350 - val_mse: 0.0295 - val_mae: 0.1329 - lr: 0.0010\n",
      "Epoch 316/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0357 - mse: 0.0308 - mae: 0.1353 - val_loss: 0.0350 - val_mse: 0.0295 - val_mae: 0.1329 - lr: 0.0010\n",
      "Epoch 317/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0356 - mse: 0.0308 - mae: 0.1350 - val_loss: 0.0349 - val_mse: 0.0295 - val_mae: 0.1328 - lr: 0.0010\n",
      "Epoch 318/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0355 - mse: 0.0308 - mae: 0.1352 - val_loss: 0.0347 - val_mse: 0.0293 - val_mae: 0.1322 - lr: 0.0010\n",
      "Epoch 319/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0354 - mse: 0.0306 - mae: 0.1347 - val_loss: 0.0347 - val_mse: 0.0293 - val_mae: 0.1322 - lr: 0.0010\n",
      "Epoch 320/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0353 - mse: 0.0305 - mae: 0.1344 - val_loss: 0.0346 - val_mse: 0.0292 - val_mae: 0.1319 - lr: 0.0010\n",
      "Epoch 321/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0352 - mse: 0.0305 - mae: 0.1342 - val_loss: 0.0344 - val_mse: 0.0291 - val_mae: 0.1317 - lr: 0.0010\n",
      "Epoch 322/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0351 - mse: 0.0304 - mae: 0.1341 - val_loss: 0.0344 - val_mse: 0.0290 - val_mae: 0.1314 - lr: 0.0010\n",
      "Epoch 323/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0350 - mse: 0.0303 - mae: 0.1338 - val_loss: 0.0343 - val_mse: 0.0290 - val_mae: 0.1313 - lr: 0.0010\n",
      "Epoch 324/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0349 - mse: 0.0303 - mae: 0.1337 - val_loss: 0.0342 - val_mse: 0.0289 - val_mae: 0.1312 - lr: 0.0010\n",
      "Epoch 325/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0348 - mse: 0.0302 - mae: 0.1334 - val_loss: 0.0341 - val_mse: 0.0288 - val_mae: 0.1307 - lr: 0.0010\n",
      "Epoch 326/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0347 - mse: 0.0301 - mae: 0.1332 - val_loss: 0.0340 - val_mse: 0.0287 - val_mae: 0.1305 - lr: 0.0010\n",
      "Epoch 327/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0347 - mse: 0.0300 - mae: 0.1330 - val_loss: 0.0339 - val_mse: 0.0287 - val_mae: 0.1302 - lr: 0.0010\n",
      "Epoch 328/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0346 - mse: 0.0300 - mae: 0.1328 - val_loss: 0.0338 - val_mse: 0.0286 - val_mae: 0.1300 - lr: 0.0010\n",
      "Epoch 329/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0345 - mse: 0.0298 - mae: 0.1325 - val_loss: 0.0337 - val_mse: 0.0285 - val_mae: 0.1296 - lr: 0.0010\n",
      "Epoch 330/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0344 - mse: 0.0298 - mae: 0.1323 - val_loss: 0.0337 - val_mse: 0.0285 - val_mae: 0.1298 - lr: 0.0010\n",
      "Epoch 331/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0343 - mse: 0.0298 - mae: 0.1322 - val_loss: 0.0335 - val_mse: 0.0283 - val_mae: 0.1294 - lr: 0.0010\n",
      "Epoch 332/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0342 - mse: 0.0297 - mae: 0.1320 - val_loss: 0.0335 - val_mse: 0.0283 - val_mae: 0.1292 - lr: 0.0010\n",
      "Epoch 333/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0342 - mse: 0.0296 - mae: 0.1317 - val_loss: 0.0334 - val_mse: 0.0282 - val_mae: 0.1289 - lr: 0.0010\n",
      "Epoch 334/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0341 - mse: 0.0295 - mae: 0.1315 - val_loss: 0.0333 - val_mse: 0.0282 - val_mae: 0.1289 - lr: 0.0010\n",
      "Epoch 335/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0340 - mse: 0.0295 - mae: 0.1313 - val_loss: 0.0332 - val_mse: 0.0281 - val_mae: 0.1287 - lr: 0.0010\n",
      "Epoch 336/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0339 - mse: 0.0294 - mae: 0.1312 - val_loss: 0.0331 - val_mse: 0.0280 - val_mae: 0.1285 - lr: 0.0010\n",
      "Epoch 337/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0338 - mse: 0.0293 - mae: 0.1310 - val_loss: 0.0330 - val_mse: 0.0279 - val_mae: 0.1283 - lr: 0.0010\n",
      "Epoch 338/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0338 - mse: 0.0293 - mae: 0.1307 - val_loss: 0.0330 - val_mse: 0.0279 - val_mae: 0.1281 - lr: 0.0010\n",
      "Epoch 339/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0337 - mse: 0.0291 - mae: 0.1305 - val_loss: 0.0329 - val_mse: 0.0278 - val_mae: 0.1278 - lr: 0.0010\n",
      "Epoch 340/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0336 - mse: 0.0291 - mae: 0.1304 - val_loss: 0.0328 - val_mse: 0.0277 - val_mae: 0.1276 - lr: 0.0010\n",
      "Epoch 341/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0335 - mse: 0.0291 - mae: 0.1303 - val_loss: 0.0328 - val_mse: 0.0277 - val_mae: 0.1275 - lr: 0.0010\n",
      "Epoch 342/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0335 - mse: 0.0290 - mae: 0.1299 - val_loss: 0.0326 - val_mse: 0.0276 - val_mae: 0.1271 - lr: 0.0010\n",
      "Epoch 343/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0334 - mse: 0.0290 - mae: 0.1298 - val_loss: 0.0326 - val_mse: 0.0276 - val_mae: 0.1272 - lr: 0.0010\n",
      "Epoch 344/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0333 - mse: 0.0288 - mae: 0.1295 - val_loss: 0.0324 - val_mse: 0.0275 - val_mae: 0.1269 - lr: 0.0010\n",
      "Epoch 345/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0332 - mse: 0.0288 - mae: 0.1295 - val_loss: 0.0323 - val_mse: 0.0273 - val_mae: 0.1265 - lr: 0.0010\n",
      "Epoch 346/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0331 - mse: 0.0287 - mae: 0.1292 - val_loss: 0.0323 - val_mse: 0.0274 - val_mae: 0.1266 - lr: 0.0010\n",
      "Epoch 347/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0330 - mse: 0.0286 - mae: 0.1291 - val_loss: 0.0322 - val_mse: 0.0272 - val_mae: 0.1262 - lr: 0.0010\n",
      "Epoch 348/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0329 - mse: 0.0286 - mae: 0.1287 - val_loss: 0.0321 - val_mse: 0.0272 - val_mae: 0.1263 - lr: 0.0010\n",
      "Epoch 349/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0329 - mse: 0.0285 - mae: 0.1287 - val_loss: 0.0321 - val_mse: 0.0271 - val_mae: 0.1259 - lr: 0.0010\n",
      "Epoch 350/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0328 - mse: 0.0284 - mae: 0.1283 - val_loss: 0.0320 - val_mse: 0.0270 - val_mae: 0.1257 - lr: 0.0010\n",
      "Epoch 351/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0327 - mse: 0.0283 - mae: 0.1283 - val_loss: 0.0319 - val_mse: 0.0270 - val_mae: 0.1254 - lr: 0.0010\n",
      "Epoch 352/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0326 - mse: 0.0283 - mae: 0.1280 - val_loss: 0.0318 - val_mse: 0.0269 - val_mae: 0.1254 - lr: 0.0010\n",
      "Epoch 353/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0325 - mse: 0.0282 - mae: 0.1278 - val_loss: 0.0317 - val_mse: 0.0268 - val_mae: 0.1251 - lr: 0.0010\n",
      "Epoch 354/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0324 - mse: 0.0281 - mae: 0.1277 - val_loss: 0.0317 - val_mse: 0.0268 - val_mae: 0.1252 - lr: 0.0010\n",
      "Epoch 355/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0324 - mse: 0.0281 - mae: 0.1276 - val_loss: 0.0316 - val_mse: 0.0267 - val_mae: 0.1249 - lr: 0.0010\n",
      "Epoch 356/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0323 - mse: 0.0280 - mae: 0.1273 - val_loss: 0.0314 - val_mse: 0.0266 - val_mae: 0.1246 - lr: 0.0010\n",
      "Epoch 357/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0322 - mse: 0.0279 - mae: 0.1272 - val_loss: 0.0315 - val_mse: 0.0266 - val_mae: 0.1245 - lr: 0.0010\n",
      "Epoch 358/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0322 - mse: 0.0279 - mae: 0.1269 - val_loss: 0.0313 - val_mse: 0.0265 - val_mae: 0.1242 - lr: 0.0010\n",
      "Epoch 359/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0321 - mse: 0.0278 - mae: 0.1269 - val_loss: 0.0312 - val_mse: 0.0264 - val_mae: 0.1239 - lr: 0.0010\n",
      "Epoch 360/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0320 - mse: 0.0278 - mae: 0.1268 - val_loss: 0.0312 - val_mse: 0.0264 - val_mae: 0.1239 - lr: 0.0010\n",
      "Epoch 361/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0319 - mse: 0.0277 - mae: 0.1264 - val_loss: 0.0311 - val_mse: 0.0263 - val_mae: 0.1236 - lr: 0.0010\n",
      "Epoch 362/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0318 - mse: 0.0276 - mae: 0.1263 - val_loss: 0.0310 - val_mse: 0.0262 - val_mae: 0.1235 - lr: 0.0010\n",
      "Epoch 363/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0318 - mse: 0.0275 - mae: 0.1261 - val_loss: 0.0309 - val_mse: 0.0262 - val_mae: 0.1235 - lr: 0.0010\n",
      "Epoch 364/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0317 - mse: 0.0275 - mae: 0.1259 - val_loss: 0.0308 - val_mse: 0.0261 - val_mae: 0.1233 - lr: 0.0010\n",
      "Epoch 365/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0316 - mse: 0.0274 - mae: 0.1257 - val_loss: 0.0308 - val_mse: 0.0261 - val_mae: 0.1231 - lr: 0.0010\n",
      "Epoch 366/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0315 - mse: 0.0273 - mae: 0.1256 - val_loss: 0.0309 - val_mse: 0.0261 - val_mae: 0.1231 - lr: 0.0010\n",
      "Epoch 367/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0314 - mse: 0.0273 - mae: 0.1255 - val_loss: 0.0306 - val_mse: 0.0259 - val_mae: 0.1226 - lr: 0.0010\n",
      "Epoch 368/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0314 - mse: 0.0272 - mae: 0.1251 - val_loss: 0.0305 - val_mse: 0.0259 - val_mae: 0.1226 - lr: 0.0010\n",
      "Epoch 369/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0313 - mse: 0.0272 - mae: 0.1251 - val_loss: 0.0305 - val_mse: 0.0258 - val_mae: 0.1225 - lr: 0.0010\n",
      "Epoch 370/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0312 - mse: 0.0271 - mae: 0.1250 - val_loss: 0.0303 - val_mse: 0.0257 - val_mae: 0.1220 - lr: 0.0010\n",
      "Epoch 371/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0311 - mse: 0.0270 - mae: 0.1247 - val_loss: 0.0303 - val_mse: 0.0256 - val_mae: 0.1219 - lr: 0.0010\n",
      "Epoch 372/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0310 - mse: 0.0270 - mae: 0.1246 - val_loss: 0.0302 - val_mse: 0.0256 - val_mae: 0.1218 - lr: 0.0010\n",
      "Epoch 373/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0310 - mse: 0.0269 - mae: 0.1243 - val_loss: 0.0301 - val_mse: 0.0255 - val_mae: 0.1216 - lr: 0.0010\n",
      "Epoch 374/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0309 - mse: 0.0268 - mae: 0.1241 - val_loss: 0.0301 - val_mse: 0.0255 - val_mae: 0.1213 - lr: 0.0010\n",
      "Epoch 375/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0308 - mse: 0.0267 - mae: 0.1239 - val_loss: 0.0300 - val_mse: 0.0255 - val_mae: 0.1215 - lr: 0.0010\n",
      "Epoch 376/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0307 - mse: 0.0267 - mae: 0.1240 - val_loss: 0.0299 - val_mse: 0.0254 - val_mae: 0.1212 - lr: 0.0010\n",
      "Epoch 377/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0306 - mse: 0.0266 - mae: 0.1237 - val_loss: 0.0298 - val_mse: 0.0252 - val_mae: 0.1209 - lr: 0.0010\n",
      "Epoch 378/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0305 - mse: 0.0265 - mae: 0.1235 - val_loss: 0.0298 - val_mse: 0.0252 - val_mae: 0.1208 - lr: 0.0010\n",
      "Epoch 379/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0304 - mse: 0.0264 - mae: 0.1232 - val_loss: 0.0294 - val_mse: 0.0249 - val_mae: 0.1203 - lr: 0.0010\n",
      "Epoch 380/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0299 - mse: 0.0260 - mae: 0.1226 - val_loss: 0.0289 - val_mse: 0.0245 - val_mae: 0.1195 - lr: 0.0010\n",
      "Epoch 381/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0294 - mse: 0.0255 - mae: 0.1219 - val_loss: 0.0285 - val_mse: 0.0242 - val_mae: 0.1190 - lr: 0.0010\n",
      "Epoch 382/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0289 - mse: 0.0252 - mae: 0.1213 - val_loss: 0.0282 - val_mse: 0.0239 - val_mae: 0.1186 - lr: 0.0010\n",
      "Epoch 383/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0285 - mse: 0.0248 - mae: 0.1208 - val_loss: 0.0278 - val_mse: 0.0235 - val_mae: 0.1179 - lr: 0.0010\n",
      "Epoch 384/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0282 - mse: 0.0245 - mae: 0.1202 - val_loss: 0.0276 - val_mse: 0.0234 - val_mae: 0.1179 - lr: 0.0010\n",
      "Epoch 385/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0280 - mse: 0.0243 - mae: 0.1199 - val_loss: 0.0274 - val_mse: 0.0231 - val_mae: 0.1172 - lr: 0.0010\n",
      "Epoch 386/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0277 - mse: 0.0241 - mae: 0.1193 - val_loss: 0.0272 - val_mse: 0.0230 - val_mae: 0.1171 - lr: 0.0010\n",
      "Epoch 387/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0275 - mse: 0.0239 - mae: 0.1190 - val_loss: 0.0270 - val_mse: 0.0228 - val_mae: 0.1166 - lr: 0.0010\n",
      "Epoch 388/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0273 - mse: 0.0238 - mae: 0.1186 - val_loss: 0.0268 - val_mse: 0.0227 - val_mae: 0.1161 - lr: 0.0010\n",
      "Epoch 389/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0272 - mse: 0.0236 - mae: 0.1182 - val_loss: 0.0266 - val_mse: 0.0226 - val_mae: 0.1160 - lr: 0.0010\n",
      "Epoch 390/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0270 - mse: 0.0235 - mae: 0.1180 - val_loss: 0.0265 - val_mse: 0.0224 - val_mae: 0.1156 - lr: 0.0010\n",
      "Epoch 391/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0269 - mse: 0.0233 - mae: 0.1175 - val_loss: 0.0263 - val_mse: 0.0223 - val_mae: 0.1152 - lr: 0.0010\n",
      "Epoch 392/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0268 - mse: 0.0233 - mae: 0.1174 - val_loss: 0.0263 - val_mse: 0.0222 - val_mae: 0.1152 - lr: 0.0010\n",
      "Epoch 393/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0266 - mse: 0.0231 - mae: 0.1169 - val_loss: 0.0261 - val_mse: 0.0221 - val_mae: 0.1147 - lr: 0.0010\n",
      "Epoch 394/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0265 - mse: 0.0230 - mae: 0.1167 - val_loss: 0.0259 - val_mse: 0.0219 - val_mae: 0.1141 - lr: 0.0010\n",
      "Epoch 395/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0263 - mse: 0.0229 - mae: 0.1164 - val_loss: 0.0258 - val_mse: 0.0218 - val_mae: 0.1141 - lr: 0.0010\n",
      "Epoch 396/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0262 - mse: 0.0228 - mae: 0.1162 - val_loss: 0.0257 - val_mse: 0.0217 - val_mae: 0.1139 - lr: 0.0010\n",
      "Epoch 397/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0261 - mse: 0.0227 - mae: 0.1159 - val_loss: 0.0256 - val_mse: 0.0216 - val_mae: 0.1135 - lr: 0.0010\n",
      "Epoch 398/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0259 - mse: 0.0226 - mae: 0.1157 - val_loss: 0.0255 - val_mse: 0.0215 - val_mae: 0.1133 - lr: 0.0010\n",
      "Epoch 399/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0258 - mse: 0.0224 - mae: 0.1154 - val_loss: 0.0253 - val_mse: 0.0214 - val_mae: 0.1130 - lr: 0.0010\n",
      "Epoch 400/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0257 - mse: 0.0224 - mae: 0.1151 - val_loss: 0.0253 - val_mse: 0.0213 - val_mae: 0.1128 - lr: 0.0010\n",
      "Epoch 401/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0256 - mse: 0.0222 - mae: 0.1149 - val_loss: 0.0252 - val_mse: 0.0212 - val_mae: 0.1124 - lr: 0.0010\n",
      "Epoch 402/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0255 - mse: 0.0221 - mae: 0.1146 - val_loss: 0.0250 - val_mse: 0.0211 - val_mae: 0.1123 - lr: 0.0010\n",
      "Epoch 403/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0254 - mse: 0.0221 - mae: 0.1145 - val_loss: 0.0249 - val_mse: 0.0211 - val_mae: 0.1122 - lr: 0.0010\n",
      "Epoch 404/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0253 - mse: 0.0220 - mae: 0.1142 - val_loss: 0.0248 - val_mse: 0.0209 - val_mae: 0.1119 - lr: 0.0010\n",
      "Epoch 405/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0252 - mse: 0.0218 - mae: 0.1138 - val_loss: 0.0248 - val_mse: 0.0209 - val_mae: 0.1116 - lr: 0.0010\n",
      "Epoch 406/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0250 - mse: 0.0218 - mae: 0.1138 - val_loss: 0.0246 - val_mse: 0.0208 - val_mae: 0.1115 - lr: 0.0010\n",
      "Epoch 407/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0250 - mse: 0.0217 - mae: 0.1135 - val_loss: 0.0245 - val_mse: 0.0206 - val_mae: 0.1110 - lr: 0.0010\n",
      "Epoch 408/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0248 - mse: 0.0216 - mae: 0.1132 - val_loss: 0.0244 - val_mse: 0.0207 - val_mae: 0.1112 - lr: 0.0010\n",
      "Epoch 409/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0247 - mse: 0.0215 - mae: 0.1131 - val_loss: 0.0243 - val_mse: 0.0205 - val_mae: 0.1107 - lr: 0.0010\n",
      "Epoch 410/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0246 - mse: 0.0214 - mae: 0.1127 - val_loss: 0.0242 - val_mse: 0.0204 - val_mae: 0.1104 - lr: 0.0010\n",
      "Epoch 411/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0245 - mse: 0.0213 - mae: 0.1126 - val_loss: 0.0241 - val_mse: 0.0204 - val_mae: 0.1103 - lr: 0.0010\n",
      "Epoch 412/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0245 - mse: 0.0212 - mae: 0.1124 - val_loss: 0.0240 - val_mse: 0.0203 - val_mae: 0.1099 - lr: 0.0010\n",
      "Epoch 413/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0243 - mse: 0.0212 - mae: 0.1121 - val_loss: 0.0239 - val_mse: 0.0202 - val_mae: 0.1098 - lr: 0.0010\n",
      "Epoch 414/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0243 - mse: 0.0211 - mae: 0.1118 - val_loss: 0.0239 - val_mse: 0.0201 - val_mae: 0.1096 - lr: 0.0010\n",
      "Epoch 415/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0242 - mse: 0.0210 - mae: 0.1116 - val_loss: 0.0238 - val_mse: 0.0201 - val_mae: 0.1097 - lr: 0.0010\n",
      "Epoch 416/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0241 - mse: 0.0210 - mae: 0.1115 - val_loss: 0.0237 - val_mse: 0.0201 - val_mae: 0.1095 - lr: 0.0010\n",
      "Epoch 417/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0240 - mse: 0.0209 - mae: 0.1114 - val_loss: 0.0236 - val_mse: 0.0200 - val_mae: 0.1093 - lr: 0.0010\n",
      "Epoch 418/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0239 - mse: 0.0208 - mae: 0.1111 - val_loss: 0.0235 - val_mse: 0.0198 - val_mae: 0.1088 - lr: 0.0010\n",
      "Epoch 419/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0238 - mse: 0.0208 - mae: 0.1110 - val_loss: 0.0235 - val_mse: 0.0199 - val_mae: 0.1089 - lr: 0.0010\n",
      "Epoch 420/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0237 - mse: 0.0207 - mae: 0.1108 - val_loss: 0.0234 - val_mse: 0.0197 - val_mae: 0.1087 - lr: 0.0010\n",
      "Epoch 421/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0236 - mse: 0.0206 - mae: 0.1105 - val_loss: 0.0233 - val_mse: 0.0197 - val_mae: 0.1084 - lr: 0.0010\n",
      "Epoch 422/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0236 - mse: 0.0205 - mae: 0.1102 - val_loss: 0.0232 - val_mse: 0.0196 - val_mae: 0.1082 - lr: 0.0010\n",
      "Epoch 423/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0235 - mse: 0.0205 - mae: 0.1102 - val_loss: 0.0231 - val_mse: 0.0195 - val_mae: 0.1080 - lr: 0.0010\n",
      "Epoch 424/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0234 - mse: 0.0203 - mae: 0.1098 - val_loss: 0.0230 - val_mse: 0.0194 - val_mae: 0.1076 - lr: 0.0010\n",
      "Epoch 425/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0233 - mse: 0.0203 - mae: 0.1097 - val_loss: 0.0230 - val_mse: 0.0194 - val_mae: 0.1077 - lr: 0.0010\n",
      "Epoch 426/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0232 - mse: 0.0202 - mae: 0.1095 - val_loss: 0.0229 - val_mse: 0.0194 - val_mae: 0.1076 - lr: 0.0010\n",
      "Epoch 427/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0231 - mse: 0.0201 - mae: 0.1093 - val_loss: 0.0228 - val_mse: 0.0193 - val_mae: 0.1073 - lr: 0.0010\n",
      "Epoch 428/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0230 - mse: 0.0201 - mae: 0.1091 - val_loss: 0.0227 - val_mse: 0.0192 - val_mae: 0.1069 - lr: 0.0010\n",
      "Epoch 429/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0230 - mse: 0.0200 - mae: 0.1089 - val_loss: 0.0227 - val_mse: 0.0192 - val_mae: 0.1071 - lr: 0.0010\n",
      "Epoch 430/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0229 - mse: 0.0199 - mae: 0.1088 - val_loss: 0.0226 - val_mse: 0.0191 - val_mae: 0.1068 - lr: 0.0010\n",
      "Epoch 431/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0228 - mse: 0.0198 - mae: 0.1086 - val_loss: 0.0225 - val_mse: 0.0191 - val_mae: 0.1068 - lr: 0.0010\n",
      "Epoch 432/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0227 - mse: 0.0198 - mae: 0.1083 - val_loss: 0.0224 - val_mse: 0.0190 - val_mae: 0.1064 - lr: 0.0010\n",
      "Epoch 433/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0227 - mse: 0.0197 - mae: 0.1082 - val_loss: 0.0223 - val_mse: 0.0189 - val_mae: 0.1061 - lr: 0.0010\n",
      "Epoch 434/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0226 - mse: 0.0197 - mae: 0.1080 - val_loss: 0.0223 - val_mse: 0.0189 - val_mae: 0.1062 - lr: 0.0010\n",
      "Epoch 435/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0225 - mse: 0.0196 - mae: 0.1078 - val_loss: 0.0222 - val_mse: 0.0188 - val_mae: 0.1061 - lr: 0.0010\n",
      "Epoch 436/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0224 - mse: 0.0195 - mae: 0.1076 - val_loss: 0.0222 - val_mse: 0.0188 - val_mae: 0.1058 - lr: 0.0010\n",
      "Epoch 437/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0224 - mse: 0.0194 - mae: 0.1074 - val_loss: 0.0221 - val_mse: 0.0187 - val_mae: 0.1056 - lr: 0.0010\n",
      "Epoch 438/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0223 - mse: 0.0194 - mae: 0.1071 - val_loss: 0.0220 - val_mse: 0.0186 - val_mae: 0.1056 - lr: 0.0010\n",
      "Epoch 439/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0222 - mse: 0.0193 - mae: 0.1071 - val_loss: 0.0220 - val_mse: 0.0185 - val_mae: 0.1053 - lr: 0.0010\n",
      "Epoch 440/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0221 - mse: 0.0193 - mae: 0.1069 - val_loss: 0.0219 - val_mse: 0.0185 - val_mae: 0.1052 - lr: 0.0010\n",
      "Epoch 441/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0221 - mse: 0.0192 - mae: 0.1067 - val_loss: 0.0218 - val_mse: 0.0184 - val_mae: 0.1051 - lr: 0.0010\n",
      "Epoch 442/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0220 - mse: 0.0192 - mae: 0.1066 - val_loss: 0.0218 - val_mse: 0.0184 - val_mae: 0.1049 - lr: 0.0010\n",
      "Epoch 443/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0219 - mse: 0.0191 - mae: 0.1064 - val_loss: 0.0217 - val_mse: 0.0183 - val_mae: 0.1045 - lr: 0.0010\n",
      "Epoch 444/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0219 - mse: 0.0190 - mae: 0.1061 - val_loss: 0.0216 - val_mse: 0.0183 - val_mae: 0.1046 - lr: 0.0010\n",
      "Epoch 445/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0218 - mse: 0.0190 - mae: 0.1060 - val_loss: 0.0216 - val_mse: 0.0182 - val_mae: 0.1043 - lr: 0.0010\n",
      "Epoch 446/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0217 - mse: 0.0189 - mae: 0.1058 - val_loss: 0.0215 - val_mse: 0.0182 - val_mae: 0.1043 - lr: 0.0010\n",
      "Epoch 447/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0217 - mse: 0.0189 - mae: 0.1058 - val_loss: 0.0215 - val_mse: 0.0182 - val_mae: 0.1042 - lr: 0.0010\n",
      "Epoch 448/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0216 - mse: 0.0188 - mae: 0.1054 - val_loss: 0.0214 - val_mse: 0.0181 - val_mae: 0.1038 - lr: 0.0010\n",
      "Epoch 449/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0216 - mse: 0.0188 - mae: 0.1054 - val_loss: 0.0214 - val_mse: 0.0180 - val_mae: 0.1038 - lr: 0.0010\n",
      "Epoch 450/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0215 - mse: 0.0187 - mae: 0.1051 - val_loss: 0.0213 - val_mse: 0.0180 - val_mae: 0.1036 - lr: 0.0010\n",
      "Epoch 451/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0214 - mse: 0.0186 - mae: 0.1050 - val_loss: 0.0212 - val_mse: 0.0179 - val_mae: 0.1035 - lr: 0.0010\n",
      "Epoch 452/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0214 - mse: 0.0186 - mae: 0.1048 - val_loss: 0.0211 - val_mse: 0.0179 - val_mae: 0.1034 - lr: 0.0010\n",
      "Epoch 453/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0213 - mse: 0.0185 - mae: 0.1047 - val_loss: 0.0211 - val_mse: 0.0178 - val_mae: 0.1032 - lr: 0.0010\n",
      "Epoch 454/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0212 - mse: 0.0185 - mae: 0.1045 - val_loss: 0.0210 - val_mse: 0.0178 - val_mae: 0.1031 - lr: 0.0010\n",
      "Epoch 455/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0212 - mse: 0.0184 - mae: 0.1043 - val_loss: 0.0210 - val_mse: 0.0178 - val_mae: 0.1029 - lr: 0.0010\n",
      "Epoch 456/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0211 - mse: 0.0184 - mae: 0.1042 - val_loss: 0.0209 - val_mse: 0.0177 - val_mae: 0.1029 - lr: 0.0010\n",
      "Epoch 457/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0211 - mse: 0.0183 - mae: 0.1040 - val_loss: 0.0209 - val_mse: 0.0176 - val_mae: 0.1025 - lr: 0.0010\n",
      "Epoch 458/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0210 - mse: 0.0182 - mae: 0.1038 - val_loss: 0.0208 - val_mse: 0.0175 - val_mae: 0.1024 - lr: 0.0010\n",
      "Epoch 459/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0209 - mse: 0.0182 - mae: 0.1038 - val_loss: 0.0207 - val_mse: 0.0175 - val_mae: 0.1022 - lr: 0.0010\n",
      "Epoch 460/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0209 - mse: 0.0181 - mae: 0.1035 - val_loss: 0.0207 - val_mse: 0.0174 - val_mae: 0.1021 - lr: 0.0010\n",
      "Epoch 461/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0209 - mse: 0.0181 - mae: 0.1035 - val_loss: 0.0207 - val_mse: 0.0174 - val_mae: 0.1020 - lr: 0.0010\n",
      "Epoch 462/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0208 - mse: 0.0181 - mae: 0.1033 - val_loss: 0.0206 - val_mse: 0.0174 - val_mae: 0.1018 - lr: 0.0010\n",
      "Epoch 463/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0207 - mse: 0.0180 - mae: 0.1031 - val_loss: 0.0205 - val_mse: 0.0173 - val_mae: 0.1016 - lr: 0.0010\n",
      "Epoch 464/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0207 - mse: 0.0179 - mae: 0.1029 - val_loss: 0.0205 - val_mse: 0.0173 - val_mae: 0.1016 - lr: 0.0010\n",
      "Epoch 465/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0206 - mse: 0.0179 - mae: 0.1027 - val_loss: 0.0205 - val_mse: 0.0173 - val_mae: 0.1015 - lr: 0.0010\n",
      "Epoch 466/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0206 - mse: 0.0178 - mae: 0.1026 - val_loss: 0.0204 - val_mse: 0.0172 - val_mae: 0.1013 - lr: 0.0010\n",
      "Epoch 467/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0205 - mse: 0.0178 - mae: 0.1026 - val_loss: 0.0204 - val_mse: 0.0172 - val_mae: 0.1015 - lr: 0.0010\n",
      "Epoch 468/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0204 - mse: 0.0177 - mae: 0.1023 - val_loss: 0.0203 - val_mse: 0.0172 - val_mae: 0.1012 - lr: 0.0010\n",
      "Epoch 469/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0204 - mse: 0.0177 - mae: 0.1022 - val_loss: 0.0202 - val_mse: 0.0172 - val_mae: 0.1013 - lr: 0.0010\n",
      "Epoch 470/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0203 - mse: 0.0177 - mae: 0.1020 - val_loss: 0.0202 - val_mse: 0.0170 - val_mae: 0.1008 - lr: 0.0010\n",
      "Epoch 471/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0203 - mse: 0.0176 - mae: 0.1019 - val_loss: 0.0202 - val_mse: 0.0170 - val_mae: 0.1008 - lr: 0.0010\n",
      "Epoch 472/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0202 - mse: 0.0176 - mae: 0.1017 - val_loss: 0.0201 - val_mse: 0.0170 - val_mae: 0.1008 - lr: 0.0010\n",
      "Epoch 473/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0202 - mse: 0.0175 - mae: 0.1017 - val_loss: 0.0200 - val_mse: 0.0169 - val_mae: 0.1002 - lr: 0.0010\n",
      "Epoch 474/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0202 - mse: 0.0175 - mae: 0.1015 - val_loss: 0.0200 - val_mse: 0.0169 - val_mae: 0.1006 - lr: 0.0010\n",
      "Epoch 475/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0201 - mse: 0.0174 - mae: 0.1014 - val_loss: 0.0200 - val_mse: 0.0168 - val_mae: 0.1000 - lr: 0.0010\n",
      "Epoch 476/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0200 - mse: 0.0174 - mae: 0.1011 - val_loss: 0.0200 - val_mse: 0.0168 - val_mae: 0.1001 - lr: 0.0010\n",
      "Epoch 477/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0174 - mae: 0.1011 - val_loss: 0.0199 - val_mse: 0.0168 - val_mae: 0.0999 - lr: 0.0010\n",
      "Epoch 478/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0199 - mse: 0.0173 - mae: 0.1009 - val_loss: 0.0198 - val_mse: 0.0167 - val_mae: 0.0999 - lr: 0.0010\n",
      "Epoch 479/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0199 - mse: 0.0172 - mae: 0.1008 - val_loss: 0.0198 - val_mse: 0.0167 - val_mae: 0.0996 - lr: 0.0010\n",
      "Epoch 480/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0198 - mse: 0.0172 - mae: 0.1005 - val_loss: 0.0197 - val_mse: 0.0166 - val_mae: 0.0996 - lr: 0.0010\n",
      "Epoch 481/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0198 - mse: 0.0172 - mae: 0.1005 - val_loss: 0.0197 - val_mse: 0.0166 - val_mae: 0.0993 - lr: 0.0010\n",
      "Epoch 482/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0197 - mse: 0.0171 - mae: 0.1004 - val_loss: 0.0196 - val_mse: 0.0165 - val_mae: 0.0993 - lr: 0.0010\n",
      "Epoch 483/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0197 - mse: 0.0171 - mae: 0.1002 - val_loss: 0.0196 - val_mse: 0.0165 - val_mae: 0.0993 - lr: 0.0010\n",
      "Epoch 484/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0197 - mse: 0.0170 - mae: 0.1001 - val_loss: 0.0196 - val_mse: 0.0165 - val_mae: 0.0991 - lr: 0.0010\n",
      "Epoch 485/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0196 - mse: 0.0170 - mae: 0.0999 - val_loss: 0.0195 - val_mse: 0.0165 - val_mae: 0.0990 - lr: 0.0010\n",
      "Epoch 486/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0196 - mse: 0.0170 - mae: 0.0999 - val_loss: 0.0194 - val_mse: 0.0163 - val_mae: 0.0985 - lr: 0.0010\n",
      "Epoch 487/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0195 - mse: 0.0169 - mae: 0.0996 - val_loss: 0.0194 - val_mse: 0.0164 - val_mae: 0.0986 - lr: 0.0010\n",
      "Epoch 488/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0195 - mse: 0.0169 - mae: 0.0995 - val_loss: 0.0194 - val_mse: 0.0163 - val_mae: 0.0983 - lr: 0.0010\n",
      "Epoch 489/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0194 - mse: 0.0168 - mae: 0.0994 - val_loss: 0.0193 - val_mse: 0.0162 - val_mae: 0.0982 - lr: 0.0010\n",
      "Epoch 490/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0194 - mse: 0.0168 - mae: 0.0993 - val_loss: 0.0193 - val_mse: 0.0162 - val_mae: 0.0981 - lr: 0.0010\n",
      "Epoch 491/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0193 - mse: 0.0167 - mae: 0.0992 - val_loss: 0.0193 - val_mse: 0.0163 - val_mae: 0.0984 - lr: 0.0010\n",
      "Epoch 492/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0193 - mse: 0.0167 - mae: 0.0990 - val_loss: 0.0192 - val_mse: 0.0162 - val_mae: 0.0979 - lr: 0.0010\n",
      "Epoch 493/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0192 - mse: 0.0167 - mae: 0.0988 - val_loss: 0.0192 - val_mse: 0.0162 - val_mae: 0.0980 - lr: 0.0010\n",
      "Epoch 494/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0192 - mse: 0.0167 - mae: 0.0988 - val_loss: 0.0191 - val_mse: 0.0161 - val_mae: 0.0978 - lr: 0.0010\n",
      "Epoch 495/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0192 - mse: 0.0166 - mae: 0.0986 - val_loss: 0.0191 - val_mse: 0.0161 - val_mae: 0.0976 - lr: 0.0010\n",
      "Epoch 496/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0191 - mse: 0.0166 - mae: 0.0986 - val_loss: 0.0191 - val_mse: 0.0161 - val_mae: 0.0978 - lr: 0.0010\n",
      "Epoch 497/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0191 - mse: 0.0166 - mae: 0.0985 - val_loss: 0.0190 - val_mse: 0.0161 - val_mae: 0.0976 - lr: 0.0010\n",
      "Epoch 498/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0190 - mse: 0.0165 - mae: 0.0983 - val_loss: 0.0189 - val_mse: 0.0159 - val_mae: 0.0972 - lr: 0.0010\n",
      "Epoch 499/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0190 - mse: 0.0164 - mae: 0.0980 - val_loss: 0.0189 - val_mse: 0.0160 - val_mae: 0.0974 - lr: 0.0010\n",
      "Epoch 500/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0190 - mse: 0.0165 - mae: 0.0981 - val_loss: 0.0189 - val_mse: 0.0159 - val_mae: 0.0970 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [X_real_train, X_imag_train],\n",
    "    y_train,\n",
    "    epochs=500,\n",
    "    batch_size=16,\n",
    "    validation_data=([X_real_test, X_imag_test], y_test),\n",
    "    callbacks=[adjust_lr, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0189 - mse: 0.0159 - mae: 0.0970\n",
      "Test MSE: 0.0159, Test MAE: 0.0970\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_mse, test_mae = model.evaluate([X_real_test, X_imag_test], y_test)\n",
    "print(f\"Test MSE: {test_mse:.4f}, Test MAE: {test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAGwCAYAAABB4NqyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABj0UlEQVR4nO3dd3gVZd7G8e85J70H0iEJoYcOoRgQEMEAViyvqAhiRwEF1l1FrFiwt1VgQdFlLaBiYRWVoIIIKDXUSA0kQEIIJSGEtHPm/SPL0ZiAkDYp9+e6zrXJM8/M/GZgObczzzNjMQzDQERERKQBsZpdgIiIiEhNUwASERGRBkcBSERERBocBSARERFpcBSAREREpMFRABIREZEGRwFIREREGhwXswuojRwOBwcPHsTX1xeLxWJ2OSIiInIODMPgxIkTREREYLWe/RqPAlA5Dh48SGRkpNlliIiISAWkpaXRtGnTs/ZRACqHr68vUHIC/fz8TK5GREREzkVOTg6RkZHO7/GzUQAqx+nbXn5+fgpAIiIidcy5DF/RIGgRERFpcBSAREREpMFRABIREZEGR2OARESkWtjtdoqKiswuQ+oZNze3v5zifi4UgEREpEoZhkFGRgbHjx83uxSph6xWKzExMbi5uVVqOwpAIiJSpU6Hn5CQELy8vPRAWakypx9UnJ6eTlRUVKX+bikAiYhIlbHb7c7w07hxY7PLkXooODiYgwcPUlxcjKura4W3o0HQIiJSZU6P+fHy8jK5EqmvTt/6stvtldqOApCIiFQ53faS6lJVf7dMD0DTp08nJiYGDw8P4uLiWL58+Rn7pqenc9NNN9GmTRusVisTJkwot9/x48cZO3Ys4eHheHh4EBsby6JFi6rpCERERKSuMTUAzZ8/nwkTJjBlyhQ2bNhA3759GTp0KKmpqeX2LygoIDg4mClTptC5c+dy+xQWFnLJJZewd+9ePv30U7Zv387s2bNp0qRJdR6KiIiI1CGmBqBXXnmF22+/nTvuuIPY2Fhee+01IiMjmTFjRrn9mzVrxuuvv86oUaPw9/cvt8+cOXM4evQoX3zxBX369CE6OpoLL7zwjIEJSoJVTk5OqY+IiEhlXHTRRWe8U1GevXv3YrFYSEpKqraa5HemBaDCwkLWrVtHQkJCqfaEhARWrlxZ4e0uXLiQ+Ph4xo4dS2hoKB06dODZZ58962CpadOm4e/v7/xERkZWeP9nY3cYZJ7IZ2/WyWrZvoiInD+LxXLWz+jRoyu03c8++4ynnnrqnPtHRkaSnp5Ohw4dKrS/c6WgVcK0afBZWVnY7XZCQ0NLtYeGhpKRkVHh7e7Zs4cffviBESNGsGjRInbu3MnYsWMpLi7mscceK3edyZMnM2nSJOfvOTk51RKCVu7OYuQ7q2kT6st3E/tV+fZFROT8paenO3+eP38+jz32GNu3b3e2eXp6lupfVFR0TtOvGzVqdF512Gw2wsLCzmsdqTjTB0H/eTS3YRiVGuHtcDgICQlh1qxZxMXFccMNNzBlypQz3lYDcHd3x8/Pr9SnOoT6eQCQkX2qWrYvIlLbGIZBXmGxKR/DMM6pxrCwMOfH398fi8Xi/D0/P5+AgAA+/vhjLrroIjw8PHj//fc5cuQIN954I02bNsXLy4uOHTvy0Ucfldrun2+BNWvWjGeffZbbbrsNX19foqKimDVrlnP5n6/MLF26FIvFwvfff0/37t3x8vKid+/epcIZwNNPP01ISAi+vr7ccccdPPTQQ3Tp0qVCf15QMizkvvvuIyQkBA8PDy688ELWrFnjXH7s2DFGjBhBcHAwnp6etGrVinfffRcoubszbtw45ySkZs2aMW3atArXUp1MuwIUFBSEzWYrc7UnMzOzzFWh8xEeHo6rqys2m83ZFhsbS0ZGBoWFhZV+dHZlRORs4ge3SWQ5/MkvGoSHq+2vVxIRqcNOFdlp99h3pux729TBeLlVzdfcgw8+yMsvv8y7776Lu7s7+fn5xMXF8eCDD+Ln58fXX3/NyJEjad68Ob169Trjdl5++WWeeuopHn74YT799FPuuece+vXrR9u2bc+4zpQpU3j55ZcJDg5mzJgx3HbbbaxYsQKADz74gGeeeYbp06fTp08f5s2bx8svv0xMTEyFj/Uf//gHCxYs4N///jfR0dG88MILDB48mF27dtGoUSMeffRRtm3bxjfffENQUBC7du3i1KmS/7B/4403WLhwIR9//DFRUVGkpaWRlpZW4Vqqk2kByM3Njbi4OBITE7n66qud7YmJiVx11VUV3m6fPn348MMPcTgczpel7dixg/DwcFPDD4C3jw/NrRn4GXlk5hQQ1VgPChMRqQsmTJjANddcU6rtgQcecP48fvx4vv32Wz755JOzBqBLL72Ue++9FygJVa+++ipLly49awB65pln6N+/PwAPPfQQl112Gfn5+Xh4ePDPf/6T22+/nVtvvRWAxx57jMWLF5Obm1uh4zx58iQzZszgvffeY+jQoQDMnj2bxMRE3nnnHf7+97+TmppK165d6d69O1ByZeu01NRUWrVqxYUXXojFYiE6OrpCddQEU1+FMWnSJEaOHEn37t2Jj49n1qxZpKamMmbMGKBkbM6BAweYO3euc53TlwZzc3M5fPgwSUlJuLm50a5dOwDuuece/vnPf3L//fczfvx4du7cybPPPst9991X48f3Z5aAkr8IQZYc1h09ogAkIvWep6uNbVMHm7bvqnL6y/40u93Oc889x/z58zlw4AAFBQUUFBTg7e191u106tTJ+fPpW22ZmZnnvE54eDhQcrckKiqK7du3OwPVaT179uSHH344p+P6s927d1NUVESfPn2cba6urvTs2ZPk5GSg5Hv22muvZf369SQkJDBs2DB69+4NwOjRo7nkkkto06YNQ4YM4fLLLy8z2am2MDUADR8+nCNHjjB16lTnyPdFixY5E2N6enqZZwJ17drV+fO6dev48MMPiY6OZu/evUDJKPrFixczceJEOnXqRJMmTbj//vt58MEHa+y4zsgzgFyLDz5GLjkZe6BV9cw2ExGpLSwWS5XdhjLTn4PNyy+/zKuvvsprr71Gx44d8fb2ZsKECRQWFp51O38ePG2xWHA4HOe8zukxsn9cp7yxtBV1et2zjc8dOnQo+/bt4+uvv2bJkiUMHDiQsWPH8tJLL9GtWzdSUlL45ptvWLJkCddffz2DBg3i008/rXBN1cX0QdD33nsve/fupaCggHXr1tGv3++zo9577z2WLl1aqr9hGGU+p8PPafHx8fzyyy/k5+eze/duHn744VJjgsx0zK0kvRdmpZhciYiIVNTy5cu56qqruPnmm+ncuTPNmzdn586dNV5HmzZtWL16dam2tWvXVnh7LVu2xM3NjZ9//tnZVlRUxNq1a4mNjXW2BQcHM3r0aN5//31ee+21UoO5/fz8GD58OLNnz2b+/PksWLCAo0ePVrim6lL3Y3kdc9IzAgp2wvHyn3YtIiK1X8uWLVmwYAErV64kMDCQV155hYyMjFIhoSaMHz+eO++8k+7du9O7d2/mz5/Ppk2baN68+V+u++fZZADt2rXjnnvu4e9//zuNGjUiKiqKF154gby8PG6//XagZJxRXFwc7du3p6CggK+++sp53K+++irh4eF06dIFq9XKJ598QlhYGAEBAVV63FVBAaiGFfpGwnFwPVE7R8WLiMhfe/TRR0lJSWHw4MF4eXlx1113MWzYMLKzs2u0jhEjRrBnzx4eeOAB8vPzuf766xk9enSZq0LlueGGG8q0paSk8Nxzz+FwOBg5ciQnTpyge/fufPfddwQGBgIlk5gmT57M3r178fT0pG/fvsybNw8AHx8fnn/+eXbu3InNZqNHjx4sWrTIOSmpNrEYlblZWE/l5OTg7+9PdnZ2lT8TaPNnL9Bx0zP86t6HXpP1glYRqV/y8/NJSUlxvuRaat4ll1xCWFgY//nPf8wupVqc7e/Y+Xx/6wpQDfMJbw2bIKhgn9mliIhIHZeXl8fMmTMZPHgwNpuNjz76iCVLlpCYmGh2abVe7bsmVc+FtekBQDPjAFnHjplcjYiI1GUWi4VFixbRt29f4uLi+O9//8uCBQsYNGiQ2aXVeroCVMM8GzUhyxJIEMc4+NsaguJr5/MRRESk9vP09GTJkiVml1En6QqQCQ56tgbg5L51JlciIiLSMCkAmeBkow4AuGUkmVuIiIhIA6UAZALX5iWPGG+W/Sv8xRNARUREpOopAJmgVY/BnDA8aWwc49D2VWaXIyIi0uAoAJnA39eHzR4lL9Y7smaBydWIiIg0PApAJjkWcxkAEfs+B3uRydWIiEhlXXTRRUyYMMH5e7NmzXjttdfOuo7FYuGLL76o9L6rajsNiQKQSWIvGs5hw48A+1FyNi40uxwRkQbriiuuOONzc1atWoXFYmH9+vXnvd01a9Zw1113Vba8Up544gm6dOlSpj09PZ2hQ4dW6b7+7L333quV7/SqKAUgkzQPa8RPXoMByP75bZOrERFpuG6//XZ++OEH9u0r+4T+OXPm0KVLF7p163be2w0ODsbLy6sqSvxLYWFhuLu718i+6gsFIBPZeowGoMnRVTiOpJhbjIhIA3X55ZcTEhLCe++9V6o9Ly+P+fPnc/vtt3PkyBFuvPFGmjZtipeXFx07duSjjz4663b/fAts586d9OvXDw8PD9q1a1fu6yoefPBBWrdujZeXF82bN+fRRx+lqKhkmMR7773Hk08+ycaNG7FYLFgsFmfNf74FtnnzZi6++GI8PT1p3Lgxd911F7m5uc7lo0ePZtiwYbz00kuEh4fTuHFjxo4d69xXRaSmpnLVVVfh4+ODn58f119/PYcOHXIu37hxIwMGDMDX1xc/Pz/i4uJYu3YtAPv27eOKK64gMDAQb29v2rdvz6JF1fu+TD0J2kSX9LmAlcs60duyidTvZxJ1/fNmlyQiUrUMA4ryzNm3qxdYLH/ZzcXFhVGjRvHee+/x2GOPYfnfOp988gmFhYWMGDGCvLw84uLiePDBB/Hz8+Prr79m5MiRNG/enF69ev3lPhwOB9dccw1BQUH88ssv5OTklBovdJqvry/vvfceERERbN68mTvvvBNfX1/+8Y9/MHz4cLZs2cK3337rfPqzv79/mW3k5eUxZMgQLrjgAtasWUNmZiZ33HEH48aNKxXyfvzxR8LDw/nxxx/ZtWsXw4cPp0uXLtx5551/eTx/ZhgGw4YNw9vbm2XLllFcXMy9997L8OHDWbp0KVDy5vquXbsyY8YMbDYbSUlJuLq6AjB27FgKCwv56aef8Pb2Ztu2bfj4+Jx3HedDAchE3u4u7G8xHPZswv+3+WB/GmyuZpclIlJ1ivLg2Qhz9v3wQXDzPqeut912Gy+++CJLly5lwIABQMntr2uuuYbAwEACAwN54IEHnP3Hjx/Pt99+yyeffHJOAWjJkiUkJyezd+9emjZtCsCzzz5bZtzOI4884vy5WbNm/O1vf2P+/Pn84x//wNPTEx8fH1xcXAgLCzvjvj744ANOnTrF3Llz8fYuOf4333yTK664gueff57Q0FAAAgMDefPNN7HZbLRt25bLLruM77//vkIBaMmSJWzatImUlBQiIyMB+M9//kP79u1Zs2YNPXr0IDU1lb///e+0bdsWgFatWjnXT01N5dprr6Vjx44ANG/e/LxrOF+6BWay7gkjyDQC8Hcc4/Daz80uR0SkQWrbti29e/dmzpw5AOzevZvly5dz2223AWC323nmmWfo1KkTjRs3xsfHh8WLF5OamnpO209OTiYqKsoZfgDi4+PL9Pv000+58MILCQsLw8fHh0cfffSc9/HHfXXu3NkZfgD69OmDw+Fg+/btzrb27dtjs9mcv4eHh5OZmXle+/rjPiMjI53hB6Bdu3YEBASQnJwMwKRJk7jjjjsYNGgQzz33HLt373b2ve+++3j66afp06cPjz/+OJs2bapQHedDV4BM1jwskIX+Q7ky5yNyV8wiuNf1ZpckIlJ1XL1KrsSYte/zcPvttzNu3Djeeust3n33XaKjoxk4cCAAL7/8Mq+++iqvvfYaHTt2xNvbmwkTJlBYWHhO2zYMo0yb5U+353755RduuOEGnnzySQYPHoy/vz/z5s3j5ZdfPq/jMAyjzLbL2+fp209/XOao4NsJzrTPP7Y/8cQT3HTTTXz99dd88803PP7448ybN4+rr76aO+64g8GDB/P111+zePFipk2bxssvv8z48eMrVM+50BWgWqBxvztxGBZictaQn7HT7HJERKqOxVJyG8qMzzmM//mj66+/HpvNxocffsi///1vbr31VueX9/Lly7nqqqu4+eab6dy5M82bN2fnznP/97pdu3akpqZy8ODvYXDVqtJvAlixYgXR0dFMmTKF7t2706pVqzIz09zc3LDb7X+5r6SkJE6ePFlq21arldatW59zzefj9PGlpaU527Zt20Z2djaxsbHOttatWzNx4kQWL17MNddcw7vvvutcFhkZyZgxY/jss8/429/+xuzZs6ul1tMUgGqBC7p1Y7WtCwApi98ytxgRkQbKx8eH4cOH8/DDD3Pw4EFGjx7tXNayZUsSExNZuXIlycnJ3H333WRkZJzztgcNGkSbNm0YNWoUGzduZPny5UyZMqVUn5YtW5Kamsq8efPYvXs3b7zxBp9/XnpoRLNmzUhJSSEpKYmsrCwKCgrK7GvEiBF4eHhwyy23sGXLFn788UfGjx/PyJEjneN/Ksput5OUlFTqs23bNgYNGkSnTp0YMWIE69evZ/Xq1YwaNYr+/fvTvXt3Tp06xbhx41i6dCn79u1jxYoVrFmzxhmOJkyYwHfffUdKSgrr16/nhx9+KBWcqoMCUC1gs1rIaT8CgPCUzzCKy/6FFhGR6nf77bdz7NgxBg0aRFRUlLP90UcfpVu3bgwePJiLLrqIsLAwhg0bds7btVqtfP755xQUFNCzZ0/uuOMOnnnmmVJ9rrrqKiZOnMi4cePo0qULK1eu5NFHHy3V59prr2XIkCEMGDCA4ODgcqfie3l58d1333H06FF69OjBddddx8CBA3nzzTfP72SUIzc3l65du5b6XHrppc5p+IGBgfTr149BgwbRvHlz5s+fD4DNZuPIkSOMGjWK1q1bc/311zN06FCefPJJoCRYjR07ltjYWIYMGUKbNm2YPn16pes9G4tR3o3JBi4nJwd/f3+ys7Px8/OrkX0eP3GSwpfaE2I5xu7+/6TFgFE1sl8RkaqUn59PSkoKMTExeHh4mF2O1ENn+zt2Pt/fugJUSwT4erM55EoA7GvmmFyNiIhI/aYAVItEDrobh2Ghdd4GjqT9ZnY5IiIi9ZYCUC3Suk17Nrp3BWBP4iyTqxEREam/FIBqmcIONwIQmfol9uJik6sRERGpnxSAapnOl4wgB2/CyGLT8oVmlyMiUiGaXyPVpar+bikA1TIent7sCB4MQOHauSZXIyJyfk4/XTgvz6QXoEq9d/rp2398jUdF6FUYtVDoRXfCJ5/RJfdnMjMzCAk580vvRERqE5vNRkBAgPOdUl5eXmd8LYPI+XI4HBw+fBgvLy9cXCoXYRSAaqHIdvHsc2lGdPFefkt8j5ARD5ldkojIOTv9pvKKvlhT5GysVitRUVGVDtYKQLWRxcKRVv9HdPKLNEpZCCgAiUjdYbFYCA8PJyQkhKKiIrPLkXrGzc0Nq7XyI3gUgGqp5n1vgOQXaVuUzP6DB2kaEWF2SSIi58Vms1V6nIZIddEg6FoqIKIlaS5RuFgcbF+p2WAiIiJVSQGoFjvW5CIA3PYsMbcQERGRekYBqBYL6HQZALEnV1NYpIciioiIVBUFoFossvMAcvEkyJLNjg3LzS5HRESk3jA9AE2fPt35Svu4uDiWLz/zF316ejo33XQTbdq0wWq1MmHChLNue968eVgsFoYNG1a1RdcQi4s7u3x6AJCz5RuTqxEREak/TA1A8+fPZ8KECUyZMoUNGzbQt29fhg4dSmpqarn9CwoKCA4OZsqUKXTu3Pms2963bx8PPPAAffv2rY7Sa0xRdD8AfDN+MbkSERGR+sPUAPTKK69w++23c8cddxAbG8trr71GZGQkM2bMKLd/s2bNeP311xk1ahT+/v5n3K7dbmfEiBE8+eSTNG/e/C/rKCgoICcnp9SntmjSZRAALQuSycs7aXI1IiIi9YNpAaiwsJB169aRkJBQqj0hIYGVK1dWattTp04lODiY22+//Zz6T5s2DX9/f+cnMjKyUvuvSuEtOnMMPzwthWxfv8zsckREROoF0wJQVlYWdrud0NDQUu2hoaFkZGRUeLsrVqzgnXfeYfbs2ee8zuTJk8nOznZ+0tLSKrz/qmaxWkn16wbAie0KQCIiIlXB9CdB//ldHoZhVPj9HidOnODmm29m9uzZBAUFnfN67u7uuLu7V2ifNaE4sjdsXUrAoV/NLkVERKReMC0ABQUFYbPZylztyczMLHNV6Fzt3r2bvXv3csUVVzjbHA4HAC4uLmzfvp0WLVpUvGiTBHcYCFufpWXBNgoL8nFz9zC7JBERkTrNtFtgbm5uxMXFkZiYWKo9MTGR3r17V2ibbdu2ZfPmzSQlJTk/V155JQMGDCApKalWje05H5FtunIMX7wsBaRs+tnsckREROo8U2+BTZo0iZEjR9K9e3fi4+OZNWsWqampjBkzBigZm3PgwAHmzp3rXCcpKQmA3NxcDh8+TFJSEm5ubrRr1w4PDw86dOhQah8BAQEAZdrrEovVxh6vLsTlLSc7+UfoMcjskkREROo0UwPQ8OHDOXLkCFOnTiU9PZ0OHTqwaNEioqOjgZIHH/75mUBdu3Z1/rxu3To+/PBDoqOj2bt3b02WXuNONYmHncvxTtfzgERERCrLYhiGYXYRtU1OTg7+/v5kZ2fj5+dndjkAbF2/gvYLLyUPDzwfScPi4mZ2SSIiIrXK+Xx/m/4qDDk3LTr05Jjhgxf5pP+mq0AiIiKVoQBUR3i4ubLDoyMAWVt+MLkaERGRuk0BqA7JCbsAAPf9lXtStoiISEOnAFSH+LTuD0Bk7iawF5tcjYiISN2lAFSHtOkcT47hhRenyNm30exyRERE6iwFoDqkkY8Hu11KnmSdlqzXYoiIiFSUAlAdcyKgPQD5qetNrkRERKTuUgCqY9wiuwDgfXSruYWIiIjUYQpAdUyT2JKZYFGFuyksLDK5GhERkbpJAaiOadqyI6dwL3kx6g4NhBYREakIBaA6xmJzIc2tZCB05o7VJlcjIiJSNykA1UEnA0sGQjsOJJlbiIiISB2lAFQHuf5vILT/8WRzCxEREamjFIDqoPC2JQOhY4p3kVeggdAiIiLnSwGoDmrcrBOFuOBvyWPH9m1mlyMiIlLnKADVRS5upLvFAJC1U0+EFhEROV8KQHVUbqOSgdD2A5oKLyIicr4UgOoot8iuAARk6xaYiIjI+VIAqqPCYvsA0LF4K8ePZZlcjYiISN2iAFRH+cZ0Z6+lKV6WAg6v/MDsckREROoUBaC6ymJhfdBVAHht/9zkYkREROoWBaA6rLhlAgAhOVugKN/kakREROoOBaA6rE1sZw4ZAbhSRHHaGrPLERERqTMUgOqwjk0DSLK2A+Dgxu9NrkZERKTuUACqw6xWCydCewJgT/nZ5GpERETqDgWgOq5x+4sBCM/ZBHa9F0xERORcKADVcd26XcBRwwcPCsjcoddiiIiInAsFoDrO39udXR6dANi/IdHkakREROoGBaB6ID/yQgB89i0xuRIREZG6QQGoHgjvdR0ALfO3kn/soMnViIiI1H4KQPVAy5at2WJphdVikLryU7PLERERqfUUgOoBi8VCWuhAAKy//dfkakRERGo/BaB6wqfL1QA0O7EOTh0zuRoREZHaTQGonujatTu/GZG4YCdz/VdmlyMiIlKrKQDVEz7uLuzy7QXAsW0/mFyNiIhI7aYAVI+4tugHgN8hPRBRRETkbEwPQNOnTycmJgYPDw/i4uJYvnz5Gfump6dz00030aZNG6xWKxMmTCjTZ/bs2fTt25fAwEACAwMZNGgQq1evrsYjqD1adb8Eu2EhvPgAp46kml2OiIhIrWVqAJo/fz4TJkxgypQpbNiwgb59+zJ06FBSU8v/8i4oKCA4OJgpU6bQuXPncvssXbqUG2+8kR9//JFVq1YRFRVFQkICBw4cqM5DqRVimoaz3dYCgD2rNBtMRETkTCyGYRhm7bxXr15069aNGTNmONtiY2MZNmwY06ZNO+u6F110EV26dOG11147az+73U5gYCBvvvkmo0aNOqe6cnJy8Pf3Jzs7Gz8/v3Nap7b4/l8PMDB9Nsl+vYmd9I3Z5YiIiNSY8/n+Nu0KUGFhIevWrSMhIaFUe0JCAitXrqyy/eTl5VFUVESjRo3O2KegoICcnJxSn7rKp2vJdPgWOasx8uvucYiIiFQn0wJQVlYWdrud0NDQUu2hoaFkZGRU2X4eeughmjRpwqBBg87YZ9q0afj7+zs/kZGRVbb/mta5Sy9SjDDcKCZ9rabDi4iIlMf0QdAWi6XU74ZhlGmrqBdeeIGPPvqIzz77DA8PjzP2mzx5MtnZ2c5PWlpalezfDB5uLiT7l8wGO7npC3OLERERqaVczNpxUFAQNputzNWezMzMMleFKuKll17i2WefZcmSJXTq1Omsfd3d3XF3d6/0PmuN2Mvh149pcng5FBeASz06NhERkSpg2hUgNzc34uLiSExMLNWemJhI7969K7XtF198kaeeeopvv/2W7t27V2pbdVGHHgM5ZATgZeSRt10PRRQREfkzU2+BTZo0ibfffps5c+aQnJzMxIkTSU1NZcyYMUDJrak/z9xKSkoiKSmJ3NxcDh8+TFJSEtu2bXMuf+GFF3jkkUeYM2cOzZo1IyMjg4yMDHJzc2v02MwUFeTDL27xAGStWWByNSIiIrWPabfAAIYPH86RI0eYOnUq6enpdOjQgUWLFhEdHQ2UPPjwz88E6tq1q/PndevW8eGHHxIdHc3evXuBkgcrFhYWct1115Va7/HHH+eJJ56o1uOpTU7EDIUd39AobTE47GC1mV2SiIhIrWHqc4Bqq7r8HKDTVu5IJ/aD7gRacnGM+gpr875mlyQiIlKt6sRzgKR6dW8eyk/EAXB4zacmVyMiIlK7KADVU24uVjKbljxk0mvXV2AvMrkiERGR2kMBqB6L6nUFhw1/fIuyMLYtNLscERGRWkMBqB7r17Yp842SJ2CfWv6WydWIiIjUHgpA9Zinm43UmBspNGx4Za6DA+vMLklERKRWUACq53p3acdXjpJnAhlr5phcjYiISO2gAFTPDWoXypcMAKD4t2/B4TC5IhEREfMpANVzPu4uBLTtS67hgWt+FqQnmV2SiIiI6RSAGoArujZjhaMDAI6diX/RW0REpP5TAGoA+rUOZrVLNwByt3xjcjUiIiLmUwBqANxcrLi3HQyAT9ZGyDtqckUiIiLmUgBqIC7o2pnfHJFYcWDs/sHsckREREylANRA9GreiBV0AeD4xq/NLUZERMRkCkANhLuLjaMR/Ut+3vejpsOLiEiDpgDUgER2vphcwwOvomOaDi8iIg2aAlAD0i82wjkdPm/bdyZXIyIiYh4FoAYkIsCTZJ8LAMhP/tbkakRERMyjANTAuLZJACDg6CZNhxcRkQZLAaiB6dKh/e/T4Xd9b3Y5IiIiplAAamDiogP5+X/T4XP0VGgREWmgFIAaGA9XG4fD+gHglvKDpsOLiEiDpADUAIW2788JwxPPomNwcL3Z5YiIiNQ4BaAG6MK2ESx1dAageMsX5hYjIiJiAgWgBqhViA8r3S8EoGjz52AYJlckIiJSsxSAGiCLxYJ3+6HkGe54ntwP6RvNLklERKRGKQA1UJd0juHH/90Gs+s2mIiINDAKQA1U92aNWOFWchusYNNnug0mIiINigJQA2WzWvD6320wr9x9cGCd2SWJiIjUGAWgBmxQlxZ84+gBgH39+yZXIyIiUnMUgBqwHs0a8aP7IAAcmxeAvcjkikRERGqGAlADZrNaiIpL4LDhh2tRDqT8ZHZJIiIiNUIBqIG7Ji6axfaS22CnNn1hbjEiIiI1RAGogWsZ4sOOxgMAsCQvhOJCkysSERGpfgpAQqsLLiPDCMSj6Djs+NbsckRERKqdApBweeemLHT0BSBntWaDiYhI/acAJAR4uZEVcwUAnvuWQtEpcwsSERGpZgpAAsAF8f3YbwThahRQvOsHs8sRERGpVqYHoOnTpxMTE4OHhwdxcXEsX778jH3T09O56aabaNOmDVarlQkTJpTbb8GCBbRr1w53d3fatWvH559/Xk3V1x/9WoewwloyGyxjtc6XiIjUb6YGoPnz5zNhwgSmTJnChg0b6Nu3L0OHDiU1NbXc/gUFBQQHBzNlyhQ6d+5cbp9Vq1YxfPhwRo4cycaNGxk5ciTXX389v/76a3UeSp3nYrNS3HIIAH6pS8DhMLkiERGR6mMxDPPegtmrVy+6devGjBkznG2xsbEMGzaMadOmnXXdiy66iC5duvDaa6+Vah8+fDg5OTl88803zrYhQ4YQGBjIRx99dE515eTk4O/vT3Z2Nn5+fud+QHXc+j2HaPXvzvhaTlF8WyIuUT3NLklEROScnc/3t2lXgAoLC1m3bh0JCQml2hMSEli5cmWFt7tq1aoy2xw8ePBZt1lQUEBOTk6pT0PUuVkIq6xdATi0RrfBRESk/jItAGVlZWG32wkNDS3VHhoaSkZGRoW3m5GRcd7bnDZtGv7+/s5PZGRkhfdfl9msFjIjLgbAddd3JlcjIiJSfUwfBG2xWEr9bhhGmbbq3ubkyZPJzs52ftLS0iq1/7ospOvlFBtWQk7thqMpZpcjIiJSLUwLQEFBQdhstjJXZjIzM8tcwTkfYWFh571Nd3d3/Pz8Sn0aqgs6tGSt0RaAIxsWmlyNiIhI9TAtALm5uREXF0diYmKp9sTERHr37l3h7cbHx5fZ5uLFiyu1zYbEz8OVHQElT4Uu2PJfk6sRERGpHi5m7nzSpEmMHDmS7t27Ex8fz6xZs0hNTWXMmDFAya2pAwcOMHfuXOc6SUlJAOTm5nL48GGSkpJwc3OjXbt2ANx///3069eP559/nquuuoovv/ySJUuW8PPPP9f48dVV7u0vg1X/IvTYOjh1DDwDzS5JRESkSpkagIYPH86RI0eYOnUq6enpdOjQgUWLFhEdHQ2UPPjwz88E6tq1q/PndevW8eGHHxIdHc3evXsB6N27N/PmzeORRx7h0UcfpUWLFsyfP59evXrV2HHVdT26xbFjRRNaWw9watOXePYabXZJIiIiVcrU5wDVVg31OUB/9K9nx3N34VyOBXYk8H5dPRMRkdqvTjwHSGq34k4jKDBcCDy2GVL1FG0REalfFICkXJde0JHP7RcCUJD4lMnViIiIVC0FIClXTJA3P4ePptCw4Z62HPbqNpiIiNQfCkByRsMG9Ga+fQAAxUueAg0XExGRekIBSM7o4rYhfOV/EwWGKy77f4E9P5pdkoiISJVQAJIzslotXN2/B+/bBwHgWDIVHA6TqxIREak8BSA5q2Fdm/Cp53XkGh5Y0zfA2nfMLklERKTSFIDkrDxcbdw2uBcvFg8HwPhuCuxbZXJVIiIilVOhAJSWlsb+/fudv69evZoJEyYwa9asKitMao9ruzVlQ+g1LLbHYbEXwOd3Q3Gh2WWJiIhUWIUC0E033cSPP5YMiM3IyOCSSy5h9erVPPzww0ydOrVKCxTzWa0WHr68IxOKxpJpBMDxfbDmbbPLEhERqbAKBaAtW7bQs2dPAD7++GM6dOjAypUr+fDDD3nvvfeqsj6pJS5o3pjL4lryavG1ABjfPwmZySZXJSIiUjEVCkBFRUW4u7sDsGTJEq688koA2rZtS3p6etVVJ7XKI5e143vPISyzd8JSnA+f3ApFp8wuS0RE5LxVKAC1b9+emTNnsnz5chITExkyZAgABw8epHHjxlVaoNQe/l6uPH11J/5WdA+HDX84nAzfPWx2WSIiIuetQgHo+eef51//+hcXXXQRN954I507dwZg4cKFzltjUj8ltA/jgk5tmVh0b0nD2jmw7UtzixIRETlPFsOo2PsN7HY7OTk5BAYGOtv27t2Ll5cXISEhVVagGXJycvD39yc7Oxs/Pz+zy6l1juQWMOiVZdxZ+B/udVkIHv4w5mcIiDK7NBERacDO5/u7QleATp06RUFBgTP87Nu3j9dee43t27fX+fAjf62xjztPXNmeV4qvY4OjFeRnw6e3Q2Ge2aWJiIickwoFoKuuuoq5c+cCcPz4cXr16sXLL7/MsGHDmDFjRpUWKLXTlZ0jGNCuCeOLxnICL9i/GuZeCQW5ZpcmIiLylyoUgNavX0/fvn0B+PTTTwkNDWXfvn3MnTuXN954o0oLlNrJYrHw4nWdsAY249aCB8i1+ML+NfDZXeCwm12eiIjIWVUoAOXl5eHr6wvA4sWLueaaa7BarVxwwQXs27evSguU2ivAy41Zo+LY6tKeUfl/o9jiBtu/hiWPm12aiIjIWVUoALVs2ZIvvviCtLQ0vvvuOxISEgDIzMzUoOEGpm2YHy/+XyfWG62ZVHBnSePKf8K6f5tbmIiIyFlUKAA99thjPPDAAzRr1oyePXsSHx8PlFwN6tq1a5UWKLXf5Z0iGNO/BQsdfXjTUfKkaL6eBLu+N7cwERGRM6jwNPiMjAzS09Pp3LkzVmtJjlq9ejV+fn60bdu2SousaZoGf/7sDoNb31vDTzsyme09k0vsy8HmBsNmQMfrzC5PREQagPP5/q5wADpt//79WCwWmjRpUpnN1CoKQBVzPK+QK99cwaGjx/l3wNtckP9zyYL/ew/aX21qbSIiUv9V+3OAHA4HU6dOxd/fn+joaKKioggICOCpp57C4XBUqGip+04PinZ19+Km42NY5ndFyYJvHix5VpCIiEgtUaEANGXKFN58802ee+45NmzYwPr163n22Wf55z//yaOPPlrVNUod0jbMj1mj4nCxuXBn5nUcdmsKuYfg3cv09ngREak1KnQLLCIigpkzZzrfAn/al19+yb333suBAweqrEAz6BZY5X27JZ17P1hPe/bwkffL+BQfA5s73DQfWgwwuzwREamHqv0W2NGjR8sd6Ny2bVuOHj1akU1KPTOkQzjTrunIZqM5A3OfYqdPT7AXwEc3wqaPzS5PREQauAoFoM6dO/Pmm2+WaX/zzTfp1KlTpYuS+mF4jyheuLYTmZZGXJY1lm0+vaH4FHx2JyQ+ZnZ5IiLSgLlUZKUXXniByy67jCVLlhAfH4/FYmHlypWkpaWxaNGiqq5R6rDre0Ti7mpl0scbuTzrXt4Ib8nlx+bCitfB3Rd63wcu7maXKSIiDUyFrgD179+fHTt2cPXVV3P8+HGOHj3KNddcw9atW3n33Xerukap467q0oS3buqKi82FcelD+NTn5pIFPzwNsy6CI7tNrU9ERBqeSj8H6I82btxIt27dsNvr9sswNQi6eqzclcWdc9dyqrCIyQGJ3Gb5CtupI+AdAle+Aa2HgMVidpkiIlJHVfsgaJGK6N0yiI/HxBPs58kzxwdzedHznGoUCycz4aMbYMkTZpcoIiINhAKQ1Kj2Ef58fm8f2oT6kpzrxYCsB9jXcmTJwhWvwef3QNEpU2sUEZH6TwFIalxEgCef3BNPn5aNySj0pP+WoSxtNhHDYoWNH8L71+nJ0SIiUq3OawzQNddcc9blx48fZ9myZRoDJOeksNjB89/+xjs/pwAwqdUhxmc+hqXgBDRuBYOfgdaDTa5SRETqimp7Geqtt956Tv3q+kwwBaCa9Z9Ve3niv9uwOwwGBmQw0zIN11OHSxb2uR8umWpugSIiUifU6Nvg6yMFoJq3bt8x7vtoAweOnyLYlse7LZbRPvV9LBhwxesQN9rsEkVEpJarU7PApk+fTkxMDB4eHsTFxbF8+fKz9l+2bBlxcXF4eHjQvHlzZs6cWabPa6+9Rps2bfD09CQyMpKJEyeSn59fXYcgVSAuOpBF9/XlknahHLZ7cfmOoSxsdFvJwkV/h7Q15hYoIiL1iqkBaP78+UyYMIEpU6awYcMG+vbty9ChQ0lNTS23f0pKCpdeeil9+/Zlw4YNPPzww9x3330sWLDA2eeDDz7goYce4vHHHyc5OZl33nmH+fPnM3ny5Jo6LKkgfy9XZo2M47HL2+Fqs3D/wYtZ4dob7IXw3/vB4TC7RBERqSdMvQXWq1cvunXrxowZM5xtsbGxDBs2jGnTppXp/+CDD7Jw4UKSk5OdbWPGjGHjxo2sWrUKgHHjxpGcnMz333/v7PO3v/2N1atX/+XVpdN0C8x86/Yd5a656yg+eZSfPSbgSx5c8zZ0+j+zSxMRkVqqTtwCKywsZN26dSQkJJRqT0hIYOXKleWus2rVqjL9Bw8ezNq1aykqKgLgwgsvZN26daxevRqAPXv2sGjRIi677LIz1lJQUEBOTk6pj5grLroRX47rQ3hYOLOLLgXAvrZuD64XEZHaw7QAlJWVhd1uJzQ0tFR7aGgoGRkZ5a6TkZFRbv/i4mKysrIAuOGGG3jqqae48MILcXV1pUWLFgwYMICHHnrojLVMmzYNf39/5ycyMrKSRydVoWmgFx+Piednn5LQa01dCdn7Ta5KRETqA9MHQVv+9O4nwzDKtP1V/z+2L126lGeeeYbp06ezfv16PvvsM7766iueeuqpM25z8uTJZGdnOz9paWkVPRypYn4ertx6aV9+dbTFgoFj6xdmlyQiIvWAi1k7DgoKwmazlbnak5mZWeYqz2lhYWHl9ndxcaFx48YAPProo4wcOZI77rgDgI4dO3Ly5EnuuusupkyZgtVaNvO5u7vj7u5eFYcl1SChfSj//LwHvfiN41sSadR7nNkliYhIHWfaFSA3Nzfi4uJITEws1Z6YmEjv3r3LXSc+Pr5M/8WLF9O9e3dcXV0ByMvLKxNybDYbhmGgRx7VTe4uNtxbXgSAV8ZqsBebW5CIiNR5pt4CmzRpEm+//TZz5swhOTmZiRMnkpqaypgxY4CSW1OjRo1y9h8zZgz79u1j0qRJJCcnM2fOHN555x0eeOABZ58rrriCGTNmMG/ePFJSUkhMTOTRRx/lyiuvxGaz1fgxStVo36032YYXHo48jIMbzC5HRETqONNugQEMHz6cI0eOMHXqVNLT0+nQoQOLFi0iOjoagPT09FLPBIqJiWHRokVMnDiRt956i4iICN544w2uvfZaZ59HHnkEi8XCI488woEDBwgODuaKK67gmWeeqfHjk6oT3zKUZUYHhlhWk/3rfwiI7GF2SSIiUofpVRjl0HOAaqdp0//F5Mx/UGz1wOWBZPBqZHZJIiJSi9SJ5wCJnK+wTpewxdEMF0c+rHnb7HJERKQOUwCSOmNA21BmFV9e8suPz8CGD8wtSERE6iwFIKkzmgV5szXwYnY7wksavrwXfvva3KJERKROUgCSOuXGC2K4tvAJfrL1KmlY9HcoLjC3KBERqXMUgKROGdErGhefxtx58h5OuodAzgHY9LHZZYmISB2jACR1iqebjQcS2lCAG9NP/e/FuMtegBOHzC1MRETqFAUgqXOu7x5Jz2aN+HfhRaRbQiA7FWb2gc2fml2aiIjUEQpAUudYrRZm3NyNwEaNGZ4/mYO2JnDyMCy4HZa/YnZ5IiJSBygASZ3U2MedObf04JhbE/qfnMa3jW4uWfDjM5Cx2dziRESk1lMAkjqrVagv02/uhmF1ZczBoST59ANHMXx2N+TnmF2eiIjUYgpAUqf1bRXMWyO64WqzckfWjZyw+kHmVni1PSy4Eza8D3rbi4iI/IkCkNR5g9uHMWNEHLmugdydP46DhEBBDmz+GL4cC5vmm12iiIjUMgpAUi8MahfKp2N6czCwJ/3yX+Lt4kt/X/j53fBSG1j1lnkFiohIraIAJPVGhyb+fHVfX67q1oyni2+mRf5/SPbsVrIwNwMSH4Ps/eYWKSIitYICkNQrPu4uvPR/nXh6WAesNheuOTaO+cYlJQsdxfDVJDiRYW6RIiJiOgUgqXcsFgs3XxDNZ/f0ITYqjAcLbuWGwkdwYIWd38EbXWHpc1CQa3apIiJiEgUgqbc6NvVn3l3x3N2vOWtoz3UFj7HBaAVFebB0GvwzDlKWm12miIiYQAFI6jU3FyuTL41l0X19cWl2AVcXPME9hfdzwBJWMi7oP8NgzduaKi8i0sBYDEP/8v9ZTk4O/v7+ZGdn4+fnZ3Y5UkUMw+DLpIM8/XUyJ3JP8ILrLK6yrSxZ2KQ7dBsFXW4Cm6u5hYqISIWcz/e3AlA5FIDqt5z8Il5L3Mm/V6Vwm+Ur/ubyCR6WopKF4Z3hhg/Bv6m5RYqIyHlTAKokBaCGITk9h0e+2ML+fbu5yraC8e5f4es4AS6e0HcS9LobPPzNLlNERM6RAlAlKQA1HMV2B2/8sIuZS3cT4sjgddfpxFl3lCy0ukLCU3DBPeYWKSIi50QBqJIUgBqefUdO8tRXySxJzuAq6wr+5vopUZbMkoXRF0LLiyF+HLi4m1uoiIickQJQJSkANVw/7TjMS4u3s2n/ce62fcXfXebjYnH83iEqvuTWWPurzStSRETKdT7f3y41VJNIndCvdTD9WgezZu9RXvquMZfv7cy1tp8YZUvE3VIEqatKPlm7SmaM+Tcxu2QREakAXQEqh64ACZRMm/95VxYvfbedjP0ptLPu4xLrWm5y+fH3TtEXQruroNmFENrOvGJFRES3wCpLAUj+yDAM1uw9xrsrUvh2azrXWZdxp+1rWloPYuUP//dpeQl0HQFtL9ezhERETKAAVEkKQHImqUfy+CLpAO+t3IvnyQNcZ/uJeJffuMCy9fdOno2gSTdoNwy6jTStVhGRhkYBqJIUgOSvnCq0s2hzOvPWpLJm71EecvmIgdYNRFiP4E1+6c6XTIU+95tTqIhIA6IAVEkKQHKuDMNgQ9pxFiYd5KtN6ZCbyeOu/+YK2y+lO7p4QmRP6DQcwjpCeCdzChYRqccUgCpJAUgqotjuYNWeI3yZdJBvt6TTrnALd7p8zSW29WU7Nx8AIe2gxcXQalDNFysiUg8pAFWSApBUVn6RnR9+y+SL9ftJ27ERm+MUN9uWcIPL0rKdWw4qmUnW8f8g9Rc4sLbkoYuunjVet4hIXaYAVEkKQFKVsvOK+GZLOl8kHWBtShZTbP8h2nIIAwsDbRuc/QybOxZ7QckvTeJKxg5FxYPVZlLlIiJ1iwJQJSkASXXJLShm7d6jLFh/gG+3pBPr2EVf62ZudllCuOVo2RV8wqDLjdBrDPiGQWEeZO2AiC41XruISG2nAFRJCkBSE7Lzitidlcu2gzl8uXoHHhlrOWAEMcq2mEG29QRaT+FtnATA8GqMpdcY2PQxHNkJl74EPe80+QhERGoXBaBKUgASM+w4dIL3f9nH98mZHDh+CleKudi6ngkunxFrTS27QrthUJgLw2bCwfUQGAPBrWu8bhGR2kIBqJIUgMRsqUfy+DXlCJ9vOMDmvencwtdcYN3GbiOCW1wSy1/JOwTGry15T1loe3D1qNmiRURMdj7f39YaqumMpk+fTkxMDB4eHsTFxbF8+fKz9l+2bBlxcXF4eHjQvHlzZs6cWabP8ePHGTt2LOHh4Xh4eBAbG8uiRYuq6xBEqlxUYy/+r3skH955Aasfv5Luo57lqy4zedXtLsYVjmetozVJjualVzqZCc9FwdsXwzOh8HJbOLDOnAMQEanlTL0CNH/+fEaOHMn06dPp06cP//rXv3j77bfZtm0bUVFRZfqnpKTQoUMH7rzzTu6++25WrFjBvffey0cffcS1114LQGFhIX369CEkJISHH36Ypk2bkpaWhq+vL507dz6nunQFSGqrIruDTfuzWbkri682pXMycw/PuLxDe+tegiw5ZVcI7wx3/AD2QsjYBL/OhDaXQqfra754EZFqVmdugfXq1Ytu3boxY8YMZ1tsbCzDhg1j2rRpZfo/+OCDLFy4kOTkZGfbmDFj2LhxI6tWrQJg5syZvPjii/z222+4ulbshZQKQFJX7Dx0gv9uSuerTQdpdeRHbnX5jm/sPfGy2XnQ+j4Ahd4RuBYex1KUV7KSZyOYsLnkmUPN++vFrSJSb9SJAFRYWIiXlxeffPIJV199tbP9/vvvJykpiWXLlpVZp1+/fnTt2pXXX3/d2fb5559z/fXXk5eXh6urK5deeimNGjXCy8uLL7/8kuDgYG666SYefPBBbLbyn6dSUFBAQUGB8/ecnBwiIyMVgKTOMAyD3zJO8NWmkldy7DuSxyDrOl5xnYGfJe/MK8aPg8HP1FyhIiLV6HwCkEsN1VRGVlYWdrud0NDQUu2hoaFkZGSUu05GRka5/YuLi8nKyiI8PJw9e/bwww8/MGLECBYtWsTOnTsZO3YsxcXFPPbYY+Vud9q0aTz55JNVc2AiJrBYLMSG+xEb7scDCW3YciCHpdtb88je3lj3LuOY3ZOfHB2Z5v5vbrD8YRD1qjdLptMveRKO7ILgNtC0J/S6y7yDERGpAaYFoNMsFkup3w3DKNP2V/3/2O5wOAgJCWHWrFnYbDbi4uI4ePAgL7744hkD0OTJk5k0aZLz99NXgETqIovFQsem/nRs6g+0IjuvPwvW7yd52W7ezR3IILdfyTG8aG79339ovP6HsXEZm2DzJyWv5vANLXf7IiL1gWkBKCgoCJvNVuZqT2ZmZpmrPKeFhYWV29/FxYXGjRsDEB4ejqura6nbXbGxsWRkZFBYWIibm1uZ7bq7u+Pu7l7ZQxKplfy9XLntwhhu6d2M7Rk9WbAzgflr0yjMSuFD12eIsh4uu9Km+dD5Rlj+EhzZDZe+AI2al+0nIlJHmRaA3NzciIuLIzExsdQYoMTERK666qpy14mPj+e///1vqbbFixfTvXt354DnPn368OGHH+JwOLBaS2b579ixg/Dw8HLDj0hDYbNaaBfhR7sIP+7q15wNaZ2Zv74d9i1fcORUMT/bO3KDyw/c7/I5JD5a8jlt6fNwzb/g51ehuAD6PwhnuVIrIlLb1Ypp8DNnziQ+Pp5Zs2Yxe/Zstm7dSnR0NJMnT+bAgQPMnTsX+H0a/N13382dd97JqlWrGDNmTKlp8GlpabRr147Ro0czfvx4du7cyW233cZ9993HlClTzqkuzQKThsThMPh5VxZzV+3jl9/2Mdf1WbpZd5XqY7h4YLnhQ3j/mpIGryBoMwTc/Uqm1Ed0NaFyEZHS6sQssNOmT5/OCy+8QHp6Oh06dODVV1+lX79+AIwePZq9e/eydOlSZ/9ly5YxceJEtm7dSkREBA8++CBjxowptc1Vq1YxceJEkpKSaNKkCbfffvtZZ4H9mQKQNFSZJ/JZm5JF7rqP8dz3A+8VXMwzrnNoa00780qRF8Dt39VckSIiZ1CnAlBtpAAkAnmFxXy1MZ2VK35gzNEXzx6CJu8Hd9+Sn1N/gYIT0OqSmilUROR/FIAqSQFIpLTdh7LZsWweW3alcEP+p+TgxX/t8TzkOg+A1P6vEtV/NORmwCuxJSslPAOZyRB7RcntMhGRaqYAVEkKQCLlszsMlm3P4OM1qSz57QhPWN/mZpfvAThuCcDTFdwLj5deyScUJv0GVtNfPSgi9VydeBCiiNQ9NquFi2PDuTg2nKzcAlb+6sXOtVlE5m0jgONQWM5KuYfg0OaS95Kd5nBAQQ54+Gs2mYiYQv9JJiIVEuTjzpUDL6LVgz9xYuJelrb4B99Z+zKl6DayDS+OGj5stpbcDkv97p8UFhWXrOhwwJwEeD4a3h4I9mITj0JEGirdAiuHboGJVEyR3UHitkOsWp/Eij3H6V68jhdcZwNwFD82Rd5Mp+gQGv38xO8rjV4EzfqYU7CI1Cu6BSYipnC1Wbm0YziXdgwnt6CYz9b25OeNJ+mW+RmNyOGitOnwp8lkxvZFWP4cgH6ZCTu/g973QYsBNXcAItJg6ApQOXQFSKRqFRfmk5z4Ho03vElo0X6WOrrwraMHL7rOIgcf1re4h9ZDxhARHATbv4WPhpesaHWBsauhcQvIzwY3Xw2mFpEz0iywSlIAEqkmhkFaVg4frUvnq7W7mF34IG2s+wE4Zbix2rs//fMSS68z5LmSt9R/dGPJS1oTnoakDyFuNHgG1PghiEjtpQBUSQpAItXPMAyO55xg75JZhG97mzB7unNZihHOlsZDuOLouxz0bk/Eya2/rxjZC9J+hT4TSp4xlLUDutxU8wcgIrWOAlAlKQCJ1DDD4PCGrzj10z8pzk5nUv4dFOLCIveHz7xOaMeS6fUANy+AloNqplYRqbU0CFpE6haLheBuV0C3KzAMgyf3Z7No0wH2bGxD88LtOAwLB4wgIq2Hf1/ndPgB2Lnk9wCUsRm+exgat4Lo3iW3zWyuNXs8IlLr6QpQOXQFSKSWKMwjf+1/WJ3lxpadKdx74nV2OppgxUEL6++3zHJ8W1B01woaW3JhVn/IOfD7Ni57BXrcbkLxIlLTdAVIROoHNy88et9NP6CfvZj81U055NaT45sW0WLf885ufid2k/piR7JcA2ljLwk/hosnluJTsPsHBSARKUMBSETqBpsLHvF3cSFAXBfIuJKUPdvZvj2ZC1OnE2U9DPbD2A0LVxY+TQAWPuBh7Ck/Y3M4NH1eREpRABKRuimsAzFhHYjpDRROImf9p+RtWMBKa1cys9ry24k8ct098Ck4zjsv/4OmA+/mki4tsVr/8O6xY/vAP1LhSKQB0higcmgMkEjdZncYJG47RNBXt9I9fyVQ8pyh790GYL/kKS6La4XL+jnw9d9KnivUe7zJFYtIVdA0+EpSABKpJ04dJ3flbArXzKVRfioAOx1NeNtzNM8XPPN7vyeyTSpQRKqSAlAlKQCJ1DOGQd7273F8NgafwsNlFifduIHOrWOwWCzlrCwidcX5fH/rxreI1H8WC15tB+Fz30qKO97IUe+WHMXfuTjmwz5MfGkm7/ycwtGThSYWKiI1RVeAyqErQCL1X0GxnbQFj9AyebqzbYW9Pe8bQ3GJHcLwnjH0btG49KBpEanVdAuskhSARBoIhwP2r6Fw9bu4bJmPFQcAaY5g5tov4WffoVzaI5b/6x5JmL+HycWKyF9RAKokBSCRBuh4Gqx5m+K17+FScByAPMOdTUZzvrZfwKGW13Ndz+YMaBuCq02jB0RqIwWgSlIAEmnACvNg8yc4fpmJ9fA2Z/N+I4h1jtYkug0kqvtQ7jQ+I3DnpyVvpE942sSCReQ0BaBKUgASEQwDMjbB3p+x//QytlNHnIsyjQBCLMcBcGAlbdQvRMW01iwyEZNpFpiISGVZLBDeGeLHYhu/Fq59B3uPOyl28XKGHwArDna9ezd3zl7K4RMF5tUrIudFV4DKoStAInJG+dmw6WOOGj6sSs3jsq2TAPjV0ZYP2vyTN0b0NLlAkYZLb4MXEakuHv7Q804aAZf1Ato1xv75vfQq/o2s3x7lQNZ8mgQFmFykiPwV3QITEamM9sOw/d87FOPCZbZf2PPZVLMrEpFzoAAkIlJZbYayPf4FALod+IDcYxkmFyQif0UBSESkCsQOupWd1hi8Lfls+fZds8sRkb+gACQiUgWsNisnW14BQOGuH7E7NL9EpDZTABIRqSJt4y8HoHPxZj76NcXkakTkbBSARESqiEdUHIUuPvhb8lj7zX9IO5pndkkicgYKQCIiVcXmgmvcKACesbzFjHmf49CtMJFaSQFIRKQKWRKmciqyL96WAsYfeoT//LDO7JJEpBwKQCIiVcnmiudN75Pt3Yxwy1Eif3qApdsOmF2ViPyJApCISFXzDMDv5rkUW1y52LqB/Hmj+XLdXrOrEpE/MD0ATZ8+nZiYGDw8PIiLi2P58uVn7b9s2TLi4uLw8PCgefPmzJw584x9582bh8ViYdiwYVVctYjI2VnCO2NcP5ciXBliXY3lizG8sWQHev2iSO1gagCaP38+EyZMYMqUKWzYsIG+ffsydOhQUlNTy+2fkpLCpZdeSt++fdmwYQMPP/ww9913HwsWLCjTd9++fTzwwAP07du3ug9DRKRcrrGXYrvpI+wWG1faVmFd+jR//ziJwmKH2aWJNHimvg2+V69edOvWjRkzZjjbYmNjGTZsGNOmTSvT/8EHH2ThwoUkJyc728aMGcPGjRtZtWqVs81ut9O/f39uvfVWli9fzvHjx/niiy/OWEdBQQEFBQXO33NycoiMjNTb4EWkavw6C775OwCf2vvxWdPJzBjZA38vV5MLE6lfzudt8KZdASosLGTdunUkJCSUak9ISGDlypXlrrNq1aoy/QcPHszatWspKipytk2dOpXg4GBuv/32c6pl2rRp+Pv7Oz+RkZHneTQiImfR6y648k0Mi43rbD9x+/6HufWtRew7ctLsykQaLNMCUFZWFna7ndDQ0FLtoaGhZGSU/yLBjIyMcvsXFxeTlZUFwIoVK3jnnXeYPXv2OdcyefJksrOznZ+0tLTzPBoRkb/QbSSWa2bhsLkz0LaBmbn38cw/p/OfVXvJL7KbXZ1Ig+NidgEWi6XU74ZhlGn7q/6n20+cOMHNN9/M7NmzCQoKOuca3N3dcXd3P4+qRUQqoON1WIPbUvzJbYQc2c4sniZx0SJGLb6JuF59GXlBNBEBnmZXKdIgmBaAgoKCsNlsZa72ZGZmlrnKc1pYWFi5/V1cXGjcuDFbt25l7969XHHFFc7lDkfJYEMXFxe2b99OixYtqvhIRETOQ1gHXO5eiiPxMVjzDpfY1jPQsYHvV3RlyvJL8IxNYHSf5vRoFnjW/xgUkcox7RaYm5sbcXFxJCYmlmpPTEykd+/e5a4THx9fpv/ixYvp3r07rq6utG3bls2bN5OUlOT8XHnllQwYMICkpCSN7RGR2sHNC+tlL2EdtxpHu6uxWgwusa3nXdfnGbVjHLNmv8kVry/j47Vpuj0mUk1MnQU2f/58Ro4cycyZM4mPj2fWrFnMnj2brVu3Eh0dzeTJkzlw4ABz584FSqbBd+jQgbvvvps777yTVatWMWbMGD766COuvfbacvcxevTov5wF9mfnM4pcRKTSMn+D9f/GseYdrPaSGam7HeG8UXw1P7tdyPjIPQzw2kN4p4G4tb/c5GJFaq/z+f42dQzQ8OHDOXLkCFOnTiU9PZ0OHTqwaNEioqOjAUhPTy/1TKCYmBgWLVrExIkTeeutt4iIiOCNN944Y/gREakTQtrCkGlYL7gXfp2JI+lDWpxK53W36cB0+N+8jKLf/s1jzf9Dt05dGNAmRNPoRSrB1CtAtZWuAImIqQpOwC8zMda8jSU3Awc2rJTcCltlb8eU4tvYZ2lCz2aNGNQulEtiQ4lq7GVy0SLmO5/vbwWgcigAiUit4LBD1g7wCMCRkwFzLsHqKMKOlaX2znxm78tKRzuO4UfrUB8GxobSt1UQcdGBuLvYzK5epMYpAFWSApCI1ErpG2Hpc7B9kbPJgZWfHJ1YYu/KMkcnDhsBjHddSA+fw2yPvAG/dhcT37wxIX4eJhYuUjMUgCpJAUhEarWsnbDxI0j+CrK2n7FbmiOYiwpfwY6NViE+9G7RmPgWQVzQvBEBXm41WLBIzVAAqiQFIBGpM7J2wrYvYNcPkPYrGKWnzR+0hvN24SB+tHchxQgHwGKBtmF+9GwWSI+YRnSPbkSYv64QSd2nAFRJCkAiUiflHYXkhRDSDvYuh++nllp8xL0py4yuLMmNYZsRzV4jDCh52GKTAE+6RQfSPTqQuOhA2ob54mIz7VFxIhWiAFRJCkAiUufZi2H393B4O+xaAvtWgqOoVJdjbhGssnTi69w2rLTHcozf/73zdLXRsak/HSL86dDEjw5N/GkZ7IPVqqdTS+2lAFRJCkAiUu/k58CepbBzMWRug/RNZQLRcY9Ikq2tSMxrwYL8HmTjU2p5kI87LUO86dw0gK5RAXRsGkCEv4de2SG1hgJQJSkAiUi9V5ALe3+G3T+U3C7L3FamS757Y/a7t2S9oxVLspuypqhZqatEAL4eLvRs1oj+bYLp0MSfduF+eLhqCr6YQwGokhSARKTByTsKBzfAgXWwZQEc/q3cbrmeTUhxa82vBc344URTNtqbcZLf32DvZrPSLsKPrlEBdI0KpFtUAE0CPHWVSGqEAlAlKQCJSIOXnw1Zu+DgekhbXfK/R3aV6WZg4ZBPO5a69mFZdhhL85pxitIzyoJ93ekaGUDnyAA6Nw2gY1N//D31Gg+pegpAlaQAJCJSjlPHIT0JDqwvCUQHkyA7rVQXh6sX2d7NSbVE8HNRa1YcC2S1vRXFf3r1ZLPGXnRqGkCnpv50ahpAhyZ+eLmZ+npKqQcUgCpJAUhE5BzlpMNvX8GObyEzGXIOlOlS7ObHAd9O7HQ04fuTLdh0wpvtRmSpUGS1QOtQX7pGlVwl6hwZQKsQH03Fl/OiAFRJCkAiIhXgcJSMHTq2Fw6sLZlpdnAD5GWV6Vro0ZidAReyO9+f5bnhLMuNIpPAUn08XW10bOJP50h/OkcG0CVS44nk7BSAKkkBSESkijjsJSEofSNkbC6ZcZabCQU5ZbrmewRz0L0Fp07l8VlBD94ruAg7pWeUBfm4Oa8QlYwp8tdrPcRJAaiSFIBERKqRvQh2fV/y6o7czJKAdDgZDEepbobVhVzPJmz27MmK/GYkHg1llyMMB7/fFrNYoE+LIJ67tiNNA71q+kikllEAqiQFIBGRGlZ4suQKUcZmyNgEWz6Dwtwy3ewuXmR5t+I3a0uW5TXj15zGbDWi6dQ0gE/H9MbNRWOGGjIFoEpSABIRMZnDAbkZsH9tyROsMzZBxhYoPlWm63fEMz5/DLf0bc2Uy9rVfK1Sa5zP97fmHIqISO1jtYJfBLS7suQDJe83O7Lrf1Px1zk/g1nFFBcfHl9+Kxc0b8zA2FBTS5e6QVeAyqErQCIidcT2b+Gj4QDcVziOpW79eO+2nnSLCvyLFaU+Op/vb90sFRGRuqvNEOhzPwAvu81kQOFSbpr9C99uSTe5MKntFIBERKRuG/gEdLgWV4p53W06Ixz/Zez7axj34XoO5eSbXZ3UUgpAIiJSt1mtcM3bcMG9ADzq+gHfuD3Eoc0/0v/FH5i2KFlBSMrQGKByaAyQiEgdZBiw8p/w86tw6igAax2tmVZ0Ixtow4A2IQzvEcmAtiG46hUb9ZKmwVeSApCISB128ggseRxj8ydYikuu/Kx1tOY7e3fm2S/G3SeQ6+KaMrxHJDFB3iYXK1VJAaiSFIBEROqBnHRYOg02vA+GHYB83NjsaMYn9v58au9Px6aBXNQmhCu7RNAi2MfkgqWyFIAqSQFIRKQeyUmHbV/C2ncga4ezeb8RxCp7O5Y4uvGjoyutIhpzUZtg+rcOoVtUgN5EXwcpAFWSApCISD3ksEPWTtjxLSx/BQqynYuOGT6scHTgF0csSx2dyXaPoG+rIC5qHUK/1sGE+XuYWLicKwWgSlIAEhGp5wpyIfUX2PMjbFkAJ0o/N2iXI4Lljo6scHTgV0csTcJCuahNCP1bB9O9WaAGUddSCkCVpAAkItKAOOwlb6bfuwL2/IiR+guW/40ZAig2rPzs6MhKRzvWO1qx0y2WC1qG0L91CD1jGtE00BMPV5uJByCnKQBVkgKQiEgDduoYpPwEe5aVvIj16O5Si3MMT1Y4OvCToxO/OmI5ZAujU1QwfVo2pmdMYzo28cfTTYHIDApAlaQAJCIiTkd2w6aP4XAyRspyLP97xtBpDsPCNiOa7x3dWO9oxWZa0iQ8gm5RAXSNCqRbVCCRjTyxWCwmHUDDoQBUSQpAIiJSLocdDibB7u9h1xKM9E1Yik+V6bbT0YQNjpasN1qxxtGGbK9mdIlqRNeoALpFBdKpqT/e7i41X389pwBUSQpAIiJyTgwDTmTAzu9g3yqM/auxHN1TppvDsHAYf9Y42rDG0ZZUQskP6kTzZs3oFhVI50h/mgf5YLXqKlFlKABVkgKQiIhU2Mks2L+m5JO2GiNtNRZ7QbldtzuassrRjs2O5uS4BmENa0ezqBg6RQbSIsSblsE+eh7ReVAAqiQFIBERqTJFpyDvKBxLgdRVsH8tRVkpuB7dXm73I4Yv2x2RbDciSXGJ4YRfK9zC29OqaSjtIvxoH+6Pv5drDR9E3aAAVEkKQCIiUu1OZsHen2HfChyHd1B0NBXX7L1YcZTp6jAspBohbDci+c2I4rBXCwobx+IZGE6ziDA6Ng2gVagv/p4NOxgpAFWSApCIiJii6BQc/g0ObcNxaCt5aZtwPZKMe37WGVfJMvxIdkSRbERzyD2KosBWuIe1JTwsgubB3rQI9iHc36NB3EqrUwFo+vTpvPjii6Snp9O+fXtee+01+vbte8b+y5YtY9KkSWzdupWIiAj+8Y9/MGbMGOfy2bNnM3fuXLZs2QJAXFwczz77LD179jznmhSARESkVjmZBYe2QuY2Cg9spvDgZjyO78TFXnYG2mlZhh+7jQh2OyJIIYJiz2A8A8PwiGhHeJNmtAj1JaqRF4293erNFP3z+f42dQ7e/PnzmTBhAtOnT6dPnz7861//YujQoWzbto2oqKgy/VNSUrj00ku58847ef/991mxYgX33nsvwcHBXHvttQAsXbqUG2+8kd69e+Ph4cELL7xAQkICW7dupUmTJjV9iCIiIpXnHQTN+0Pz/rgBblAyA60or+SKUcYWCg9uIT/jN1yO7MArP4MgSw5Blhx6WX8r2UYRkFnyyd3gwX4jmG1GALlWHzK8WnPSvy3+gY3xC40hKCKGqMbeRATU3ytHpl4B6tWrF926dWPGjBnOttjYWIYNG8a0adPK9H/wwQdZuHAhycnJzrYxY8awceNGVq1aVe4+7HY7gYGBvPnmm4waNarcPgUFBRQU/D5CPycnh8jISF0BEhGRuqkgF47shKydGJnbyc/cQWHOYTiRju/J1HLHGf3RISOANCOEDBqT4xpCnkcolsYtaBTSBN8mbQkJDibc35Mgn9p19ahOXAEqLCxk3bp1PPTQQ6XaExISWLlyZbnrrFq1ioSEhFJtgwcP5p133qGoqAhX17KDv/Ly8igqKqJRo0ZnrGXatGk8+eSTFTgKERGRWsjdByK6QkRXLIDn/z5AyTij7ANw+DeKTh4lOysd+8Ek3I7ugMI8fAsPEWo5TqjleEl/O3Dyf59UYC0cM3zINALYQSBH3MLJ9InFPTCcgKAIfBuF49M4nMaBgUQ28qq1V5BMC0BZWVnY7XZCQ0NLtYeGhpKRkVHuOhkZGeX2Ly4uJisri/Dw8DLrPPTQQzRp0oRBgwadsZbJkyczadIk5++nrwCJiIjUO66eENQSglriCgT9eXlhHhzaiiP7ANmHUig8uh/H8TRcju3GteAYAfYjBFpyCbTk0ob9ULwZji+G40DK75s5ZbiRTgAHreEcdwsFd39y/Frh6huMW6OmBER1oHebiBo77D8z/Tncf750ZhjGWS+nlde/vHaAF154gY8++oilS5fi4eFxxm26u7vj7u5+PmWLiIjUT25eENkDa2QPAjuUs/zUcTiRTtHxg+QcTqPwwCaMrJ1w8ghuBUfwLT6GOwV4WgqJJJNIIxMKKPnk/L6ZA6si4LHkcnZQM0wLQEFBQdhstjJXezIzM8tc5TktLCys3P4uLi40bty4VPtLL73Es88+y5IlS+jUqVPVFi8iItJQeQaAZwCuIbE0bn2GPoUncZzI5EjGPvIztlN4PJ3inEN4Hd+BtTCHwIIDnPBrVZNVl2FaAHJzcyMuLo7ExESuvvpqZ3tiYiJXXXVVuevEx8fz3//+t1Tb4sWL6d69e6nxPy+++CJPP/003333Hd27d6+eAxAREZHyuXljbRxDcOMYaH9R2eWGQduCEzVe1h+ZOjJp0qRJvP3228yZM4fk5GQmTpxIamqq87k+kydPLjVza8yYMezbt49JkyaRnJzMnDlzeOedd3jggQecfV544QUeeeQR5syZQ7NmzcjIyCAjI4Pc3NwaPz4REREph8UCHubOsjZ1DNDw4cM5cuQIU6dOJT09nQ4dOrBo0SKio6MBSE9PJzU11dk/JiaGRYsWMXHiRN566y0iIiJ44403nM8AgpIHKxYWFnLdddeV2tfjjz/OE088USPHJSIiIrWb6U+Cro30JGgREZG653y+v2vn5HwRERGRaqQAJCIiIg2OApCIiIg0OApAIiIi0uAoAImIiEiDowAkIiIiDY4CkIiIiDQ4CkAiIiLS4CgAiYiISIOjACQiIiINjgKQiIiINDimvgy1tjr9erScnByTKxEREZFzdfp7+1xec6oAVI4TJ04AEBkZaXIlIiIicr5OnDiBv7//WfvobfDlcDgcHDx4EF9fXywWS5VuOycnh8jISNLS0vSm+Wqk81xzdK5rhs5zzdB5rjnVca4Nw+DEiRNERERgtZ59lI+uAJXDarXStGnTat2Hn5+f/s9VA3Sea47Odc3Qea4ZOs81p6rP9V9d+TlNg6BFRESkwVEAEhERkQZHAaiGubu78/jjj+Pu7m52KfWaznPN0bmuGTrPNUPnueaYfa41CFpEREQaHF0BEhERkQZHAUhEREQaHAUgERERaXAUgERERKTBUQCqQdOnTycmJgYPDw/i4uJYvny52SXVKT/99BNXXHEFERERWCwWvvjii1LLDcPgiSeeICIiAk9PTy666CK2bt1aqk9BQQHjx48nKCgIb29vrrzySvbv31+DR1H7TZs2jR49euDr60tISAjDhg1j+/btpfroXFeNGTNm0KlTJ+eD4OLj4/nmm2+cy3Weq8e0adOwWCxMmDDB2aZzXXlPPPEEFoul1CcsLMy5vNadY0NqxLx58wxXV1dj9uzZxrZt24z777/f8Pb2Nvbt22d2aXXGokWLjClTphgLFiwwAOPzzz8vtfy5554zfH19jQULFhibN282hg8fboSHhxs5OTnOPmPGjDGaNGliJCYmGuvXrzcGDBhgdO7c2SguLq7ho6m9Bg8ebLz77rvGli1bjKSkJOOyyy4zoqKijNzcXGcfneuqsXDhQuPrr782tm/fbmzfvt14+OGHDVdXV2PLli2GYeg8V4fVq1cbzZo1Mzp16mTcf//9znad68p7/PHHjfbt2xvp6enOT2ZmpnN5bTvHCkA1pGfPnsaYMWNKtbVt29Z46KGHTKqobvtzAHI4HEZYWJjx3HPPOdvy8/MNf39/Y+bMmYZhGMbx48cNV1dXY968ec4+Bw4cMKxWq/Htt9/WWO11TWZmpgEYy5YtMwxD57q6BQYGGm+//bbOczU4ceKE0apVKyMxMdHo37+/MwDpXFeNxx9/3OjcuXO5y2rjOdYtsBpQWFjIunXrSEhIKNWekJDAypUrTaqqfklJSSEjI6PUOXZ3d6d///7Oc7xu3TqKiopK9YmIiKBDhw76cziL7OxsABo1agToXFcXu93OvHnzOHnyJPHx8TrP1WDs2LFcdtllDBo0qFS7znXV2blzJxEREcTExHDDDTewZ88eoHaeY70MtQZkZWVht9sJDQ0t1R4aGkpGRoZJVdUvp89jeed43759zj5ubm4EBgaW6aM/h/IZhsGkSZO48MIL6dChA6BzXdU2b95MfHw8+fn5+Pj48Pnnn9OuXTvnP/g6z1Vj3rx5rF+/njVr1pRZpr/TVaNXr17MnTuX1q1bc+jQIZ5++ml69+7N1q1ba+U5VgCqQRaLpdTvhmGUaZPKqcg51p/DmY0bN45Nmzbx888/l1mmc1012rRpQ1JSEsePH2fBggXccsstLFu2zLlc57ny0tLSuP/++1m8eDEeHh5n7KdzXTlDhw51/tyxY0fi4+Np0aIF//73v7nggguA2nWOdQusBgQFBWGz2cok2MzMzDJpWCrm9EyDs53jsLAwCgsLOXbs2Bn7yO/Gjx/PwoUL+fHHH2natKmzXee6arm5udGyZUu6d+/OtGnT6Ny5M6+//rrOcxVat24dmZmZxMXF4eLigouLC8uWLeONN97AxcXFea50rquWt7c3HTt2ZOfOnbXy77MCUA1wc3MjLi6OxMTEUu2JiYn07t3bpKrql5iYGMLCwkqd48LCQpYtW+Y8x3Fxcbi6upbqk56ezpYtW/Tn8AeGYTBu3Dg+++wzfvjhB2JiYkot17muXoZhUFBQoPNchQYOHMjmzZtJSkpyfrp3786IESNISkqiefPmOtfVoKCggOTkZMLDw2vn3+cqH1Yt5To9Df6dd94xtm3bZkyYMMHw9vY29u7da3ZpdcaJEyeMDRs2GBs2bDAA45VXXjE2bNjgfJTAc889Z/j7+xufffaZsXnzZuPGG28sd4pl06ZNjSVLlhjr1683Lr74Yk1j/ZN77rnH8Pf3N5YuXVpqOmteXp6zj8511Zg8ebLx008/GSkpKcamTZuMhx9+2LBarcbixYsNw9B5rk5/nAVmGDrXVeFvf/ubsXTpUmPPnj3GL7/8Ylx++eWGr6+v83uutp1jBaAa9NZbbxnR0dGGm5ub0a1bN+e0Yjk3P/74owGU+dxyyy2GYZRMs3z88ceNsLAww93d3ejXr5+xefPmUts4deqUMW7cOKNRo0aGp6encfnllxupqakmHE3tVd45Box3333X2Ufnumrcdtttzn8TgoODjYEDBzrDj2HoPFenPwcgnevKO/1cH1dXVyMiIsK45pprjK1btzqX17ZzbDEMw6j660oiIiIitZfGAImIiEiDowAkIiIiDY4CkIiIiDQ4CkAiIiLS4CgAiYiISIOjACQiIiINjgKQiIiINDgKQCIiItLgKACJiJyBxWLhiy++MLsMEakGCkAiUiuNHj0ai8VS5jNkyBCzSxOResDF7AJERM5kyJAhvPvuu6Xa3N3dTapGROoTXQESkVrL3d2dsLCwUp/AwECg5PbUjBkzGDp0KJ6ensTExPDJJ5+UWn/z5s1cfPHFeHp60rhxY+666y5yc3NL9ZkzZw7t27fH3d2d8PBwxo0bV2p5VlYWV199NV5eXrRq1YqFCxc6lx07dowRI0YQHByMp6cnrVq1KhPYRKR2UgASkTrr0Ucf5dprr2Xjxo3cfPPN3HjjjSQnJwOQl5fHkCFDCAwMZM2aNXzyyScsWbKkVMCZMWMGY8eO5a677mLz5s0sXLiQli1bltrHk08+yfXXX8+mTZu49NJLGTFiBEePHnXuf9u2bXzzzTckJyczY8YMgoKCau4EiEjFVcs75kVEKumWW24xbDab4e3tXeozdepUwzAMAzDGjBlTap1evXoZ99xzj2EYhjFr1iwjMDDQyM3NdS7/+uuvDavVamRkZBiGYRgRERHGlClTzlgDYDzyyCPO33Nzcw2LxWJ88803hmEYxhVXXGHceuutVXPAIlKjNAZIRGqtAQMGMGPGjFJtjRo1cv4cHx9fall8fDxJSUkAJCcn07lzZ7y9vZ3L+/Tpg8PhYPv27VgsFg4ePMjAgQPPWkOnTp2cP3t7e+Pr60tmZiYA99xzD9deey3r168nISGBYcOG0bt37wodq4jULAUgEam1vL29y9yS+isWiwUAwzCcP5fXx9PT85y25+rqWmZdh8MBwNChQ9m3bx9ff/01S5YsYeDAgYwdO5aXXnrpvGoWkZqnMUAiUmf98ssvZX5v27YtAO3atSMpKYmTJ086l69YsQKr1Urr1q3x9fWlWbNmfP/995WqITg4mNGjR/P+++/z2muvMWvWrEptT0Rqhq4AiUitVVBQQEZGRqk2FxcX50DjTz75hO7du3PhhRfywQcfsHr1at555x0ARowYweOPP84tt9zCE088weHDhxk/fjwjR44kNDQUgCeeeIIxY8YQEhLC0KFDOXHiBCtWrGD8+PHnVN9jjz1GXFwc7du3p6CggK+++orY2NgqPAMiUl0UgESk1vr2228JDw8v1damTRt+++03oGSG1rx587j33nsJCwvjgw8+oF27dgB4eXnx3Xffcf/999OjRw+8vLy49tpreeWVV5zbuuWWW8jPz+fVV1/lgQceICgoiOuuu+6c63Nzc2Py5Mns3bsXT09P+vbty7x586rgyEWkulkMwzDMLkJE5HxZLBY+//xzhg0bZnYpIlIHaQyQiIiINDgKQCIiItLgaAyQiNRJunsvIpWhK0AiIiLS4CgAiYiISIOjACQiIiINjgKQiIiINDgKQCIiItLgKACJiIhIg6MAJCIiIg2OApCIiIg0OP8PSEYUsV/Zyf4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear layer input: (None, 32)\n",
      "linear layer output: (None, 16)\n",
      "7/7 [==============================] - 1s 2ms/step\n",
      "Comparison of predictions and ground truth:\n",
      "Sample 1:\n",
      "  Predicted:    [1. 1. 2. 1. 1. 1. 3. 0. 1. 1. 1. 2. 1. 1. 4. 2.]\n",
      "  Ground Truth: [1. 0. 1. 1. 1. 1. 3. 0. 1. 3. 1. 2. 1. 1. 4. 2.]\n",
      "----------------------------------------\n",
      "Sample 2:\n",
      "  Predicted:    [4. 1. 1. 1. 2. 0. 3. 1. 1. 1. 2. 3. 3. 0. 1. 1.]\n",
      "  Ground Truth: [4. 0. 2. 0. 2. 0. 3. 2. 1. 2. 2. 2. 3. 0. 1. 0.]\n",
      "----------------------------------------\n",
      "Sample 3:\n",
      "  Predicted:    [3. 2. 0. 0. 2. 0. 4. 3. 3. 3. 2. 3. 4. 3. 1. 1.]\n",
      "  Ground Truth: [3. 2. 0. 0. 2. 0. 4. 4. 3. 3. 1. 3. 4. 3. 0. 1.]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = model.predict([X_real_test, X_imag_test])\n",
    "y_pred_test_rescaled = y_pred_test * (q - 1)\n",
    "# y_pred_test_rescaled = (y_pred_test * y_std) + y_mean\n",
    "y_test_rescaled = y_test * (q - 1)\n",
    "\n",
    "print(\"Comparison of predictions and ground truth:\")\n",
    "for i in range(3):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  Predicted:    {np.round(y_pred_test_rescaled[i])}\")\n",
    "    print(f\"  Ground Truth: {np.round(y_test_rescaled[i])}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoded Unseen Data (Real and Imaginary Parts):\n",
      "Real Part Shape: (5, 16)\n",
      "Imaginary Part Shape: (5, 16)\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "\n",
      "Comparison of Unseen Data and Predictions:\n",
      "============================================================\n",
      "Sample 1:\n",
      "  Ground Truth: [2 3 2 1 0 0 0 4 4 1 2 4 2 2 4 4]\n",
      "  Prediction  : [2 3 3 1 0 0 0 3 4 2 1 3 2 2 4 4]\n",
      "------------------------------------------------------------\n",
      "Sample 2:\n",
      "  Ground Truth: [1 3 0 4 4 2 3 4 3 0 2 1 0 3 3 0]\n",
      "  Prediction  : [1 1 1 3 4 2 3 4 3 2 2 1 0 3 3 0]\n",
      "------------------------------------------------------------\n",
      "Sample 3:\n",
      "  Ground Truth: [3 1 1 3 3 1 3 4 2 4 3 3 2 3 2 1]\n",
      "  Prediction  : [3 1 1 2 3 1 3 4 2 4 3 3 2 3 2 1]\n",
      "------------------------------------------------------------\n",
      "Sample 4:\n",
      "  Ground Truth: [2 2 4 3 1 0 2 0 3 2 0 4 0 4 1 3]\n",
      "  Prediction  : [2 2 3 3 1 0 2 0 3 1 2 3 0 4 2 3]\n",
      "------------------------------------------------------------\n",
      "Sample 5:\n",
      "  Ground Truth: [0 2 3 0 2 2 2 2 0 1 4 4 4 1 2 2]\n",
      "  Prediction  : [0 2 3 0 2 2 2 1 0 0 3 3 4 1 1 2]\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_unseen_samples = 5\n",
    "unseen_data = np.random.randint(0, q, size=(num_unseen_samples, n))\n",
    "\n",
    "padded_unseen_data = np.hstack((unseen_data, np.zeros((num_unseen_samples, n_padded - n))))\n",
    "\n",
    "unseen_data_normalized = unseen_data.astype(np.float32) / (q - 1)\n",
    "\n",
    "# print(\"Unseen Input Data (Before Encoding):\")\n",
    "# print(unseen_data)\n",
    "\n",
    "unseen_encoded = np.array([dft(message, n_padded) for message in padded_unseen_data])\n",
    "unseen_encoded[np.abs(unseen_encoded) < 1e-10] = 0\n",
    "unseen_encoded = np.round(unseen_encoded, decimals=10)\n",
    "\n",
    "X_real_unseen = np.real(unseen_encoded).astype(np.float32)\n",
    "X_imag_unseen = np.imag(unseen_encoded).astype(np.float32)\n",
    "\n",
    "print(\"\\nEncoded Unseen Data (Real and Imaginary Parts):\")\n",
    "print(\"Real Part Shape:\", X_real_unseen.shape)\n",
    "print(\"Imaginary Part Shape:\", X_imag_unseen.shape)\n",
    "\n",
    "y_pred_unseen = model.predict([X_real_unseen, X_imag_unseen])\n",
    "\n",
    "y_pred_unseen_rescaled = y_pred_unseen * (q - 1)\n",
    "\n",
    "y_pred_unseen_final = np.mod(np.round(y_pred_unseen_rescaled), q)\n",
    "\n",
    "print(\"\\nComparison of Unseen Data and Predictions:\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(num_unseen_samples):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  Ground Truth: {padded_unseen_data[i].astype(int)}\")\n",
    "    print(f\"  Prediction  : {y_pred_unseen_final[i].astype(int)}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check gradient flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real_input has no trainable variables.\n",
      "imag_input has no trainable variables.\n",
      "real_layer1 Gradient Mean: 0.84663737\n",
      "imag_layer1 Gradient Mean: 0.52898324\n",
      "leaky_re_lu has no trainable variables.\n",
      "leaky_re_lu_3 has no trainable variables.\n",
      "real_support_layer_1 Gradient Mean: 2.463353\n",
      "imag_support_layer_1 Gradient Mean: 2.1630797\n",
      "leaky_re_lu_1 has no trainable variables.\n",
      "leaky_re_lu_4 has no trainable variables.\n",
      "real_layer2 Gradient Mean: 1.4168992\n",
      "imag_layer2 Gradient Mean: 1.342762\n",
      "leaky_re_lu_2 has no trainable variables.\n",
      "leaky_re_lu_5 has no trainable variables.\n",
      "merge_real_imag has no trainable variables.\n",
      "output_layer Gradient Mean: 3.9699821\n",
      "activation has no trainable variables.\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    if layer.trainable_variables:\n",
    "        grads = tf.reduce_mean(tf.abs(layer.trainable_variables[0]))\n",
    "        print(layer.name, \"Gradient Mean:\", grads.numpy())\n",
    "    else:\n",
    "        print(layer.name, \"has no trainable variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear layer input: (4, 32)\n",
      "linear layer output: (4, 16)\n",
      " Gradient OK for real_layer1/kernel_b1:0, mean: 0.000343226216500625\n",
      " Gradient OK for real_layer1/kernel_b2:0, mean: -0.00014678123989142478\n",
      " Gradient OK for real_layer1/kernel_d1:0, mean: -0.0012914541875943542\n",
      " Gradient OK for real_layer1/kernel_d2:0, mean: 0.0011681061005219817\n",
      " Gradient OK for real_layer1/bias:0, mean: 0.0008456771029159427\n",
      " Gradient OK for imag_layer1/kernel_b1:0, mean: -0.0018259388161823153\n",
      " Gradient OK for imag_layer1/kernel_b2:0, mean: -0.003033579094335437\n",
      " Gradient OK for imag_layer1/kernel_d1:0, mean: 0.0036254741717129946\n",
      " Gradient OK for imag_layer1/kernel_d2:0, mean: -0.0006424519815482199\n",
      " Gradient OK for imag_layer1/bias:0, mean: -0.007481678854674101\n",
      " Gradient OK for real_support_layer_1/kernel_m:0, mean: 0.0016991078155115247\n",
      " Gradient OK for real_support_layer_1/bias:0, mean: 0.0007917534676380455\n",
      " Gradient OK for imag_support_layer_1/kernel_m:0, mean: 0.0019616317003965378\n",
      " Gradient OK for imag_support_layer_1/bias:0, mean: -0.008884560316801071\n",
      " Gradient OK for real_layer2/kernel_d1:0, mean: 0.0013099750503897667\n",
      " Gradient OK for real_layer2/bias:0, mean: 0.0002826398704200983\n",
      " Gradient OK for imag_layer2/kernel_d1:0, mean: 0.0044550299644470215\n",
      " Gradient OK for imag_layer2/bias:0, mean: -0.024415982887148857\n",
      " Gradient OK for output_layer/kernel_m_1:0, mean: -0.02062186226248741\n",
      " Gradient OK for output_layer/bias:0, mean: -0.020670495927333832\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4 \n",
    "X_real_dummy = np.random.rand(batch_size, X_real_train.shape[1]).astype(np.float32)\n",
    "X_imag_dummy = np.random.rand(batch_size, X_imag_train.shape[1]).astype(np.float32)\n",
    "y_dummy = np.random.rand(batch_size, y_train.shape[1]).astype(np.float32)\n",
    "\n",
    "def check_gradients(model, X_real_dummy, X_imag_dummy, y_dummy):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(model.trainable_variables)\n",
    "        predictions = model([X_real_dummy, X_imag_dummy], training=True)\n",
    "        loss = tf.keras.losses.mean_squared_error(y_dummy, predictions)\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    for i, (grad, var) in enumerate(zip(grads, model.trainable_variables)):\n",
    "        if grad is None:\n",
    "            print(f\" Gradient is None for {var.name} at index {i}\")\n",
    "        else:\n",
    "            print(f\" Gradient OK for {var.name}, mean: {tf.reduce_mean(grad).numpy()}\")\n",
    "\n",
    "check_gradients(model, X_real_dummy, X_imag_dummy, y_dummy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
