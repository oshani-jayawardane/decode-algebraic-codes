{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DFTSNN - DFT Encoded dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
     "The dataset is encoded using the recursive radix-2 DFT algorithm. The imaginary and real values are parellel processed until concatenation. Results demonstrate perfect reconstruction of the original codeword"
  ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (r + 1) | (q - 1)\n",
    "# (r + 1) | n\n",
    "# q is a prime number\n",
    "# n = 2^t\n",
    "# r < n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset -f\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original n: 16\n",
      "Padded n: 16\n",
      "Generated dataset shape: (1000, 16)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n = 16\n",
    "q = 5\n",
    "num_samples = 1000\n",
    "\n",
    "def next_power_of_two(x):\n",
    "    return 1 if x == 0 else 2**(x - 1).bit_length()\n",
    "\n",
    "n_padded = next_power_of_two(n)\n",
    "\n",
    "dataset = np.random.randint(0, q, size=(num_samples, n))\n",
    "\n",
    "if n_padded > n:\n",
    "    pad_width = n_padded - n\n",
    "    dataset = np.pad(dataset, ((0, 0), (0, pad_width)), mode='constant', constant_values=0)\n",
    "\n",
    "print(\"Original n:\", n)\n",
    "print(\"Padded n:\", n_padded)\n",
    "print(\"Generated dataset shape:\", dataset.shape)\n",
    "\n",
    "n = n_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 4 2 ... 1 3 1]\n",
      " [3 4 0 ... 3 2 3]\n",
      " [3 0 2 ... 0 1 4]\n",
      " ...\n",
      " [4 4 4 ... 0 2 2]\n",
      " [4 2 4 ... 4 0 0]\n",
      " [2 2 2 ... 0 4 2]]\n",
      "(1000, 16)\n",
      "(16,)\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(dataset.shape)\n",
    "print(dataset[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dft(u, n):\n",
    "    n1 = n // 2\n",
    "    if n == 2:  \n",
    "        return np.array([u[0] + u[1], u[0] - u[1]])\n",
    "    \n",
    "    zeta = np.exp(-2j * np.pi / n)\n",
    "    Dn = np.diag([zeta**k for k in range(n1)])\n",
    "    Hn = np.block([[np.eye(n1), np.eye(n1)],\n",
    "                   [Dn, -Dn]])\n",
    "    \n",
    "    p = np.dot(Hn, u)\n",
    "    \n",
    "    s1 = dft(p[:n1], n1)\n",
    "    s2 = dft(p[n1:], n1)\n",
    "    \n",
    "    interleaved = np.empty((len(s1) + len(s2)), dtype=complex)\n",
    "    interleaved[0::2] = s1\n",
    "    interleaved[1::2] = s2\n",
    "    \n",
    "    y = interleaved\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42.         +0.j          0.84148733 -0.81622894j\n",
      "  -0.87867966 -6.36396103j ... -1.2304425  -1.25570089j\n",
      "  -0.87867966 +6.36396103j  0.84148733 +0.81622894j]\n",
      " [34.         +0.j          5.88076075 +0.31815162j\n",
      "  -2.41421356 +3.j         ...  6.11085797 -0.77903661j\n",
      "  -2.41421356 -3.j          5.88076075 -0.31815162j]\n",
      " [29.         +0.j          2.38895517 -4.90489287j\n",
      "   0.17157288 -1.17157288j ...  2.07192983 -7.86463619j\n",
      "   0.17157288 +1.17157288j  2.38895517 +4.90489287j]\n",
      " ...\n",
      " [37.         +0.j          6.60006572-11.28512148j\n",
      "   2.70710678 -0.70710678j ... -0.68037683 +4.15172419j\n",
      "   2.70710678 +0.70710678j  6.60006572+11.28512148j]\n",
      " [40.         +0.j         -4.28191411 +3.31849689j\n",
      "   0.70710678 -3.70710678j ...  1.90134022+10.29671013j\n",
      "   0.70710678 +3.70710678j -4.28191411 -3.31849689j]\n",
      " [35.         +0.j         -2.11652017 +0.96427923j\n",
      "   2.41421356 +0.34314575j ...  0.28130457 -2.5159377j\n",
      "   2.41421356 -0.34314575j -2.11652017 -0.96427923j]]\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset = np.array([dft(message, n) for message in dataset])\n",
    "encoded_dataset[np.abs(encoded_dataset) < 1e-10] = 0\n",
    "encoded_dataset = np.round(encoded_dataset, decimals=10)\n",
    "print(encoded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 16)\n"
     ]
    }
   ],
   "source": [
    "print(encoded_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 2 4 4 1 2 2 2 4 3 2 4 1 3 1]\n",
      "[42.        +0.j          0.84148733-0.81622894j -0.87867966-6.36396103j\n",
      " -1.2304425 +1.25570089j  3.        -1.j          3.2304425 -1.57272623j\n",
      " -5.12132034-6.36396103j  1.15851267-3.64465606j  4.        +0.j\n",
      "  1.15851267+3.64465606j -5.12132034+6.36396103j  3.2304425 +1.57272623j\n",
      "  3.        +1.j         -1.2304425 -1.25570089j -0.87867966+6.36396103j\n",
      "  0.84148733+0.81622894j]\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])\n",
    "print(encoded_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to zero mean and unit variance - output results of normalizing made unseen data converge to q-1\n",
    "# encoded_dataset = (encoded_dataset - np.mean(encoded_dataset, axis=0)) / np.std(encoded_dataset, axis=0)\n",
    "# print(encoded_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train_real : (1000, 16)\n",
      "[42.          0.84148735 -0.87867963 -1.2304425   3.          3.2304425\n",
      " -5.1213202   1.1585127   4.          1.1585127  -5.1213202   3.2304425\n",
      "  3.         -1.2304425  -0.87867963  0.84148735]\n",
      "\n",
      "X_train_imag : (1000, 16)\n",
      "[ 0.        -0.8162289 -6.363961   1.255701  -1.        -1.5727262\n",
      " -6.363961  -3.644656   0.         3.644656   6.363961   1.5727262\n",
      "  1.        -1.255701   6.363961   0.8162289]\n"
     ]
    }
   ],
   "source": [
    "X_real = np.real(encoded_dataset).astype(np.float32)\n",
    "X_imag = np.imag(encoded_dataset).astype(np.float32)\n",
    "\n",
    "print(\"\\nX_train_real :\", X_real.shape)\n",
    "print(X_real[0])\n",
    "print(\"\\nX_train_imag :\", X_imag.shape)\n",
    "print(X_imag[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: \n",
      "[3. 4. 2. 4. 4. 1. 2. 2. 2. 4. 3. 2. 4. 1. 3. 1.]\n",
      "\n",
      "y_normalized: \n",
      "[0.75 1.   0.5  1.   1.   0.25 0.5  0.5  0.5  1.   0.75 0.5  1.   0.25\n",
      " 0.75 0.25]\n"
     ]
    }
   ],
   "source": [
    "# normalize target data (integers 0-q to [0, 1])\n",
    "y_normalized = dataset.astype(np.float32) / (q - 1)  # Scale to [0, 1]\n",
    "print(\"y: \")\n",
    "print(dataset.astype(np.float32)[0])\n",
    "print(\"\\ny_normalized: \")\n",
    "print(y_normalized[0])\n",
    "\n",
    "labels = y_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shapes: X_real: (800, 16) X_imag: (800, 16) y: (800, 16)\n",
      "Testing data shapes: X_real: (200, 16) X_imag: (200, 16) y: (200, 16)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_real_train, X_real_test, y_train, y_test = train_test_split(\n",
    "    X_real, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_imag_train, X_imag_test, _, _ = train_test_split(\n",
    "    X_imag, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training data shapes: X_real:\", X_real_train.shape, \"X_imag:\", X_imag_train.shape, \"y:\", y_train.shape)\n",
    "print(\"Testing data shapes: X_real:\", X_real_test.shape, \"X_imag:\", X_imag_test.shape, \"y:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "print(X_real_train.shape[1])\n",
    "print(X_real_test.shape[1])\n",
    "print(X_imag_train.shape[1])\n",
    "print(X_imag_test.shape[1])\n",
    "print(y_train.shape[1])\n",
    "print(y_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_real_train.shape[1]\n",
    "output_dim = y_train.shape[1]\n",
    "print(input_dim)\n",
    "print(output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDFT - Structure Imposed Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, kernel_initializer='he_normal', bias_initializer='zeros', use_bias=True, **kwargs):\n",
    "        super(FirstLayer, self).__init__(**kwargs)\n",
    "        self.units = units  # Features/neurons\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n = self.units\n",
    "        n1 = n // 2\n",
    "        num_blocks = n1 // 2\n",
    "\n",
    "        self.b_1 = self.add_weight(name=\"kernel_b1\",\n",
    "                                   shape=(num_blocks, 2, 2),\n",
    "                                   initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                   trainable=True)\n",
    "        self.b_2 = self.add_weight(name=\"kernel_b2\",\n",
    "                                   shape=(num_blocks, 2, 2),\n",
    "                                   initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                   trainable=True)\n",
    "        self.d_1 = self.add_weight(name=\"kernel_d1\",\n",
    "                                   shape=(n1 - 2,),\n",
    "                                   initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                   trainable=True)\n",
    "        self.d_2 = self.add_weight(name=\"kernel_d2\",\n",
    "                                   shape=(n1 - 2,),\n",
    "                                   initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                   trainable=True)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name=\"bias\",\n",
    "                                        shape=(self.units,),\n",
    "                                        initializer=tf.keras.initializers.get(self.bias_initializer),\n",
    "                                        trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        def recursiveIDFT(inputVector, B, d, level):\n",
    "            n = inputVector.shape[1]\n",
    "            n1 = n // 2\n",
    "\n",
    "            if n == 2:\n",
    "                out = tf.matmul(inputVector, B[level])\n",
    "                return out\n",
    "            else:\n",
    "                q = tf.concat([inputVector[:, ::2], inputVector[:, 1::2]], axis=1)\n",
    "\n",
    "                B1 = recursiveIDFT(q[:, :n1], B, d[n1:], level + 1)\n",
    "                B2 = recursiveIDFT(q[:, n1:], B, d[n1:], level + 1)\n",
    "\n",
    "                d_n = tf.reshape(d[:n1], (1, -1))\n",
    "                z1 = tf.concat([(B1 + tf.multiply(B2, d_n)), (B1 - tf.multiply(B2, d_n))], axis=1)\n",
    "\n",
    "                return z1 / tf.sqrt(tf.constant(2.0, dtype=tf.float32))\n",
    "\n",
    "        n = self.units\n",
    "        n1 = n // 2\n",
    "\n",
    "        q = tf.concat([inputs[:, ::2], inputs[:, 1::2]], axis=1)\n",
    "\n",
    "        B1 = recursiveIDFT(q[:, :n1], self.b_1, self.d_1, level=0)\n",
    "        B2 = recursiveIDFT(q[:, n1:], self.b_2, self.d_2, level=0)\n",
    "\n",
    "        out = tf.concat([B1, B2], axis=1)\n",
    "\n",
    "        if self.use_bias:\n",
    "            out += self.bias\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, kernel_initializer='he_normal', bias_initializer='zeros', use_bias=True, **kwargs):\n",
    "        super(SecondLayer, self).__init__(**kwargs)\n",
    "        self.units = units  # Features/neurons\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n = self.units\n",
    "        n1 = n // 2\n",
    "\n",
    "        self.d_1 = self.add_weight(name=\"kernel_d1\",\n",
    "                                   shape=(n1,),\n",
    "                                   initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                   trainable=True)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name=\"bias\",\n",
    "                                        shape=(n,),\n",
    "                                        initializer=tf.keras.initializers.get(self.bias_initializer),\n",
    "                                        trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        out1 = inputs[:, :int(inputs.shape[1] / 2)]\n",
    "        out2 = inputs[:, int(inputs.shape[1] / 2):]\n",
    "\n",
    "        z1 = tf.concat([(out1 + tf.multiply(out2, self.d_1)), (out1 - tf.multiply(out2, self.d_1))], axis=1)\n",
    "        out = z1 / tf.sqrt(tf.constant(2.0, dtype=tf.float32))\n",
    "\n",
    "        if self.use_bias:\n",
    "            out += self.bias\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, kernel_initializer='he_normal', bias_initializer='zeros', use_bias=True, **kwargs):\n",
    "        super(CustomLayer, self).__init__(**kwargs)\n",
    "        self.units = units  # features/neurons\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n = self.units\n",
    "\n",
    "        self.m = self.add_weight(name=\"kernel_m\",\n",
    "                                 shape=(n,),\n",
    "                                 initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                 trainable=True)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name=\"bias\",\n",
    "                                        shape=(self.units,),\n",
    "                                        initializer=tf.keras.initializers.get(self.bias_initializer),\n",
    "                                        trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        out = tf.multiply(inputs, self.m)\n",
    "        if self.use_bias:\n",
    "            out += self.bias\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, kernel_initializer='ones', bias_initializer='zeros', use_bias=True, **kwargs):\n",
    "        super(LinearLayer, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n = self.units\n",
    "\n",
    "        self.m = self.add_weight(name=\"kernel_m_1\",\n",
    "                                 shape=(n,),\n",
    "                                 initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                 trainable=True)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name=\"bias\",\n",
    "                                        shape=(self.units,),\n",
    "                                        initializer=tf.keras.initializers.get(self.bias_initializer),\n",
    "                                        trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        print(\"linear layer input:\", inputs.shape)\n",
    "\n",
    "        inputs1 = inputs[:, :int(inputs.shape[1] / 2)]\n",
    "        inputs2 = inputs[:, int(inputs.shape[1] / 2):]\n",
    "\n",
    "        out = tf.math.sqrt(inputs1**2 + inputs2**2)\n",
    "        out = tf.multiply(out, self.m)\n",
    "\n",
    "        if self.use_bias:\n",
    "            out += self.bias\n",
    "\n",
    "        print(\"linear layer output:\", out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "linear layer input: (None, 32)\n",
      "linear layer output: (None, 16)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " real_input (InputLayer)     [(None, 16)]                 0         []                            \n",
      "                                                                                                  \n",
      " imag_input (InputLayer)     [(None, 16)]                 0         []                            \n",
      "                                                                                                  \n",
      " real_layer1 (FirstLayer)    (None, 16)                   60        ['real_input[0][0]']          \n",
      "                                                                                                  \n",
      " imag_layer1 (FirstLayer)    (None, 16)                   60        ['imag_input[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)     (None, 16)                   0         ['real_layer1[0][0]']         \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 16)                   0         ['imag_layer1[0][0]']         \n",
      "                                                                                                  \n",
      " real_support_layer_1 (Cust  (None, 16)                   32        ['leaky_re_lu[0][0]']         \n",
      " omLayer)                                                                                         \n",
      "                                                                                                  \n",
      " imag_support_layer_1 (Cust  (None, 16)                   32        ['leaky_re_lu_3[0][0]']       \n",
      " omLayer)                                                                                         \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 16)                   0         ['real_support_layer_1[0][0]']\n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 16)                   0         ['imag_support_layer_1[0][0]']\n",
      "                                                                                                  \n",
      " real_layer2 (SecondLayer)   (None, 16)                   24        ['leaky_re_lu_1[0][0]']       \n",
      "                                                                                                  \n",
      " imag_layer2 (SecondLayer)   (None, 16)                   24        ['leaky_re_lu_4[0][0]']       \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 16)                   0         ['real_layer2[0][0]']         \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 16)                   0         ['imag_layer2[0][0]']         \n",
      "                                                                                                  \n",
      " merge_real_imag (Concatena  (None, 32)                   0         ['leaky_re_lu_2[0][0]',       \n",
      " te)                                                                 'leaky_re_lu_5[0][0]']       \n",
      "                                                                                                  \n",
      " output_layer (LinearLayer)  (None, 16)                   32        ['merge_real_imag[0][0]']     \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 16)                   0         ['output_layer[0][0]']        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 264 (1.03 KB)\n",
      "Trainable params: 264 (1.03 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Concatenate, LeakyReLU, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def cosine_loss(y_true, y_pred):\n",
    "    y_true_norm = tf.nn.l2_normalize(y_true, axis=-1)\n",
    "    y_pred_norm = tf.nn.l2_normalize(y_pred, axis=-1)\n",
    "    return 1 - tf.reduce_mean(tf.reduce_sum(y_true_norm * y_pred_norm, axis=-1))\n",
    "\n",
    "def hybrid_loss(y_true, y_pred):\n",
    "    mse_part = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    cos_part = cosine_loss(y_true, y_pred)\n",
    "    return mse_part + cos_part\n",
    "\n",
    "def structured_NN(input_dim, output_dim):\n",
    "    real_input = Input(shape=(input_dim,), name=\"real_input\")\n",
    "    imag_input = Input(shape=(input_dim,), name=\"imag_input\")\n",
    "\n",
    "    real_x = FirstLayer(units=input_dim, kernel_initializer='he_normal', bias_initializer='zeros', name=\"real_layer1\")(real_input)\n",
    "    real_x = LeakyReLU(alpha=0.1)(real_x)\n",
    "    real_x = CustomLayer(units=input_dim, kernel_initializer='he_normal', bias_initializer='zeros', name=\"real_support_layer_1\")(real_x)\n",
    "    real_x = LeakyReLU(alpha=0.1)(real_x)\n",
    "    real_x = SecondLayer(units=input_dim, kernel_initializer='he_normal', bias_initializer='zeros', name=\"real_layer2\")(real_x)\n",
    "    real_x = LeakyReLU(alpha=0.1)(real_x)\n",
    "\n",
    "    imag_x = FirstLayer(units=input_dim, kernel_initializer='he_normal', bias_initializer='zeros', name=\"imag_layer1\")(imag_input)\n",
    "    imag_x = LeakyReLU(alpha=0.1)(imag_x)\n",
    "    imag_x = CustomLayer(units=input_dim, kernel_initializer='he_normal', bias_initializer='zeros', name=\"imag_support_layer_1\")(imag_x)\n",
    "    imag_x = LeakyReLU(alpha=0.1)(imag_x)\n",
    "    imag_x = SecondLayer(units=input_dim, kernel_initializer='he_normal', bias_initializer='zeros', name=\"imag_layer2\")(imag_x)\n",
    "    imag_x = LeakyReLU(alpha=0.1)(imag_x)\n",
    "\n",
    "    merged = Concatenate(name=\"merge_real_imag\")([real_x, imag_x])\n",
    "    \n",
    "    output = LinearLayer(units=output_dim, kernel_initializer='ones', bias_initializer='zeros', name=\"output_layer\")(merged)\n",
    "    output = Activation('sigmoid')(output)\n",
    "\n",
    "    model = Model(inputs=[real_input, imag_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss=hybrid_loss, metrics=['mse', 'mae'])\n",
    "\n",
    "    return model\n",
    "\n",
    "input_dim = X_real_train.shape[1]\n",
    "output_dim = y_train.shape[1]\n",
    "model = structured_NN(input_dim, output_dim)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real_input [(None, 16)]\n",
      "imag_input [(None, 16)]\n",
      "real_layer1 (None, 16)\n",
      "imag_layer1 (None, 16)\n",
      "leaky_re_lu (None, 16)\n",
      "leaky_re_lu_3 (None, 16)\n",
      "real_support_layer_1 (None, 16)\n",
      "imag_support_layer_1 (None, 16)\n",
      "leaky_re_lu_1 (None, 16)\n",
      "leaky_re_lu_4 (None, 16)\n",
      "real_layer2 (None, 16)\n",
      "imag_layer2 (None, 16)\n",
      "leaky_re_lu_2 (None, 16)\n",
      "leaky_re_lu_5 (None, 16)\n",
      "merge_real_imag (None, 32)\n",
      "output_layer (None, 16)\n",
      "activation (None, 16)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name, layer.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "adjust_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "linear layer input: (16, 32)\n",
      "linear layer output: (16, 16)\n",
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "linear layer input: (16, 32)\n",
      "linear layer output: (16, 16)\n",
      "41/50 [=======================>......] - ETA: 0s - loss: 0.4216 - mse: 0.2060 - mae: 0.3707 linear layer input: (None, 32)\n",
      "linear layer output: (None, 16)\n",
      "50/50 [==============================] - 6s 25ms/step - loss: 0.4225 - mse: 0.2054 - mae: 0.3712 - val_loss: 0.4159 - val_mse: 0.1984 - val_mae: 0.3651 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3958 - mse: 0.1864 - mae: 0.3582 - val_loss: 0.3846 - val_mse: 0.1766 - val_mae: 0.3496 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3616 - mse: 0.1632 - mae: 0.3407 - val_loss: 0.3474 - val_mse: 0.1519 - val_mae: 0.3297 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3263 - mse: 0.1408 - mae: 0.3208 - val_loss: 0.3169 - val_mse: 0.1328 - val_mae: 0.3124 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3031 - mse: 0.1271 - mae: 0.3082 - val_loss: 0.3007 - val_mse: 0.1236 - val_mae: 0.3029 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2904 - mse: 0.1206 - mae: 0.3004 - val_loss: 0.2909 - val_mse: 0.1189 - val_mae: 0.2963 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2808 - mse: 0.1164 - mae: 0.2943 - val_loss: 0.2821 - val_mse: 0.1153 - val_mae: 0.2907 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2724 - mse: 0.1132 - mae: 0.2891 - val_loss: 0.2744 - val_mse: 0.1124 - val_mae: 0.2864 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2651 - mse: 0.1105 - mae: 0.2850 - val_loss: 0.2671 - val_mse: 0.1098 - val_mae: 0.2827 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2570 - mse: 0.1077 - mae: 0.2809 - val_loss: 0.2575 - val_mse: 0.1065 - val_mae: 0.2782 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2489 - mse: 0.1049 - mae: 0.2765 - val_loss: 0.2500 - val_mse: 0.1040 - val_mae: 0.2743 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2424 - mse: 0.1027 - mae: 0.2729 - val_loss: 0.2441 - val_mse: 0.1020 - val_mae: 0.2711 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2367 - mse: 0.1006 - mae: 0.2696 - val_loss: 0.2389 - val_mse: 0.1002 - val_mae: 0.2682 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2316 - mse: 0.0988 - mae: 0.2667 - val_loss: 0.2344 - val_mse: 0.0985 - val_mae: 0.2657 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2268 - mse: 0.0969 - mae: 0.2639 - val_loss: 0.2298 - val_mse: 0.0967 - val_mae: 0.2630 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2221 - mse: 0.0951 - mae: 0.2611 - val_loss: 0.2252 - val_mse: 0.0950 - val_mae: 0.2604 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2173 - mse: 0.0931 - mae: 0.2583 - val_loss: 0.2204 - val_mse: 0.0930 - val_mae: 0.2575 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2123 - mse: 0.0911 - mae: 0.2552 - val_loss: 0.2154 - val_mse: 0.0910 - val_mae: 0.2546 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2072 - mse: 0.0890 - mae: 0.2521 - val_loss: 0.2099 - val_mse: 0.0888 - val_mae: 0.2512 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2020 - mse: 0.0869 - mae: 0.2487 - val_loss: 0.2047 - val_mse: 0.0866 - val_mae: 0.2479 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1967 - mse: 0.0847 - mae: 0.2453 - val_loss: 0.1993 - val_mse: 0.0844 - val_mae: 0.2444 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1917 - mse: 0.0826 - mae: 0.2419 - val_loss: 0.1940 - val_mse: 0.0823 - val_mae: 0.2408 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.1868 - mse: 0.0806 - mae: 0.2385 - val_loss: 0.1888 - val_mse: 0.0801 - val_mae: 0.2371 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1824 - mse: 0.0787 - mae: 0.2352 - val_loss: 0.1841 - val_mse: 0.0781 - val_mae: 0.2338 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1781 - mse: 0.0769 - mae: 0.2319 - val_loss: 0.1798 - val_mse: 0.0763 - val_mae: 0.2304 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1743 - mse: 0.0752 - mae: 0.2288 - val_loss: 0.1760 - val_mse: 0.0747 - val_mae: 0.2274 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1707 - mse: 0.0737 - mae: 0.2258 - val_loss: 0.1724 - val_mse: 0.0732 - val_mae: 0.2245 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1675 - mse: 0.0724 - mae: 0.2231 - val_loss: 0.1691 - val_mse: 0.0718 - val_mae: 0.2219 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1645 - mse: 0.0710 - mae: 0.2206 - val_loss: 0.1661 - val_mse: 0.0705 - val_mae: 0.2196 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1615 - mse: 0.0698 - mae: 0.2183 - val_loss: 0.1634 - val_mse: 0.0694 - val_mae: 0.2175 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1588 - mse: 0.0686 - mae: 0.2160 - val_loss: 0.1609 - val_mse: 0.0683 - val_mae: 0.2155 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1563 - mse: 0.0675 - mae: 0.2138 - val_loss: 0.1587 - val_mse: 0.0673 - val_mae: 0.2136 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1540 - mse: 0.0666 - mae: 0.2119 - val_loss: 0.1567 - val_mse: 0.0665 - val_mae: 0.2121 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1520 - mse: 0.0657 - mae: 0.2102 - val_loss: 0.1549 - val_mse: 0.0657 - val_mae: 0.2106 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1501 - mse: 0.0649 - mae: 0.2086 - val_loss: 0.1531 - val_mse: 0.0650 - val_mae: 0.2091 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1484 - mse: 0.0642 - mae: 0.2072 - val_loss: 0.1515 - val_mse: 0.0643 - val_mae: 0.2077 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1468 - mse: 0.0635 - mae: 0.2057 - val_loss: 0.1500 - val_mse: 0.0636 - val_mae: 0.2064 - lr: 0.0010\n",
      "Epoch 38/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1453 - mse: 0.0628 - mae: 0.2045 - val_loss: 0.1486 - val_mse: 0.0630 - val_mae: 0.2052 - lr: 0.0010\n",
      "Epoch 39/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1440 - mse: 0.0622 - mae: 0.2034 - val_loss: 0.1472 - val_mse: 0.0624 - val_mae: 0.2041 - lr: 0.0010\n",
      "Epoch 40/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1427 - mse: 0.0617 - mae: 0.2023 - val_loss: 0.1459 - val_mse: 0.0619 - val_mae: 0.2030 - lr: 0.0010\n",
      "Epoch 41/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1414 - mse: 0.0611 - mae: 0.2013 - val_loss: 0.1446 - val_mse: 0.0613 - val_mae: 0.2019 - lr: 0.0010\n",
      "Epoch 42/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1403 - mse: 0.0606 - mae: 0.2004 - val_loss: 0.1435 - val_mse: 0.0609 - val_mae: 0.2010 - lr: 0.0010\n",
      "Epoch 43/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1392 - mse: 0.0602 - mae: 0.1994 - val_loss: 0.1424 - val_mse: 0.0604 - val_mae: 0.2001 - lr: 0.0010\n",
      "Epoch 44/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1382 - mse: 0.0597 - mae: 0.1985 - val_loss: 0.1415 - val_mse: 0.0600 - val_mae: 0.1993 - lr: 0.0010\n",
      "Epoch 45/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1373 - mse: 0.0594 - mae: 0.1978 - val_loss: 0.1405 - val_mse: 0.0596 - val_mae: 0.1985 - lr: 0.0010\n",
      "Epoch 46/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1363 - mse: 0.0590 - mae: 0.1969 - val_loss: 0.1395 - val_mse: 0.0592 - val_mae: 0.1975 - lr: 0.0010\n",
      "Epoch 47/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1354 - mse: 0.0586 - mae: 0.1962 - val_loss: 0.1386 - val_mse: 0.0588 - val_mae: 0.1967 - lr: 0.0010\n",
      "Epoch 48/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1346 - mse: 0.0582 - mae: 0.1955 - val_loss: 0.1378 - val_mse: 0.0584 - val_mae: 0.1960 - lr: 0.0010\n",
      "Epoch 49/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1338 - mse: 0.0579 - mae: 0.1948 - val_loss: 0.1369 - val_mse: 0.0580 - val_mae: 0.1951 - lr: 0.0010\n",
      "Epoch 50/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1330 - mse: 0.0575 - mae: 0.1940 - val_loss: 0.1362 - val_mse: 0.0577 - val_mae: 0.1945 - lr: 0.0010\n",
      "Epoch 51/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1322 - mse: 0.0572 - mae: 0.1933 - val_loss: 0.1355 - val_mse: 0.0575 - val_mae: 0.1938 - lr: 0.0010\n",
      "Epoch 52/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1315 - mse: 0.0569 - mae: 0.1926 - val_loss: 0.1348 - val_mse: 0.0571 - val_mae: 0.1930 - lr: 0.0010\n",
      "Epoch 53/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1308 - mse: 0.0566 - mae: 0.1920 - val_loss: 0.1341 - val_mse: 0.0568 - val_mae: 0.1924 - lr: 0.0010\n",
      "Epoch 54/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1301 - mse: 0.0563 - mae: 0.1913 - val_loss: 0.1334 - val_mse: 0.0565 - val_mae: 0.1917 - lr: 0.0010\n",
      "Epoch 55/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1295 - mse: 0.0560 - mae: 0.1907 - val_loss: 0.1327 - val_mse: 0.0562 - val_mae: 0.1909 - lr: 0.0010\n",
      "Epoch 56/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1289 - mse: 0.0558 - mae: 0.1901 - val_loss: 0.1321 - val_mse: 0.0560 - val_mae: 0.1902 - lr: 0.0010\n",
      "Epoch 57/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1282 - mse: 0.0555 - mae: 0.1894 - val_loss: 0.1313 - val_mse: 0.0556 - val_mae: 0.1895 - lr: 0.0010\n",
      "Epoch 58/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1276 - mse: 0.0552 - mae: 0.1888 - val_loss: 0.1307 - val_mse: 0.0554 - val_mae: 0.1889 - lr: 0.0010\n",
      "Epoch 59/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1270 - mse: 0.0550 - mae: 0.1883 - val_loss: 0.1301 - val_mse: 0.0551 - val_mae: 0.1883 - lr: 0.0010\n",
      "Epoch 60/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1264 - mse: 0.0547 - mae: 0.1877 - val_loss: 0.1297 - val_mse: 0.0549 - val_mae: 0.1878 - lr: 0.0010\n",
      "Epoch 61/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1258 - mse: 0.0544 - mae: 0.1871 - val_loss: 0.1290 - val_mse: 0.0546 - val_mae: 0.1870 - lr: 0.0010\n",
      "Epoch 62/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1253 - mse: 0.0542 - mae: 0.1865 - val_loss: 0.1286 - val_mse: 0.0544 - val_mae: 0.1866 - lr: 0.0010\n",
      "Epoch 63/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1247 - mse: 0.0539 - mae: 0.1859 - val_loss: 0.1281 - val_mse: 0.0542 - val_mae: 0.1860 - lr: 0.0010\n",
      "Epoch 64/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1242 - mse: 0.0537 - mae: 0.1854 - val_loss: 0.1277 - val_mse: 0.0540 - val_mae: 0.1856 - lr: 0.0010\n",
      "Epoch 65/500\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.1236 - mse: 0.0535 - mae: 0.1849 - val_loss: 0.1272 - val_mse: 0.0539 - val_mae: 0.1851 - lr: 0.0010\n",
      "Epoch 66/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1231 - mse: 0.0533 - mae: 0.1843 - val_loss: 0.1267 - val_mse: 0.0537 - val_mae: 0.1847 - lr: 0.0010\n",
      "Epoch 67/500\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.1227 - mse: 0.0531 - mae: 0.1840 - val_loss: 0.1263 - val_mse: 0.0534 - val_mae: 0.1842 - lr: 0.0010\n",
      "Epoch 68/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1222 - mse: 0.0529 - mae: 0.1836 - val_loss: 0.1258 - val_mse: 0.0533 - val_mae: 0.1838 - lr: 0.0010\n",
      "Epoch 69/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1219 - mse: 0.0527 - mae: 0.1830 - val_loss: 0.1254 - val_mse: 0.0531 - val_mae: 0.1833 - lr: 0.0010\n",
      "Epoch 70/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1214 - mse: 0.0525 - mae: 0.1827 - val_loss: 0.1249 - val_mse: 0.0529 - val_mae: 0.1829 - lr: 0.0010\n",
      "Epoch 71/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1210 - mse: 0.0524 - mae: 0.1824 - val_loss: 0.1245 - val_mse: 0.0527 - val_mae: 0.1825 - lr: 0.0010\n",
      "Epoch 72/500\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1206 - mse: 0.0522 - mae: 0.1819 - val_loss: 0.1242 - val_mse: 0.0526 - val_mae: 0.1821 - lr: 0.0010\n",
      "Epoch 73/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.1202 - mse: 0.0520 - mae: 0.1815 - val_loss: 0.1237 - val_mse: 0.0524 - val_mae: 0.1817 - lr: 0.0010\n",
      "Epoch 74/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1199 - mse: 0.0519 - mae: 0.1813 - val_loss: 0.1233 - val_mse: 0.0522 - val_mae: 0.1814 - lr: 0.0010\n",
      "Epoch 75/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1195 - mse: 0.0517 - mae: 0.1809 - val_loss: 0.1229 - val_mse: 0.0521 - val_mae: 0.1808 - lr: 0.0010\n",
      "Epoch 76/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1191 - mse: 0.0516 - mae: 0.1806 - val_loss: 0.1226 - val_mse: 0.0519 - val_mae: 0.1807 - lr: 0.0010\n",
      "Epoch 77/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1188 - mse: 0.0514 - mae: 0.1802 - val_loss: 0.1222 - val_mse: 0.0517 - val_mae: 0.1801 - lr: 0.0010\n",
      "Epoch 78/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1184 - mse: 0.0512 - mae: 0.1800 - val_loss: 0.1218 - val_mse: 0.0516 - val_mae: 0.1799 - lr: 0.0010\n",
      "Epoch 79/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1181 - mse: 0.0511 - mae: 0.1797 - val_loss: 0.1215 - val_mse: 0.0515 - val_mae: 0.1795 - lr: 0.0010\n",
      "Epoch 80/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1178 - mse: 0.0510 - mae: 0.1793 - val_loss: 0.1211 - val_mse: 0.0513 - val_mae: 0.1791 - lr: 0.0010\n",
      "Epoch 81/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1175 - mse: 0.0508 - mae: 0.1790 - val_loss: 0.1209 - val_mse: 0.0512 - val_mae: 0.1789 - lr: 0.0010\n",
      "Epoch 82/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1171 - mse: 0.0507 - mae: 0.1788 - val_loss: 0.1207 - val_mse: 0.0511 - val_mae: 0.1787 - lr: 0.0010\n",
      "Epoch 83/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1168 - mse: 0.0505 - mae: 0.1784 - val_loss: 0.1204 - val_mse: 0.0510 - val_mae: 0.1784 - lr: 0.0010\n",
      "Epoch 84/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1165 - mse: 0.0504 - mae: 0.1783 - val_loss: 0.1200 - val_mse: 0.0508 - val_mae: 0.1782 - lr: 0.0010\n",
      "Epoch 85/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1163 - mse: 0.0503 - mae: 0.1778 - val_loss: 0.1197 - val_mse: 0.0507 - val_mae: 0.1781 - lr: 0.0010\n",
      "Epoch 86/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1161 - mse: 0.0502 - mae: 0.1778 - val_loss: 0.1194 - val_mse: 0.0506 - val_mae: 0.1774 - lr: 0.0010\n",
      "Epoch 87/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1157 - mse: 0.0501 - mae: 0.1773 - val_loss: 0.1192 - val_mse: 0.0505 - val_mae: 0.1776 - lr: 0.0010\n",
      "Epoch 88/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1155 - mse: 0.0500 - mae: 0.1772 - val_loss: 0.1190 - val_mse: 0.0504 - val_mae: 0.1773 - lr: 0.0010\n",
      "Epoch 89/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1153 - mse: 0.0499 - mae: 0.1770 - val_loss: 0.1188 - val_mse: 0.0503 - val_mae: 0.1769 - lr: 0.0010\n",
      "Epoch 90/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1151 - mse: 0.0498 - mae: 0.1767 - val_loss: 0.1185 - val_mse: 0.0502 - val_mae: 0.1766 - lr: 0.0010\n",
      "Epoch 91/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1148 - mse: 0.0497 - mae: 0.1763 - val_loss: 0.1184 - val_mse: 0.0502 - val_mae: 0.1767 - lr: 0.0010\n",
      "Epoch 92/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1146 - mse: 0.0496 - mae: 0.1762 - val_loss: 0.1182 - val_mse: 0.0501 - val_mae: 0.1765 - lr: 0.0010\n",
      "Epoch 93/500\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.1144 - mse: 0.0495 - mae: 0.1760 - val_loss: 0.1178 - val_mse: 0.0499 - val_mae: 0.1757 - lr: 0.0010\n",
      "Epoch 94/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1141 - mse: 0.0494 - mae: 0.1757 - val_loss: 0.1176 - val_mse: 0.0498 - val_mae: 0.1759 - lr: 0.0010\n",
      "Epoch 95/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1139 - mse: 0.0493 - mae: 0.1756 - val_loss: 0.1173 - val_mse: 0.0497 - val_mae: 0.1755 - lr: 0.0010\n",
      "Epoch 96/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1137 - mse: 0.0492 - mae: 0.1752 - val_loss: 0.1171 - val_mse: 0.0496 - val_mae: 0.1751 - lr: 0.0010\n",
      "Epoch 97/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1135 - mse: 0.0491 - mae: 0.1751 - val_loss: 0.1168 - val_mse: 0.0495 - val_mae: 0.1750 - lr: 0.0010\n",
      "Epoch 98/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1131 - mse: 0.0489 - mae: 0.1748 - val_loss: 0.1167 - val_mse: 0.0494 - val_mae: 0.1747 - lr: 0.0010\n",
      "Epoch 99/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1129 - mse: 0.0488 - mae: 0.1746 - val_loss: 0.1165 - val_mse: 0.0494 - val_mae: 0.1746 - lr: 0.0010\n",
      "Epoch 100/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1127 - mse: 0.0487 - mae: 0.1744 - val_loss: 0.1163 - val_mse: 0.0493 - val_mae: 0.1746 - lr: 0.0010\n",
      "Epoch 101/500\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1125 - mse: 0.0486 - mae: 0.1741 - val_loss: 0.1160 - val_mse: 0.0492 - val_mae: 0.1742 - lr: 0.0010\n",
      "Epoch 102/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1123 - mse: 0.0486 - mae: 0.1740 - val_loss: 0.1159 - val_mse: 0.0491 - val_mae: 0.1741 - lr: 0.0010\n",
      "Epoch 103/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1121 - mse: 0.0485 - mae: 0.1737 - val_loss: 0.1157 - val_mse: 0.0490 - val_mae: 0.1739 - lr: 0.0010\n",
      "Epoch 104/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1119 - mse: 0.0484 - mae: 0.1734 - val_loss: 0.1156 - val_mse: 0.0490 - val_mae: 0.1738 - lr: 0.0010\n",
      "Epoch 105/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1117 - mse: 0.0483 - mae: 0.1735 - val_loss: 0.1153 - val_mse: 0.0488 - val_mae: 0.1734 - lr: 0.0010\n",
      "Epoch 106/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1115 - mse: 0.0482 - mae: 0.1731 - val_loss: 0.1152 - val_mse: 0.0488 - val_mae: 0.1734 - lr: 0.0010\n",
      "Epoch 107/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1113 - mse: 0.0481 - mae: 0.1729 - val_loss: 0.1151 - val_mse: 0.0488 - val_mae: 0.1734 - lr: 0.0010\n",
      "Epoch 108/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1111 - mse: 0.0481 - mae: 0.1727 - val_loss: 0.1147 - val_mse: 0.0486 - val_mae: 0.1727 - lr: 0.0010\n",
      "Epoch 109/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1109 - mse: 0.0480 - mae: 0.1726 - val_loss: 0.1145 - val_mse: 0.0485 - val_mae: 0.1726 - lr: 0.0010\n",
      "Epoch 110/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1107 - mse: 0.0479 - mae: 0.1723 - val_loss: 0.1144 - val_mse: 0.0485 - val_mae: 0.1726 - lr: 0.0010\n",
      "Epoch 111/500\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.1105 - mse: 0.0478 - mae: 0.1720 - val_loss: 0.1143 - val_mse: 0.0484 - val_mae: 0.1726 - lr: 0.0010\n",
      "Epoch 112/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1103 - mse: 0.0477 - mae: 0.1720 - val_loss: 0.1138 - val_mse: 0.0482 - val_mae: 0.1719 - lr: 0.0010\n",
      "Epoch 113/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1101 - mse: 0.0476 - mae: 0.1717 - val_loss: 0.1135 - val_mse: 0.0481 - val_mae: 0.1717 - lr: 0.0010\n",
      "Epoch 114/500\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.1098 - mse: 0.0475 - mae: 0.1714 - val_loss: 0.1132 - val_mse: 0.0480 - val_mae: 0.1716 - lr: 0.0010\n",
      "Epoch 115/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1095 - mse: 0.0474 - mae: 0.1712 - val_loss: 0.1128 - val_mse: 0.0478 - val_mae: 0.1712 - lr: 0.0010\n",
      "Epoch 116/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1091 - mse: 0.0472 - mae: 0.1708 - val_loss: 0.1126 - val_mse: 0.0477 - val_mae: 0.1711 - lr: 0.0010\n",
      "Epoch 117/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1089 - mse: 0.0471 - mae: 0.1708 - val_loss: 0.1123 - val_mse: 0.0476 - val_mae: 0.1706 - lr: 0.0010\n",
      "Epoch 118/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1087 - mse: 0.0470 - mae: 0.1704 - val_loss: 0.1119 - val_mse: 0.0474 - val_mae: 0.1702 - lr: 0.0010\n",
      "Epoch 119/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1084 - mse: 0.0469 - mae: 0.1702 - val_loss: 0.1116 - val_mse: 0.0473 - val_mae: 0.1701 - lr: 0.0010\n",
      "Epoch 120/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1082 - mse: 0.0468 - mae: 0.1700 - val_loss: 0.1115 - val_mse: 0.0472 - val_mae: 0.1699 - lr: 0.0010\n",
      "Epoch 121/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1079 - mse: 0.0467 - mae: 0.1698 - val_loss: 0.1113 - val_mse: 0.0472 - val_mae: 0.1700 - lr: 0.0010\n",
      "Epoch 122/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1076 - mse: 0.0466 - mae: 0.1694 - val_loss: 0.1110 - val_mse: 0.0471 - val_mae: 0.1697 - lr: 0.0010\n",
      "Epoch 123/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1074 - mse: 0.0465 - mae: 0.1693 - val_loss: 0.1107 - val_mse: 0.0469 - val_mae: 0.1692 - lr: 0.0010\n",
      "Epoch 124/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1072 - mse: 0.0464 - mae: 0.1690 - val_loss: 0.1105 - val_mse: 0.0469 - val_mae: 0.1689 - lr: 0.0010\n",
      "Epoch 125/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1070 - mse: 0.0463 - mae: 0.1690 - val_loss: 0.1104 - val_mse: 0.0468 - val_mae: 0.1691 - lr: 0.0010\n",
      "Epoch 126/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1067 - mse: 0.0462 - mae: 0.1687 - val_loss: 0.1101 - val_mse: 0.0467 - val_mae: 0.1689 - lr: 0.0010\n",
      "Epoch 127/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1064 - mse: 0.0461 - mae: 0.1686 - val_loss: 0.1098 - val_mse: 0.0466 - val_mae: 0.1684 - lr: 0.0010\n",
      "Epoch 128/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1062 - mse: 0.0460 - mae: 0.1683 - val_loss: 0.1096 - val_mse: 0.0465 - val_mae: 0.1682 - lr: 0.0010\n",
      "Epoch 129/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1060 - mse: 0.0459 - mae: 0.1680 - val_loss: 0.1093 - val_mse: 0.0464 - val_mae: 0.1680 - lr: 0.0010\n",
      "Epoch 130/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1058 - mse: 0.0458 - mae: 0.1680 - val_loss: 0.1092 - val_mse: 0.0463 - val_mae: 0.1679 - lr: 0.0010\n",
      "Epoch 131/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1056 - mse: 0.0457 - mae: 0.1677 - val_loss: 0.1089 - val_mse: 0.0462 - val_mae: 0.1676 - lr: 0.0010\n",
      "Epoch 132/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1053 - mse: 0.0456 - mae: 0.1675 - val_loss: 0.1087 - val_mse: 0.0461 - val_mae: 0.1676 - lr: 0.0010\n",
      "Epoch 133/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1051 - mse: 0.0455 - mae: 0.1674 - val_loss: 0.1084 - val_mse: 0.0460 - val_mae: 0.1672 - lr: 0.0010\n",
      "Epoch 134/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1049 - mse: 0.0454 - mae: 0.1670 - val_loss: 0.1082 - val_mse: 0.0459 - val_mae: 0.1671 - lr: 0.0010\n",
      "Epoch 135/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1046 - mse: 0.0453 - mae: 0.1670 - val_loss: 0.1079 - val_mse: 0.0458 - val_mae: 0.1666 - lr: 0.0010\n",
      "Epoch 136/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1044 - mse: 0.0452 - mae: 0.1668 - val_loss: 0.1078 - val_mse: 0.0458 - val_mae: 0.1667 - lr: 0.0010\n",
      "Epoch 137/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1042 - mse: 0.0451 - mae: 0.1664 - val_loss: 0.1075 - val_mse: 0.0457 - val_mae: 0.1665 - lr: 0.0010\n",
      "Epoch 138/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1039 - mse: 0.0450 - mae: 0.1665 - val_loss: 0.1074 - val_mse: 0.0456 - val_mae: 0.1664 - lr: 0.0010\n",
      "Epoch 139/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1037 - mse: 0.0449 - mae: 0.1661 - val_loss: 0.1071 - val_mse: 0.0455 - val_mae: 0.1661 - lr: 0.0010\n",
      "Epoch 140/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1034 - mse: 0.0448 - mae: 0.1659 - val_loss: 0.1068 - val_mse: 0.0454 - val_mae: 0.1658 - lr: 0.0010\n",
      "Epoch 141/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1032 - mse: 0.0447 - mae: 0.1657 - val_loss: 0.1065 - val_mse: 0.0452 - val_mae: 0.1655 - lr: 0.0010\n",
      "Epoch 142/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1030 - mse: 0.0446 - mae: 0.1655 - val_loss: 0.1064 - val_mse: 0.0452 - val_mae: 0.1655 - lr: 0.0010\n",
      "Epoch 143/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1027 - mse: 0.0445 - mae: 0.1653 - val_loss: 0.1060 - val_mse: 0.0450 - val_mae: 0.1648 - lr: 0.0010\n",
      "Epoch 144/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1024 - mse: 0.0444 - mae: 0.1649 - val_loss: 0.1060 - val_mse: 0.0450 - val_mae: 0.1650 - lr: 0.0010\n",
      "Epoch 145/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1021 - mse: 0.0442 - mae: 0.1646 - val_loss: 0.1058 - val_mse: 0.0450 - val_mae: 0.1651 - lr: 0.0010\n",
      "Epoch 146/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1020 - mse: 0.0442 - mae: 0.1646 - val_loss: 0.1055 - val_mse: 0.0449 - val_mae: 0.1648 - lr: 0.0010\n",
      "Epoch 147/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1017 - mse: 0.0441 - mae: 0.1644 - val_loss: 0.1052 - val_mse: 0.0447 - val_mae: 0.1644 - lr: 0.0010\n",
      "Epoch 148/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.1015 - mse: 0.0440 - mae: 0.1641 - val_loss: 0.1050 - val_mse: 0.0447 - val_mae: 0.1643 - lr: 0.0010\n",
      "Epoch 149/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1012 - mse: 0.0439 - mae: 0.1640 - val_loss: 0.1048 - val_mse: 0.0445 - val_mae: 0.1638 - lr: 0.0010\n",
      "Epoch 150/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1009 - mse: 0.0438 - mae: 0.1638 - val_loss: 0.1045 - val_mse: 0.0444 - val_mae: 0.1636 - lr: 0.0010\n",
      "Epoch 151/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1007 - mse: 0.0437 - mae: 0.1634 - val_loss: 0.1044 - val_mse: 0.0444 - val_mae: 0.1636 - lr: 0.0010\n",
      "Epoch 152/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1005 - mse: 0.0436 - mae: 0.1632 - val_loss: 0.1042 - val_mse: 0.0443 - val_mae: 0.1633 - lr: 0.0010\n",
      "Epoch 153/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1003 - mse: 0.0435 - mae: 0.1630 - val_loss: 0.1041 - val_mse: 0.0443 - val_mae: 0.1636 - lr: 0.0010\n",
      "Epoch 154/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1001 - mse: 0.0434 - mae: 0.1630 - val_loss: 0.1038 - val_mse: 0.0441 - val_mae: 0.1629 - lr: 0.0010\n",
      "Epoch 155/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0998 - mse: 0.0433 - mae: 0.1628 - val_loss: 0.1035 - val_mse: 0.0440 - val_mae: 0.1623 - lr: 0.0010\n",
      "Epoch 156/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0996 - mse: 0.0432 - mae: 0.1625 - val_loss: 0.1034 - val_mse: 0.0440 - val_mae: 0.1625 - lr: 0.0010\n",
      "Epoch 157/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0994 - mse: 0.0431 - mae: 0.1622 - val_loss: 0.1032 - val_mse: 0.0439 - val_mae: 0.1622 - lr: 0.0010\n",
      "Epoch 158/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0991 - mse: 0.0430 - mae: 0.1620 - val_loss: 0.1031 - val_mse: 0.0439 - val_mae: 0.1624 - lr: 0.0010\n",
      "Epoch 159/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0989 - mse: 0.0430 - mae: 0.1619 - val_loss: 0.1028 - val_mse: 0.0438 - val_mae: 0.1619 - lr: 0.0010\n",
      "Epoch 160/500\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.0987 - mse: 0.0428 - mae: 0.1616 - val_loss: 0.1026 - val_mse: 0.0437 - val_mae: 0.1618 - lr: 0.0010\n",
      "Epoch 161/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0985 - mse: 0.0427 - mae: 0.1615 - val_loss: 0.1023 - val_mse: 0.0436 - val_mae: 0.1616 - lr: 0.0010\n",
      "Epoch 162/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0983 - mse: 0.0427 - mae: 0.1613 - val_loss: 0.1022 - val_mse: 0.0435 - val_mae: 0.1614 - lr: 0.0010\n",
      "Epoch 163/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0980 - mse: 0.0426 - mae: 0.1610 - val_loss: 0.1021 - val_mse: 0.0435 - val_mae: 0.1612 - lr: 0.0010\n",
      "Epoch 164/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0978 - mse: 0.0425 - mae: 0.1609 - val_loss: 0.1019 - val_mse: 0.0434 - val_mae: 0.1611 - lr: 0.0010\n",
      "Epoch 165/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0976 - mse: 0.0424 - mae: 0.1606 - val_loss: 0.1017 - val_mse: 0.0433 - val_mae: 0.1610 - lr: 0.0010\n",
      "Epoch 166/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0974 - mse: 0.0423 - mae: 0.1604 - val_loss: 0.1015 - val_mse: 0.0433 - val_mae: 0.1610 - lr: 0.0010\n",
      "Epoch 167/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0972 - mse: 0.0422 - mae: 0.1604 - val_loss: 0.1013 - val_mse: 0.0432 - val_mae: 0.1606 - lr: 0.0010\n",
      "Epoch 168/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0970 - mse: 0.0422 - mae: 0.1600 - val_loss: 0.1012 - val_mse: 0.0431 - val_mae: 0.1607 - lr: 0.0010\n",
      "Epoch 169/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0968 - mse: 0.0420 - mae: 0.1598 - val_loss: 0.1009 - val_mse: 0.0430 - val_mae: 0.1603 - lr: 0.0010\n",
      "Epoch 170/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0966 - mse: 0.0419 - mae: 0.1596 - val_loss: 0.1007 - val_mse: 0.0429 - val_mae: 0.1602 - lr: 0.0010\n",
      "Epoch 171/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0964 - mse: 0.0419 - mae: 0.1595 - val_loss: 0.1005 - val_mse: 0.0429 - val_mae: 0.1600 - lr: 0.0010\n",
      "Epoch 172/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0961 - mse: 0.0418 - mae: 0.1593 - val_loss: 0.1004 - val_mse: 0.0428 - val_mae: 0.1598 - lr: 0.0010\n",
      "Epoch 173/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0959 - mse: 0.0417 - mae: 0.1591 - val_loss: 0.1002 - val_mse: 0.0427 - val_mae: 0.1596 - lr: 0.0010\n",
      "Epoch 174/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0957 - mse: 0.0416 - mae: 0.1587 - val_loss: 0.1000 - val_mse: 0.0427 - val_mae: 0.1598 - lr: 0.0010\n",
      "Epoch 175/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0955 - mse: 0.0415 - mae: 0.1586 - val_loss: 0.0999 - val_mse: 0.0426 - val_mae: 0.1593 - lr: 0.0010\n",
      "Epoch 176/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0953 - mse: 0.0414 - mae: 0.1585 - val_loss: 0.0996 - val_mse: 0.0425 - val_mae: 0.1590 - lr: 0.0010\n",
      "Epoch 177/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0951 - mse: 0.0414 - mae: 0.1582 - val_loss: 0.0995 - val_mse: 0.0425 - val_mae: 0.1593 - lr: 0.0010\n",
      "Epoch 178/500\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.0949 - mse: 0.0413 - mae: 0.1581 - val_loss: 0.0993 - val_mse: 0.0424 - val_mae: 0.1588 - lr: 0.0010\n",
      "Epoch 179/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0947 - mse: 0.0412 - mae: 0.1579 - val_loss: 0.0991 - val_mse: 0.0423 - val_mae: 0.1588 - lr: 0.0010\n",
      "Epoch 180/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0945 - mse: 0.0411 - mae: 0.1577 - val_loss: 0.0990 - val_mse: 0.0422 - val_mae: 0.1586 - lr: 0.0010\n",
      "Epoch 181/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0943 - mse: 0.0410 - mae: 0.1574 - val_loss: 0.0987 - val_mse: 0.0421 - val_mae: 0.1582 - lr: 0.0010\n",
      "Epoch 182/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0941 - mse: 0.0410 - mae: 0.1574 - val_loss: 0.0985 - val_mse: 0.0420 - val_mae: 0.1580 - lr: 0.0010\n",
      "Epoch 183/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0938 - mse: 0.0408 - mae: 0.1571 - val_loss: 0.0983 - val_mse: 0.0420 - val_mae: 0.1580 - lr: 0.0010\n",
      "Epoch 184/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0937 - mse: 0.0408 - mae: 0.1570 - val_loss: 0.0982 - val_mse: 0.0419 - val_mae: 0.1576 - lr: 0.0010\n",
      "Epoch 185/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0935 - mse: 0.0407 - mae: 0.1567 - val_loss: 0.0980 - val_mse: 0.0418 - val_mae: 0.1577 - lr: 0.0010\n",
      "Epoch 186/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0933 - mse: 0.0406 - mae: 0.1565 - val_loss: 0.0978 - val_mse: 0.0418 - val_mae: 0.1578 - lr: 0.0010\n",
      "Epoch 187/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0930 - mse: 0.0405 - mae: 0.1565 - val_loss: 0.0976 - val_mse: 0.0417 - val_mae: 0.1574 - lr: 0.0010\n",
      "Epoch 188/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0929 - mse: 0.0404 - mae: 0.1562 - val_loss: 0.0974 - val_mse: 0.0416 - val_mae: 0.1572 - lr: 0.0010\n",
      "Epoch 189/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0926 - mse: 0.0403 - mae: 0.1560 - val_loss: 0.0973 - val_mse: 0.0416 - val_mae: 0.1576 - lr: 0.0010\n",
      "Epoch 190/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0925 - mse: 0.0403 - mae: 0.1559 - val_loss: 0.0970 - val_mse: 0.0415 - val_mae: 0.1570 - lr: 0.0010\n",
      "Epoch 191/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0922 - mse: 0.0402 - mae: 0.1557 - val_loss: 0.0968 - val_mse: 0.0414 - val_mae: 0.1567 - lr: 0.0010\n",
      "Epoch 192/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0920 - mse: 0.0401 - mae: 0.1555 - val_loss: 0.0967 - val_mse: 0.0413 - val_mae: 0.1566 - lr: 0.0010\n",
      "Epoch 193/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0918 - mse: 0.0400 - mae: 0.1551 - val_loss: 0.0964 - val_mse: 0.0412 - val_mae: 0.1563 - lr: 0.0010\n",
      "Epoch 194/500\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.0916 - mse: 0.0399 - mae: 0.1551 - val_loss: 0.0963 - val_mse: 0.0412 - val_mae: 0.1562 - lr: 0.0010\n",
      "Epoch 195/500\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.0914 - mse: 0.0398 - mae: 0.1548 - val_loss: 0.0961 - val_mse: 0.0411 - val_mae: 0.1562 - lr: 0.0010\n",
      "Epoch 196/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0912 - mse: 0.0398 - mae: 0.1547 - val_loss: 0.0959 - val_mse: 0.0410 - val_mae: 0.1561 - lr: 0.0010\n",
      "Epoch 197/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0910 - mse: 0.0397 - mae: 0.1545 - val_loss: 0.0958 - val_mse: 0.0410 - val_mae: 0.1561 - lr: 0.0010\n",
      "Epoch 198/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0908 - mse: 0.0396 - mae: 0.1543 - val_loss: 0.0955 - val_mse: 0.0409 - val_mae: 0.1558 - lr: 0.0010\n",
      "Epoch 199/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0906 - mse: 0.0395 - mae: 0.1541 - val_loss: 0.0954 - val_mse: 0.0409 - val_mae: 0.1559 - lr: 0.0010\n",
      "Epoch 200/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0903 - mse: 0.0394 - mae: 0.1540 - val_loss: 0.0952 - val_mse: 0.0407 - val_mae: 0.1555 - lr: 0.0010\n",
      "Epoch 201/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0901 - mse: 0.0393 - mae: 0.1538 - val_loss: 0.0948 - val_mse: 0.0406 - val_mae: 0.1551 - lr: 0.0010\n",
      "Epoch 202/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0899 - mse: 0.0392 - mae: 0.1536 - val_loss: 0.0947 - val_mse: 0.0405 - val_mae: 0.1550 - lr: 0.0010\n",
      "Epoch 203/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0897 - mse: 0.0391 - mae: 0.1533 - val_loss: 0.0945 - val_mse: 0.0404 - val_mae: 0.1548 - lr: 0.0010\n",
      "Epoch 204/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0895 - mse: 0.0390 - mae: 0.1533 - val_loss: 0.0943 - val_mse: 0.0403 - val_mae: 0.1543 - lr: 0.0010\n",
      "Epoch 205/500\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.0893 - mse: 0.0389 - mae: 0.1528 - val_loss: 0.0940 - val_mse: 0.0402 - val_mae: 0.1540 - lr: 0.0010\n",
      "Epoch 206/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0890 - mse: 0.0389 - mae: 0.1528 - val_loss: 0.0939 - val_mse: 0.0402 - val_mae: 0.1543 - lr: 0.0010\n",
      "Epoch 207/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0888 - mse: 0.0388 - mae: 0.1526 - val_loss: 0.0936 - val_mse: 0.0401 - val_mae: 0.1540 - lr: 0.0010\n",
      "Epoch 208/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0886 - mse: 0.0387 - mae: 0.1525 - val_loss: 0.0934 - val_mse: 0.0400 - val_mae: 0.1538 - lr: 0.0010\n",
      "Epoch 209/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0884 - mse: 0.0386 - mae: 0.1522 - val_loss: 0.0931 - val_mse: 0.0399 - val_mae: 0.1535 - lr: 0.0010\n",
      "Epoch 210/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0882 - mse: 0.0385 - mae: 0.1522 - val_loss: 0.0929 - val_mse: 0.0398 - val_mae: 0.1533 - lr: 0.0010\n",
      "Epoch 211/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0879 - mse: 0.0384 - mae: 0.1517 - val_loss: 0.0927 - val_mse: 0.0397 - val_mae: 0.1529 - lr: 0.0010\n",
      "Epoch 212/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0877 - mse: 0.0383 - mae: 0.1515 - val_loss: 0.0925 - val_mse: 0.0396 - val_mae: 0.1529 - lr: 0.0010\n",
      "Epoch 213/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0875 - mse: 0.0382 - mae: 0.1515 - val_loss: 0.0922 - val_mse: 0.0395 - val_mae: 0.1526 - lr: 0.0010\n",
      "Epoch 214/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0872 - mse: 0.0381 - mae: 0.1512 - val_loss: 0.0922 - val_mse: 0.0395 - val_mae: 0.1527 - lr: 0.0010\n",
      "Epoch 215/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0871 - mse: 0.0380 - mae: 0.1509 - val_loss: 0.0921 - val_mse: 0.0395 - val_mae: 0.1530 - lr: 0.0010\n",
      "Epoch 216/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0869 - mse: 0.0380 - mae: 0.1510 - val_loss: 0.0918 - val_mse: 0.0393 - val_mae: 0.1523 - lr: 0.0010\n",
      "Epoch 217/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0866 - mse: 0.0378 - mae: 0.1505 - val_loss: 0.0915 - val_mse: 0.0392 - val_mae: 0.1522 - lr: 0.0010\n",
      "Epoch 218/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0864 - mse: 0.0378 - mae: 0.1504 - val_loss: 0.0913 - val_mse: 0.0391 - val_mae: 0.1520 - lr: 0.0010\n",
      "Epoch 219/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0862 - mse: 0.0377 - mae: 0.1503 - val_loss: 0.0912 - val_mse: 0.0391 - val_mae: 0.1520 - lr: 0.0010\n",
      "Epoch 220/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0860 - mse: 0.0376 - mae: 0.1500 - val_loss: 0.0910 - val_mse: 0.0390 - val_mae: 0.1518 - lr: 0.0010\n",
      "Epoch 221/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0858 - mse: 0.0375 - mae: 0.1499 - val_loss: 0.0907 - val_mse: 0.0389 - val_mae: 0.1512 - lr: 0.0010\n",
      "Epoch 222/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0856 - mse: 0.0374 - mae: 0.1496 - val_loss: 0.0905 - val_mse: 0.0388 - val_mae: 0.1511 - lr: 0.0010\n",
      "Epoch 223/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0855 - mse: 0.0373 - mae: 0.1495 - val_loss: 0.0904 - val_mse: 0.0387 - val_mae: 0.1510 - lr: 0.0010\n",
      "Epoch 224/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0852 - mse: 0.0373 - mae: 0.1494 - val_loss: 0.0901 - val_mse: 0.0387 - val_mae: 0.1508 - lr: 0.0010\n",
      "Epoch 225/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0850 - mse: 0.0372 - mae: 0.1492 - val_loss: 0.0900 - val_mse: 0.0386 - val_mae: 0.1510 - lr: 0.0010\n",
      "Epoch 226/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0849 - mse: 0.0371 - mae: 0.1489 - val_loss: 0.0898 - val_mse: 0.0385 - val_mae: 0.1506 - lr: 0.0010\n",
      "Epoch 227/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0846 - mse: 0.0370 - mae: 0.1488 - val_loss: 0.0895 - val_mse: 0.0384 - val_mae: 0.1503 - lr: 0.0010\n",
      "Epoch 228/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0845 - mse: 0.0370 - mae: 0.1486 - val_loss: 0.0896 - val_mse: 0.0384 - val_mae: 0.1506 - lr: 0.0010\n",
      "Epoch 229/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0843 - mse: 0.0369 - mae: 0.1484 - val_loss: 0.0893 - val_mse: 0.0383 - val_mae: 0.1502 - lr: 0.0010\n",
      "Epoch 230/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0841 - mse: 0.0368 - mae: 0.1483 - val_loss: 0.0891 - val_mse: 0.0382 - val_mae: 0.1501 - lr: 0.0010\n",
      "Epoch 231/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0840 - mse: 0.0367 - mae: 0.1481 - val_loss: 0.0890 - val_mse: 0.0381 - val_mae: 0.1498 - lr: 0.0010\n",
      "Epoch 232/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0838 - mse: 0.0367 - mae: 0.1480 - val_loss: 0.0888 - val_mse: 0.0381 - val_mae: 0.1500 - lr: 0.0010\n",
      "Epoch 233/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0836 - mse: 0.0366 - mae: 0.1478 - val_loss: 0.0885 - val_mse: 0.0380 - val_mae: 0.1490 - lr: 0.0010\n",
      "Epoch 234/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0835 - mse: 0.0366 - mae: 0.1477 - val_loss: 0.0885 - val_mse: 0.0380 - val_mae: 0.1493 - lr: 0.0010\n",
      "Epoch 235/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0834 - mse: 0.0365 - mae: 0.1474 - val_loss: 0.0881 - val_mse: 0.0378 - val_mae: 0.1492 - lr: 0.0010\n",
      "Epoch 236/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0831 - mse: 0.0364 - mae: 0.1473 - val_loss: 0.0880 - val_mse: 0.0378 - val_mae: 0.1489 - lr: 0.0010\n",
      "Epoch 237/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0829 - mse: 0.0363 - mae: 0.1470 - val_loss: 0.0878 - val_mse: 0.0377 - val_mae: 0.1487 - lr: 0.0010\n",
      "Epoch 238/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0828 - mse: 0.0362 - mae: 0.1470 - val_loss: 0.0878 - val_mse: 0.0377 - val_mae: 0.1489 - lr: 0.0010\n",
      "Epoch 239/500\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.0826 - mse: 0.0362 - mae: 0.1468 - val_loss: 0.0877 - val_mse: 0.0376 - val_mae: 0.1488 - lr: 0.0010\n",
      "Epoch 240/500\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.0825 - mse: 0.0361 - mae: 0.1467 - val_loss: 0.0874 - val_mse: 0.0375 - val_mae: 0.1483 - lr: 0.0010\n",
      "Epoch 241/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0823 - mse: 0.0360 - mae: 0.1465 - val_loss: 0.0873 - val_mse: 0.0375 - val_mae: 0.1483 - lr: 0.0010\n",
      "Epoch 242/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0821 - mse: 0.0360 - mae: 0.1463 - val_loss: 0.0871 - val_mse: 0.0374 - val_mae: 0.1478 - lr: 0.0010\n",
      "Epoch 243/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0820 - mse: 0.0359 - mae: 0.1461 - val_loss: 0.0870 - val_mse: 0.0373 - val_mae: 0.1480 - lr: 0.0010\n",
      "Epoch 244/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0819 - mse: 0.0359 - mae: 0.1460 - val_loss: 0.0868 - val_mse: 0.0373 - val_mae: 0.1480 - lr: 0.0010\n",
      "Epoch 245/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0817 - mse: 0.0358 - mae: 0.1458 - val_loss: 0.0867 - val_mse: 0.0372 - val_mae: 0.1478 - lr: 0.0010\n",
      "Epoch 246/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0816 - mse: 0.0357 - mae: 0.1457 - val_loss: 0.0864 - val_mse: 0.0371 - val_mae: 0.1473 - lr: 0.0010\n",
      "Epoch 247/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0814 - mse: 0.0357 - mae: 0.1456 - val_loss: 0.0863 - val_mse: 0.0371 - val_mae: 0.1474 - lr: 0.0010\n",
      "Epoch 248/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0813 - mse: 0.0356 - mae: 0.1455 - val_loss: 0.0862 - val_mse: 0.0370 - val_mae: 0.1471 - lr: 0.0010\n",
      "Epoch 249/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0812 - mse: 0.0356 - mae: 0.1453 - val_loss: 0.0861 - val_mse: 0.0370 - val_mae: 0.1472 - lr: 0.0010\n",
      "Epoch 250/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0810 - mse: 0.0355 - mae: 0.1450 - val_loss: 0.0861 - val_mse: 0.0369 - val_mae: 0.1472 - lr: 0.0010\n",
      "Epoch 251/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0809 - mse: 0.0355 - mae: 0.1450 - val_loss: 0.0858 - val_mse: 0.0369 - val_mae: 0.1470 - lr: 0.0010\n",
      "Epoch 252/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0808 - mse: 0.0354 - mae: 0.1448 - val_loss: 0.0857 - val_mse: 0.0368 - val_mae: 0.1468 - lr: 0.0010\n",
      "Epoch 253/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0806 - mse: 0.0354 - mae: 0.1447 - val_loss: 0.0855 - val_mse: 0.0368 - val_mae: 0.1468 - lr: 0.0010\n",
      "Epoch 254/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0805 - mse: 0.0353 - mae: 0.1445 - val_loss: 0.0855 - val_mse: 0.0368 - val_mae: 0.1470 - lr: 0.0010\n",
      "Epoch 255/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0804 - mse: 0.0352 - mae: 0.1444 - val_loss: 0.0851 - val_mse: 0.0366 - val_mae: 0.1463 - lr: 0.0010\n",
      "Epoch 256/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0802 - mse: 0.0351 - mae: 0.1442 - val_loss: 0.0851 - val_mse: 0.0366 - val_mae: 0.1462 - lr: 0.0010\n",
      "Epoch 257/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0801 - mse: 0.0352 - mae: 0.1443 - val_loss: 0.0850 - val_mse: 0.0365 - val_mae: 0.1463 - lr: 0.0010\n",
      "Epoch 258/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0799 - mse: 0.0350 - mae: 0.1439 - val_loss: 0.0848 - val_mse: 0.0364 - val_mae: 0.1460 - lr: 0.0010\n",
      "Epoch 259/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0798 - mse: 0.0350 - mae: 0.1438 - val_loss: 0.0848 - val_mse: 0.0364 - val_mae: 0.1459 - lr: 0.0010\n",
      "Epoch 260/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0797 - mse: 0.0350 - mae: 0.1438 - val_loss: 0.0846 - val_mse: 0.0364 - val_mae: 0.1459 - lr: 0.0010\n",
      "Epoch 261/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0796 - mse: 0.0349 - mae: 0.1435 - val_loss: 0.0845 - val_mse: 0.0363 - val_mae: 0.1456 - lr: 0.0010\n",
      "Epoch 262/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0795 - mse: 0.0349 - mae: 0.1436 - val_loss: 0.0843 - val_mse: 0.0362 - val_mae: 0.1454 - lr: 0.0010\n",
      "Epoch 263/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0794 - mse: 0.0348 - mae: 0.1432 - val_loss: 0.0843 - val_mse: 0.0362 - val_mae: 0.1457 - lr: 0.0010\n",
      "Epoch 264/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0793 - mse: 0.0348 - mae: 0.1433 - val_loss: 0.0840 - val_mse: 0.0361 - val_mae: 0.1452 - lr: 0.0010\n",
      "Epoch 265/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0791 - mse: 0.0347 - mae: 0.1430 - val_loss: 0.0840 - val_mse: 0.0361 - val_mae: 0.1454 - lr: 0.0010\n",
      "Epoch 266/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0790 - mse: 0.0347 - mae: 0.1431 - val_loss: 0.0839 - val_mse: 0.0361 - val_mae: 0.1453 - lr: 0.0010\n",
      "Epoch 267/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0788 - mse: 0.0346 - mae: 0.1428 - val_loss: 0.0838 - val_mse: 0.0360 - val_mae: 0.1452 - lr: 0.0010\n",
      "Epoch 268/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0787 - mse: 0.0345 - mae: 0.1428 - val_loss: 0.0835 - val_mse: 0.0359 - val_mae: 0.1448 - lr: 0.0010\n",
      "Epoch 269/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0787 - mse: 0.0345 - mae: 0.1425 - val_loss: 0.0835 - val_mse: 0.0359 - val_mae: 0.1449 - lr: 0.0010\n",
      "Epoch 270/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0786 - mse: 0.0345 - mae: 0.1425 - val_loss: 0.0834 - val_mse: 0.0359 - val_mae: 0.1446 - lr: 0.0010\n",
      "Epoch 271/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0784 - mse: 0.0344 - mae: 0.1423 - val_loss: 0.0833 - val_mse: 0.0358 - val_mae: 0.1446 - lr: 0.0010\n",
      "Epoch 272/500\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0783 - mse: 0.0344 - mae: 0.1423 - val_loss: 0.0831 - val_mse: 0.0357 - val_mae: 0.1444 - lr: 0.0010\n",
      "Epoch 273/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0782 - mse: 0.0343 - mae: 0.1421 - val_loss: 0.0829 - val_mse: 0.0356 - val_mae: 0.1442 - lr: 0.0010\n",
      "Epoch 274/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0781 - mse: 0.0342 - mae: 0.1419 - val_loss: 0.0829 - val_mse: 0.0357 - val_mae: 0.1444 - lr: 0.0010\n",
      "Epoch 275/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0780 - mse: 0.0343 - mae: 0.1420 - val_loss: 0.0827 - val_mse: 0.0356 - val_mae: 0.1439 - lr: 0.0010\n",
      "Epoch 276/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0779 - mse: 0.0342 - mae: 0.1418 - val_loss: 0.0826 - val_mse: 0.0355 - val_mae: 0.1439 - lr: 0.0010\n",
      "Epoch 277/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0778 - mse: 0.0341 - mae: 0.1417 - val_loss: 0.0825 - val_mse: 0.0355 - val_mae: 0.1439 - lr: 0.0010\n",
      "Epoch 278/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0777 - mse: 0.0341 - mae: 0.1416 - val_loss: 0.0824 - val_mse: 0.0354 - val_mae: 0.1435 - lr: 0.0010\n",
      "Epoch 279/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0776 - mse: 0.0340 - mae: 0.1414 - val_loss: 0.0823 - val_mse: 0.0354 - val_mae: 0.1435 - lr: 0.0010\n",
      "Epoch 280/500\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0775 - mse: 0.0340 - mae: 0.1414 - val_loss: 0.0822 - val_mse: 0.0354 - val_mae: 0.1437 - lr: 0.0010\n",
      "Epoch 281/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0774 - mse: 0.0340 - mae: 0.1412 - val_loss: 0.0821 - val_mse: 0.0353 - val_mae: 0.1436 - lr: 0.0010\n",
      "Epoch 282/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0773 - mse: 0.0339 - mae: 0.1411 - val_loss: 0.0820 - val_mse: 0.0353 - val_mae: 0.1433 - lr: 0.0010\n",
      "Epoch 283/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0772 - mse: 0.0339 - mae: 0.1411 - val_loss: 0.0819 - val_mse: 0.0353 - val_mae: 0.1435 - lr: 0.0010\n",
      "Epoch 284/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0771 - mse: 0.0339 - mae: 0.1409 - val_loss: 0.0817 - val_mse: 0.0351 - val_mae: 0.1430 - lr: 0.0010\n",
      "Epoch 285/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0770 - mse: 0.0337 - mae: 0.1407 - val_loss: 0.0817 - val_mse: 0.0352 - val_mae: 0.1432 - lr: 0.0010\n",
      "Epoch 286/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0769 - mse: 0.0338 - mae: 0.1408 - val_loss: 0.0816 - val_mse: 0.0351 - val_mae: 0.1429 - lr: 0.0010\n",
      "Epoch 287/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0768 - mse: 0.0337 - mae: 0.1405 - val_loss: 0.0815 - val_mse: 0.0351 - val_mae: 0.1432 - lr: 0.0010\n",
      "Epoch 288/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0767 - mse: 0.0337 - mae: 0.1406 - val_loss: 0.0814 - val_mse: 0.0351 - val_mae: 0.1427 - lr: 0.0010\n",
      "Epoch 289/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0764 - mse: 0.0335 - mae: 0.1402 - val_loss: 0.0804 - val_mse: 0.0345 - val_mae: 0.1418 - lr: 0.0010\n",
      "Epoch 290/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0754 - mse: 0.0330 - mae: 0.1394 - val_loss: 0.0794 - val_mse: 0.0341 - val_mae: 0.1413 - lr: 0.0010\n",
      "Epoch 291/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0746 - mse: 0.0326 - mae: 0.1391 - val_loss: 0.0786 - val_mse: 0.0336 - val_mae: 0.1404 - lr: 0.0010\n",
      "Epoch 292/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0741 - mse: 0.0323 - mae: 0.1384 - val_loss: 0.0780 - val_mse: 0.0334 - val_mae: 0.1397 - lr: 0.0010\n",
      "Epoch 293/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0736 - mse: 0.0322 - mae: 0.1383 - val_loss: 0.0774 - val_mse: 0.0331 - val_mae: 0.1394 - lr: 0.0010\n",
      "Epoch 294/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0733 - mse: 0.0320 - mae: 0.1379 - val_loss: 0.0770 - val_mse: 0.0330 - val_mae: 0.1389 - lr: 0.0010\n",
      "Epoch 295/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0729 - mse: 0.0318 - mae: 0.1375 - val_loss: 0.0768 - val_mse: 0.0328 - val_mae: 0.1387 - lr: 0.0010\n",
      "Epoch 296/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0726 - mse: 0.0317 - mae: 0.1372 - val_loss: 0.0764 - val_mse: 0.0326 - val_mae: 0.1383 - lr: 0.0010\n",
      "Epoch 297/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0723 - mse: 0.0316 - mae: 0.1370 - val_loss: 0.0760 - val_mse: 0.0325 - val_mae: 0.1377 - lr: 0.0010\n",
      "Epoch 298/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0721 - mse: 0.0314 - mae: 0.1366 - val_loss: 0.0759 - val_mse: 0.0324 - val_mae: 0.1377 - lr: 0.0010\n",
      "Epoch 299/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0718 - mse: 0.0313 - mae: 0.1363 - val_loss: 0.0756 - val_mse: 0.0323 - val_mae: 0.1378 - lr: 0.0010\n",
      "Epoch 300/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0715 - mse: 0.0311 - mae: 0.1361 - val_loss: 0.0754 - val_mse: 0.0322 - val_mae: 0.1372 - lr: 0.0010\n",
      "Epoch 301/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0714 - mse: 0.0311 - mae: 0.1361 - val_loss: 0.0751 - val_mse: 0.0321 - val_mae: 0.1368 - lr: 0.0010\n",
      "Epoch 302/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0712 - mse: 0.0310 - mae: 0.1356 - val_loss: 0.0751 - val_mse: 0.0320 - val_mae: 0.1370 - lr: 0.0010\n",
      "Epoch 303/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0710 - mse: 0.0309 - mae: 0.1354 - val_loss: 0.0748 - val_mse: 0.0319 - val_mae: 0.1369 - lr: 0.0010\n",
      "Epoch 304/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0707 - mse: 0.0308 - mae: 0.1353 - val_loss: 0.0745 - val_mse: 0.0318 - val_mae: 0.1363 - lr: 0.0010\n",
      "Epoch 305/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0705 - mse: 0.0307 - mae: 0.1350 - val_loss: 0.0744 - val_mse: 0.0317 - val_mae: 0.1363 - lr: 0.0010\n",
      "Epoch 306/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0703 - mse: 0.0306 - mae: 0.1348 - val_loss: 0.0742 - val_mse: 0.0316 - val_mae: 0.1358 - lr: 0.0010\n",
      "Epoch 307/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0702 - mse: 0.0306 - mae: 0.1347 - val_loss: 0.0740 - val_mse: 0.0316 - val_mae: 0.1357 - lr: 0.0010\n",
      "Epoch 308/500\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.0700 - mse: 0.0305 - mae: 0.1343 - val_loss: 0.0738 - val_mse: 0.0315 - val_mae: 0.1354 - lr: 0.0010\n",
      "Epoch 309/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0698 - mse: 0.0304 - mae: 0.1343 - val_loss: 0.0736 - val_mse: 0.0314 - val_mae: 0.1352 - lr: 0.0010\n",
      "Epoch 310/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0698 - mse: 0.0304 - mae: 0.1342 - val_loss: 0.0735 - val_mse: 0.0314 - val_mae: 0.1354 - lr: 0.0010\n",
      "Epoch 311/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0697 - mse: 0.0303 - mae: 0.1342 - val_loss: 0.0734 - val_mse: 0.0313 - val_mae: 0.1351 - lr: 0.0010\n",
      "Epoch 312/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0695 - mse: 0.0302 - mae: 0.1339 - val_loss: 0.0732 - val_mse: 0.0312 - val_mae: 0.1349 - lr: 0.0010\n",
      "Epoch 313/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0693 - mse: 0.0301 - mae: 0.1335 - val_loss: 0.0730 - val_mse: 0.0311 - val_mae: 0.1346 - lr: 0.0010\n",
      "Epoch 314/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0691 - mse: 0.0301 - mae: 0.1335 - val_loss: 0.0729 - val_mse: 0.0310 - val_mae: 0.1345 - lr: 0.0010\n",
      "Epoch 315/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0690 - mse: 0.0300 - mae: 0.1333 - val_loss: 0.0728 - val_mse: 0.0310 - val_mae: 0.1343 - lr: 0.0010\n",
      "Epoch 316/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0688 - mse: 0.0299 - mae: 0.1332 - val_loss: 0.0727 - val_mse: 0.0310 - val_mae: 0.1345 - lr: 0.0010\n",
      "Epoch 317/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0687 - mse: 0.0299 - mae: 0.1330 - val_loss: 0.0725 - val_mse: 0.0309 - val_mae: 0.1341 - lr: 0.0010\n",
      "Epoch 318/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0686 - mse: 0.0298 - mae: 0.1329 - val_loss: 0.0723 - val_mse: 0.0308 - val_mae: 0.1339 - lr: 0.0010\n",
      "Epoch 319/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0686 - mse: 0.0298 - mae: 0.1328 - val_loss: 0.0723 - val_mse: 0.0308 - val_mae: 0.1342 - lr: 0.0010\n",
      "Epoch 320/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0684 - mse: 0.0297 - mae: 0.1327 - val_loss: 0.0721 - val_mse: 0.0307 - val_mae: 0.1334 - lr: 0.0010\n",
      "Epoch 321/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0683 - mse: 0.0297 - mae: 0.1324 - val_loss: 0.0720 - val_mse: 0.0307 - val_mae: 0.1338 - lr: 0.0010\n",
      "Epoch 322/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0682 - mse: 0.0296 - mae: 0.1324 - val_loss: 0.0719 - val_mse: 0.0306 - val_mae: 0.1335 - lr: 0.0010\n",
      "Epoch 323/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0680 - mse: 0.0296 - mae: 0.1323 - val_loss: 0.0717 - val_mse: 0.0305 - val_mae: 0.1332 - lr: 0.0010\n",
      "Epoch 324/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0679 - mse: 0.0295 - mae: 0.1321 - val_loss: 0.0717 - val_mse: 0.0305 - val_mae: 0.1329 - lr: 0.0010\n",
      "Epoch 325/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0677 - mse: 0.0294 - mae: 0.1319 - val_loss: 0.0714 - val_mse: 0.0304 - val_mae: 0.1329 - lr: 0.0010\n",
      "Epoch 326/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0676 - mse: 0.0294 - mae: 0.1318 - val_loss: 0.0713 - val_mse: 0.0303 - val_mae: 0.1327 - lr: 0.0010\n",
      "Epoch 327/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0676 - mse: 0.0294 - mae: 0.1317 - val_loss: 0.0713 - val_mse: 0.0303 - val_mae: 0.1327 - lr: 0.0010\n",
      "Epoch 328/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0675 - mse: 0.0293 - mae: 0.1316 - val_loss: 0.0712 - val_mse: 0.0303 - val_mae: 0.1325 - lr: 0.0010\n",
      "Epoch 329/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0673 - mse: 0.0292 - mae: 0.1313 - val_loss: 0.0710 - val_mse: 0.0302 - val_mae: 0.1325 - lr: 0.0010\n",
      "Epoch 330/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0672 - mse: 0.0292 - mae: 0.1313 - val_loss: 0.0709 - val_mse: 0.0301 - val_mae: 0.1322 - lr: 0.0010\n",
      "Epoch 331/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0671 - mse: 0.0291 - mae: 0.1312 - val_loss: 0.0708 - val_mse: 0.0301 - val_mae: 0.1327 - lr: 0.0010\n",
      "Epoch 332/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0670 - mse: 0.0291 - mae: 0.1310 - val_loss: 0.0706 - val_mse: 0.0300 - val_mae: 0.1319 - lr: 0.0010\n",
      "Epoch 333/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0670 - mse: 0.0291 - mae: 0.1310 - val_loss: 0.0706 - val_mse: 0.0300 - val_mae: 0.1319 - lr: 0.0010\n",
      "Epoch 334/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0669 - mse: 0.0290 - mae: 0.1308 - val_loss: 0.0705 - val_mse: 0.0300 - val_mae: 0.1318 - lr: 0.0010\n",
      "Epoch 335/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0668 - mse: 0.0290 - mae: 0.1309 - val_loss: 0.0705 - val_mse: 0.0300 - val_mae: 0.1318 - lr: 0.0010\n",
      "Epoch 336/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0666 - mse: 0.0289 - mae: 0.1306 - val_loss: 0.0703 - val_mse: 0.0298 - val_mae: 0.1316 - lr: 0.0010\n",
      "Epoch 337/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0665 - mse: 0.0289 - mae: 0.1305 - val_loss: 0.0701 - val_mse: 0.0298 - val_mae: 0.1313 - lr: 0.0010\n",
      "Epoch 338/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0664 - mse: 0.0288 - mae: 0.1303 - val_loss: 0.0700 - val_mse: 0.0298 - val_mae: 0.1312 - lr: 0.0010\n",
      "Epoch 339/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0663 - mse: 0.0288 - mae: 0.1304 - val_loss: 0.0700 - val_mse: 0.0298 - val_mae: 0.1314 - lr: 0.0010\n",
      "Epoch 340/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0663 - mse: 0.0288 - mae: 0.1303 - val_loss: 0.0698 - val_mse: 0.0297 - val_mae: 0.1311 - lr: 0.0010\n",
      "Epoch 341/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0661 - mse: 0.0287 - mae: 0.1300 - val_loss: 0.0698 - val_mse: 0.0296 - val_mae: 0.1312 - lr: 0.0010\n",
      "Epoch 342/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0660 - mse: 0.0286 - mae: 0.1300 - val_loss: 0.0697 - val_mse: 0.0296 - val_mae: 0.1308 - lr: 0.0010\n",
      "Epoch 343/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0659 - mse: 0.0286 - mae: 0.1298 - val_loss: 0.0696 - val_mse: 0.0295 - val_mae: 0.1306 - lr: 0.0010\n",
      "Epoch 344/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0659 - mse: 0.0286 - mae: 0.1299 - val_loss: 0.0694 - val_mse: 0.0295 - val_mae: 0.1305 - lr: 0.0010\n",
      "Epoch 345/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0658 - mse: 0.0285 - mae: 0.1296 - val_loss: 0.0694 - val_mse: 0.0294 - val_mae: 0.1306 - lr: 0.0010\n",
      "Epoch 346/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0657 - mse: 0.0285 - mae: 0.1296 - val_loss: 0.0692 - val_mse: 0.0294 - val_mae: 0.1304 - lr: 0.0010\n",
      "Epoch 347/500\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0656 - mse: 0.0285 - mae: 0.1295 - val_loss: 0.0692 - val_mse: 0.0294 - val_mae: 0.1305 - lr: 0.0010\n",
      "Epoch 348/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0656 - mse: 0.0284 - mae: 0.1294 - val_loss: 0.0690 - val_mse: 0.0293 - val_mae: 0.1301 - lr: 0.0010\n",
      "Epoch 349/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0654 - mse: 0.0283 - mae: 0.1292 - val_loss: 0.0690 - val_mse: 0.0292 - val_mae: 0.1299 - lr: 0.0010\n",
      "Epoch 350/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0653 - mse: 0.0284 - mae: 0.1293 - val_loss: 0.0689 - val_mse: 0.0292 - val_mae: 0.1299 - lr: 0.0010\n",
      "Epoch 351/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0653 - mse: 0.0283 - mae: 0.1291 - val_loss: 0.0688 - val_mse: 0.0292 - val_mae: 0.1301 - lr: 0.0010\n",
      "Epoch 352/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0652 - mse: 0.0282 - mae: 0.1289 - val_loss: 0.0687 - val_mse: 0.0291 - val_mae: 0.1298 - lr: 0.0010\n",
      "Epoch 353/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0651 - mse: 0.0282 - mae: 0.1288 - val_loss: 0.0685 - val_mse: 0.0291 - val_mae: 0.1298 - lr: 0.0010\n",
      "Epoch 354/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0650 - mse: 0.0282 - mae: 0.1288 - val_loss: 0.0685 - val_mse: 0.0291 - val_mae: 0.1297 - lr: 0.0010\n",
      "Epoch 355/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0649 - mse: 0.0281 - mae: 0.1286 - val_loss: 0.0684 - val_mse: 0.0290 - val_mae: 0.1295 - lr: 0.0010\n",
      "Epoch 356/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0648 - mse: 0.0281 - mae: 0.1285 - val_loss: 0.0683 - val_mse: 0.0289 - val_mae: 0.1292 - lr: 0.0010\n",
      "Epoch 357/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0648 - mse: 0.0281 - mae: 0.1285 - val_loss: 0.0682 - val_mse: 0.0289 - val_mae: 0.1293 - lr: 0.0010\n",
      "Epoch 358/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0647 - mse: 0.0280 - mae: 0.1283 - val_loss: 0.0681 - val_mse: 0.0289 - val_mae: 0.1292 - lr: 0.0010\n",
      "Epoch 359/500\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0646 - mse: 0.0280 - mae: 0.1282 - val_loss: 0.0681 - val_mse: 0.0289 - val_mae: 0.1293 - lr: 0.0010\n",
      "Epoch 360/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0645 - mse: 0.0280 - mae: 0.1282 - val_loss: 0.0679 - val_mse: 0.0288 - val_mae: 0.1290 - lr: 0.0010\n",
      "Epoch 361/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0645 - mse: 0.0279 - mae: 0.1280 - val_loss: 0.0679 - val_mse: 0.0288 - val_mae: 0.1292 - lr: 0.0010\n",
      "Epoch 362/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0645 - mse: 0.0279 - mae: 0.1281 - val_loss: 0.0678 - val_mse: 0.0288 - val_mae: 0.1289 - lr: 0.0010\n",
      "Epoch 363/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0643 - mse: 0.0278 - mae: 0.1278 - val_loss: 0.0677 - val_mse: 0.0287 - val_mae: 0.1288 - lr: 0.0010\n",
      "Epoch 364/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0642 - mse: 0.0278 - mae: 0.1278 - val_loss: 0.0676 - val_mse: 0.0287 - val_mae: 0.1288 - lr: 0.0010\n",
      "Epoch 365/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0642 - mse: 0.0278 - mae: 0.1278 - val_loss: 0.0675 - val_mse: 0.0286 - val_mae: 0.1288 - lr: 0.0010\n",
      "Epoch 366/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0640 - mse: 0.0277 - mae: 0.1276 - val_loss: 0.0674 - val_mse: 0.0286 - val_mae: 0.1283 - lr: 0.0010\n",
      "Epoch 367/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0640 - mse: 0.0277 - mae: 0.1274 - val_loss: 0.0673 - val_mse: 0.0286 - val_mae: 0.1284 - lr: 0.0010\n",
      "Epoch 368/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0639 - mse: 0.0277 - mae: 0.1274 - val_loss: 0.0671 - val_mse: 0.0285 - val_mae: 0.1281 - lr: 0.0010\n",
      "Epoch 369/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0638 - mse: 0.0276 - mae: 0.1272 - val_loss: 0.0669 - val_mse: 0.0283 - val_mae: 0.1279 - lr: 0.0010\n",
      "Epoch 370/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0636 - mse: 0.0276 - mae: 0.1272 - val_loss: 0.0668 - val_mse: 0.0283 - val_mae: 0.1279 - lr: 0.0010\n",
      "Epoch 371/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0635 - mse: 0.0275 - mae: 0.1271 - val_loss: 0.0667 - val_mse: 0.0283 - val_mae: 0.1277 - lr: 0.0010\n",
      "Epoch 372/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0635 - mse: 0.0275 - mae: 0.1269 - val_loss: 0.0667 - val_mse: 0.0283 - val_mae: 0.1279 - lr: 0.0010\n",
      "Epoch 373/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0634 - mse: 0.0275 - mae: 0.1269 - val_loss: 0.0665 - val_mse: 0.0282 - val_mae: 0.1273 - lr: 0.0010\n",
      "Epoch 374/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0634 - mse: 0.0274 - mae: 0.1267 - val_loss: 0.0664 - val_mse: 0.0281 - val_mae: 0.1273 - lr: 0.0010\n",
      "Epoch 375/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0633 - mse: 0.0274 - mae: 0.1267 - val_loss: 0.0663 - val_mse: 0.0281 - val_mae: 0.1274 - lr: 0.0010\n",
      "Epoch 376/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0632 - mse: 0.0274 - mae: 0.1267 - val_loss: 0.0664 - val_mse: 0.0282 - val_mae: 0.1272 - lr: 0.0010\n",
      "Epoch 377/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0632 - mse: 0.0274 - mae: 0.1266 - val_loss: 0.0664 - val_mse: 0.0281 - val_mae: 0.1273 - lr: 0.0010\n",
      "Epoch 378/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0632 - mse: 0.0273 - mae: 0.1266 - val_loss: 0.0661 - val_mse: 0.0280 - val_mae: 0.1269 - lr: 0.0010\n",
      "Epoch 379/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0630 - mse: 0.0273 - mae: 0.1264 - val_loss: 0.0662 - val_mse: 0.0281 - val_mae: 0.1268 - lr: 0.0010\n",
      "Epoch 380/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0629 - mse: 0.0272 - mae: 0.1261 - val_loss: 0.0660 - val_mse: 0.0280 - val_mae: 0.1269 - lr: 0.0010\n",
      "Epoch 381/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0628 - mse: 0.0272 - mae: 0.1262 - val_loss: 0.0659 - val_mse: 0.0279 - val_mae: 0.1268 - lr: 0.0010\n",
      "Epoch 382/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0627 - mse: 0.0272 - mae: 0.1261 - val_loss: 0.0657 - val_mse: 0.0279 - val_mae: 0.1266 - lr: 0.0010\n",
      "Epoch 383/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0627 - mse: 0.0271 - mae: 0.1259 - val_loss: 0.0657 - val_mse: 0.0278 - val_mae: 0.1264 - lr: 0.0010\n",
      "Epoch 384/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0625 - mse: 0.0271 - mae: 0.1260 - val_loss: 0.0656 - val_mse: 0.0279 - val_mae: 0.1266 - lr: 0.0010\n",
      "Epoch 385/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0625 - mse: 0.0271 - mae: 0.1258 - val_loss: 0.0657 - val_mse: 0.0278 - val_mae: 0.1265 - lr: 0.0010\n",
      "Epoch 386/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0625 - mse: 0.0270 - mae: 0.1258 - val_loss: 0.0657 - val_mse: 0.0279 - val_mae: 0.1269 - lr: 0.0010\n",
      "Epoch 387/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0624 - mse: 0.0270 - mae: 0.1256 - val_loss: 0.0654 - val_mse: 0.0278 - val_mae: 0.1264 - lr: 0.0010\n",
      "Epoch 388/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0623 - mse: 0.0269 - mae: 0.1255 - val_loss: 0.0654 - val_mse: 0.0277 - val_mae: 0.1262 - lr: 0.0010\n",
      "Epoch 389/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0622 - mse: 0.0269 - mae: 0.1254 - val_loss: 0.0653 - val_mse: 0.0277 - val_mae: 0.1264 - lr: 0.0010\n",
      "Epoch 390/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0622 - mse: 0.0269 - mae: 0.1253 - val_loss: 0.0651 - val_mse: 0.0275 - val_mae: 0.1257 - lr: 0.0010\n",
      "Epoch 391/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0621 - mse: 0.0269 - mae: 0.1253 - val_loss: 0.0649 - val_mse: 0.0275 - val_mae: 0.1257 - lr: 0.0010\n",
      "Epoch 392/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0620 - mse: 0.0268 - mae: 0.1252 - val_loss: 0.0652 - val_mse: 0.0276 - val_mae: 0.1259 - lr: 0.0010\n",
      "Epoch 393/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0620 - mse: 0.0268 - mae: 0.1252 - val_loss: 0.0651 - val_mse: 0.0276 - val_mae: 0.1257 - lr: 0.0010\n",
      "Epoch 394/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0619 - mse: 0.0268 - mae: 0.1252 - val_loss: 0.0650 - val_mse: 0.0275 - val_mae: 0.1257 - lr: 0.0010\n",
      "Epoch 395/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0618 - mse: 0.0268 - mae: 0.1251 - val_loss: 0.0649 - val_mse: 0.0275 - val_mae: 0.1256 - lr: 0.0010\n",
      "Epoch 396/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0617 - mse: 0.0267 - mae: 0.1249 - val_loss: 0.0647 - val_mse: 0.0274 - val_mae: 0.1254 - lr: 0.0010\n",
      "Epoch 397/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0617 - mse: 0.0266 - mae: 0.1247 - val_loss: 0.0646 - val_mse: 0.0274 - val_mae: 0.1252 - lr: 0.0010\n",
      "Epoch 398/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0616 - mse: 0.0267 - mae: 0.1248 - val_loss: 0.0645 - val_mse: 0.0273 - val_mae: 0.1253 - lr: 0.0010\n",
      "Epoch 399/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0615 - mse: 0.0266 - mae: 0.1247 - val_loss: 0.0645 - val_mse: 0.0273 - val_mae: 0.1254 - lr: 0.0010\n",
      "Epoch 400/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0614 - mse: 0.0266 - mae: 0.1245 - val_loss: 0.0644 - val_mse: 0.0273 - val_mae: 0.1253 - lr: 0.0010\n",
      "Epoch 401/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0613 - mse: 0.0265 - mae: 0.1244 - val_loss: 0.0643 - val_mse: 0.0273 - val_mae: 0.1249 - lr: 0.0010\n",
      "Epoch 402/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0613 - mse: 0.0265 - mae: 0.1244 - val_loss: 0.0642 - val_mse: 0.0272 - val_mae: 0.1253 - lr: 0.0010\n",
      "Epoch 403/500\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0612 - mse: 0.0265 - mae: 0.1244 - val_loss: 0.0642 - val_mse: 0.0272 - val_mae: 0.1252 - lr: 0.0010\n",
      "Epoch 404/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0611 - mse: 0.0265 - mae: 0.1243 - val_loss: 0.0642 - val_mse: 0.0272 - val_mae: 0.1249 - lr: 0.0010\n",
      "Epoch 405/500\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0611 - mse: 0.0265 - mae: 0.1242 - val_loss: 0.0640 - val_mse: 0.0271 - val_mae: 0.1248 - lr: 0.0010\n",
      "Epoch 406/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0611 - mse: 0.0264 - mae: 0.1240 - val_loss: 0.0640 - val_mse: 0.0271 - val_mae: 0.1245 - lr: 0.0010\n",
      "Epoch 407/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0609 - mse: 0.0264 - mae: 0.1240 - val_loss: 0.0639 - val_mse: 0.0271 - val_mae: 0.1247 - lr: 0.0010\n",
      "Epoch 408/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0609 - mse: 0.0263 - mae: 0.1240 - val_loss: 0.0639 - val_mse: 0.0271 - val_mae: 0.1247 - lr: 0.0010\n",
      "Epoch 409/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0608 - mse: 0.0263 - mae: 0.1239 - val_loss: 0.0637 - val_mse: 0.0270 - val_mae: 0.1242 - lr: 0.0010\n",
      "Epoch 410/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0608 - mse: 0.0262 - mae: 0.1237 - val_loss: 0.0636 - val_mse: 0.0270 - val_mae: 0.1241 - lr: 0.0010\n",
      "Epoch 411/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0607 - mse: 0.0263 - mae: 0.1237 - val_loss: 0.0637 - val_mse: 0.0270 - val_mae: 0.1243 - lr: 0.0010\n",
      "Epoch 412/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0607 - mse: 0.0262 - mae: 0.1236 - val_loss: 0.0637 - val_mse: 0.0269 - val_mae: 0.1239 - lr: 0.0010\n",
      "Epoch 413/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0606 - mse: 0.0262 - mae: 0.1235 - val_loss: 0.0636 - val_mse: 0.0269 - val_mae: 0.1242 - lr: 0.0010\n",
      "Epoch 414/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0605 - mse: 0.0262 - mae: 0.1235 - val_loss: 0.0634 - val_mse: 0.0269 - val_mae: 0.1241 - lr: 0.0010\n",
      "Epoch 415/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0605 - mse: 0.0261 - mae: 0.1234 - val_loss: 0.0633 - val_mse: 0.0268 - val_mae: 0.1240 - lr: 0.0010\n",
      "Epoch 416/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0604 - mse: 0.0261 - mae: 0.1234 - val_loss: 0.0632 - val_mse: 0.0267 - val_mae: 0.1238 - lr: 0.0010\n",
      "Epoch 417/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0602 - mse: 0.0260 - mae: 0.1231 - val_loss: 0.0630 - val_mse: 0.0267 - val_mae: 0.1238 - lr: 0.0010\n",
      "Epoch 418/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0602 - mse: 0.0260 - mae: 0.1231 - val_loss: 0.0629 - val_mse: 0.0267 - val_mae: 0.1235 - lr: 0.0010\n",
      "Epoch 419/500\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.0601 - mse: 0.0260 - mae: 0.1229 - val_loss: 0.0631 - val_mse: 0.0267 - val_mae: 0.1235 - lr: 0.0010\n",
      "Epoch 420/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0601 - mse: 0.0260 - mae: 0.1229 - val_loss: 0.0630 - val_mse: 0.0267 - val_mae: 0.1234 - lr: 0.0010\n",
      "Epoch 421/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0600 - mse: 0.0260 - mae: 0.1229 - val_loss: 0.0628 - val_mse: 0.0266 - val_mae: 0.1233 - lr: 0.0010\n",
      "Epoch 422/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0600 - mse: 0.0260 - mae: 0.1230 - val_loss: 0.0630 - val_mse: 0.0267 - val_mae: 0.1234 - lr: 0.0010\n",
      "Epoch 423/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0599 - mse: 0.0259 - mae: 0.1227 - val_loss: 0.0629 - val_mse: 0.0267 - val_mae: 0.1233 - lr: 0.0010\n",
      "Epoch 424/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0599 - mse: 0.0259 - mae: 0.1227 - val_loss: 0.0627 - val_mse: 0.0266 - val_mae: 0.1233 - lr: 0.0010\n",
      "Epoch 425/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0598 - mse: 0.0258 - mae: 0.1226 - val_loss: 0.0627 - val_mse: 0.0265 - val_mae: 0.1230 - lr: 0.0010\n",
      "Epoch 426/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0597 - mse: 0.0258 - mae: 0.1224 - val_loss: 0.0625 - val_mse: 0.0265 - val_mae: 0.1230 - lr: 0.0010\n",
      "Epoch 427/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0596 - mse: 0.0258 - mae: 0.1224 - val_loss: 0.0625 - val_mse: 0.0265 - val_mae: 0.1228 - lr: 0.0010\n",
      "Epoch 428/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0596 - mse: 0.0257 - mae: 0.1223 - val_loss: 0.0625 - val_mse: 0.0265 - val_mae: 0.1232 - lr: 0.0010\n",
      "Epoch 429/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0594 - mse: 0.0257 - mae: 0.1223 - val_loss: 0.0623 - val_mse: 0.0264 - val_mae: 0.1230 - lr: 0.0010\n",
      "Epoch 430/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0594 - mse: 0.0257 - mae: 0.1221 - val_loss: 0.0623 - val_mse: 0.0264 - val_mae: 0.1227 - lr: 0.0010\n",
      "Epoch 431/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0594 - mse: 0.0257 - mae: 0.1222 - val_loss: 0.0624 - val_mse: 0.0264 - val_mae: 0.1228 - lr: 0.0010\n",
      "Epoch 432/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0594 - mse: 0.0257 - mae: 0.1222 - val_loss: 0.0624 - val_mse: 0.0264 - val_mae: 0.1232 - lr: 0.0010\n",
      "Epoch 433/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0593 - mse: 0.0256 - mae: 0.1220 - val_loss: 0.0622 - val_mse: 0.0264 - val_mae: 0.1230 - lr: 0.0010\n",
      "Epoch 434/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0593 - mse: 0.0256 - mae: 0.1220 - val_loss: 0.0621 - val_mse: 0.0263 - val_mae: 0.1228 - lr: 0.0010\n",
      "Epoch 435/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0591 - mse: 0.0256 - mae: 0.1219 - val_loss: 0.0621 - val_mse: 0.0263 - val_mae: 0.1228 - lr: 0.0010\n",
      "Epoch 436/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0592 - mse: 0.0255 - mae: 0.1218 - val_loss: 0.0623 - val_mse: 0.0264 - val_mae: 0.1233 - lr: 0.0010\n",
      "Epoch 437/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0591 - mse: 0.0255 - mae: 0.1219 - val_loss: 0.0620 - val_mse: 0.0263 - val_mae: 0.1225 - lr: 0.0010\n",
      "Epoch 438/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0590 - mse: 0.0255 - mae: 0.1216 - val_loss: 0.0618 - val_mse: 0.0262 - val_mae: 0.1222 - lr: 0.0010\n",
      "Epoch 439/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0589 - mse: 0.0254 - mae: 0.1215 - val_loss: 0.0619 - val_mse: 0.0262 - val_mae: 0.1224 - lr: 0.0010\n",
      "Epoch 440/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0588 - mse: 0.0254 - mae: 0.1215 - val_loss: 0.0617 - val_mse: 0.0261 - val_mae: 0.1221 - lr: 0.0010\n",
      "Epoch 441/500\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0588 - mse: 0.0254 - mae: 0.1213 - val_loss: 0.0617 - val_mse: 0.0261 - val_mae: 0.1223 - lr: 0.0010\n",
      "Epoch 442/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0588 - mse: 0.0254 - mae: 0.1214 - val_loss: 0.0615 - val_mse: 0.0261 - val_mae: 0.1220 - lr: 0.0010\n",
      "Epoch 443/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0586 - mse: 0.0253 - mae: 0.1211 - val_loss: 0.0614 - val_mse: 0.0260 - val_mae: 0.1219 - lr: 0.0010\n",
      "Epoch 444/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0587 - mse: 0.0254 - mae: 0.1213 - val_loss: 0.0616 - val_mse: 0.0261 - val_mae: 0.1221 - lr: 0.0010\n",
      "Epoch 445/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0586 - mse: 0.0253 - mae: 0.1212 - val_loss: 0.0614 - val_mse: 0.0260 - val_mae: 0.1218 - lr: 0.0010\n",
      "Epoch 446/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0585 - mse: 0.0253 - mae: 0.1210 - val_loss: 0.0613 - val_mse: 0.0260 - val_mae: 0.1216 - lr: 0.0010\n",
      "Epoch 447/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0584 - mse: 0.0253 - mae: 0.1210 - val_loss: 0.0614 - val_mse: 0.0260 - val_mae: 0.1217 - lr: 0.0010\n",
      "Epoch 448/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0584 - mse: 0.0252 - mae: 0.1209 - val_loss: 0.0612 - val_mse: 0.0259 - val_mae: 0.1216 - lr: 0.0010\n",
      "Epoch 449/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0583 - mse: 0.0252 - mae: 0.1207 - val_loss: 0.0612 - val_mse: 0.0259 - val_mae: 0.1216 - lr: 0.0010\n",
      "Epoch 450/500\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0582 - mse: 0.0252 - mae: 0.1208 - val_loss: 0.0611 - val_mse: 0.0258 - val_mae: 0.1214 - lr: 0.0010\n",
      "Epoch 451/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0582 - mse: 0.0251 - mae: 0.1206 - val_loss: 0.0611 - val_mse: 0.0259 - val_mae: 0.1215 - lr: 0.0010\n",
      "Epoch 452/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0581 - mse: 0.0251 - mae: 0.1205 - val_loss: 0.0609 - val_mse: 0.0257 - val_mae: 0.1210 - lr: 0.0010\n",
      "Epoch 453/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0580 - mse: 0.0251 - mae: 0.1205 - val_loss: 0.0608 - val_mse: 0.0257 - val_mae: 0.1210 - lr: 0.0010\n",
      "Epoch 454/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0580 - mse: 0.0250 - mae: 0.1203 - val_loss: 0.0607 - val_mse: 0.0257 - val_mae: 0.1212 - lr: 0.0010\n",
      "Epoch 455/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0579 - mse: 0.0250 - mae: 0.1203 - val_loss: 0.0608 - val_mse: 0.0258 - val_mae: 0.1212 - lr: 0.0010\n",
      "Epoch 456/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0578 - mse: 0.0249 - mae: 0.1201 - val_loss: 0.0607 - val_mse: 0.0257 - val_mae: 0.1212 - lr: 0.0010\n",
      "Epoch 457/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0577 - mse: 0.0249 - mae: 0.1202 - val_loss: 0.0606 - val_mse: 0.0256 - val_mae: 0.1211 - lr: 0.0010\n",
      "Epoch 458/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0577 - mse: 0.0249 - mae: 0.1201 - val_loss: 0.0605 - val_mse: 0.0256 - val_mae: 0.1211 - lr: 0.0010\n",
      "Epoch 459/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0577 - mse: 0.0249 - mae: 0.1200 - val_loss: 0.0605 - val_mse: 0.0256 - val_mae: 0.1208 - lr: 0.0010\n",
      "Epoch 460/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0576 - mse: 0.0249 - mae: 0.1199 - val_loss: 0.0604 - val_mse: 0.0256 - val_mae: 0.1207 - lr: 0.0010\n",
      "Epoch 461/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0575 - mse: 0.0249 - mae: 0.1199 - val_loss: 0.0603 - val_mse: 0.0255 - val_mae: 0.1205 - lr: 0.0010\n",
      "Epoch 462/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0575 - mse: 0.0248 - mae: 0.1198 - val_loss: 0.0604 - val_mse: 0.0256 - val_mae: 0.1207 - lr: 0.0010\n",
      "Epoch 463/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0575 - mse: 0.0249 - mae: 0.1199 - val_loss: 0.0604 - val_mse: 0.0256 - val_mae: 0.1206 - lr: 0.0010\n",
      "Epoch 464/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0574 - mse: 0.0248 - mae: 0.1196 - val_loss: 0.0603 - val_mse: 0.0255 - val_mae: 0.1204 - lr: 0.0010\n",
      "Epoch 465/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0574 - mse: 0.0248 - mae: 0.1197 - val_loss: 0.0603 - val_mse: 0.0255 - val_mae: 0.1206 - lr: 0.0010\n",
      "Epoch 466/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0572 - mse: 0.0247 - mae: 0.1196 - val_loss: 0.0601 - val_mse: 0.0255 - val_mae: 0.1203 - lr: 0.0010\n",
      "Epoch 467/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0572 - mse: 0.0247 - mae: 0.1194 - val_loss: 0.0601 - val_mse: 0.0254 - val_mae: 0.1204 - lr: 0.0010\n",
      "Epoch 468/500\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0571 - mse: 0.0247 - mae: 0.1193 - val_loss: 0.0600 - val_mse: 0.0254 - val_mae: 0.1201 - lr: 0.0010\n",
      "Epoch 469/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0570 - mse: 0.0246 - mae: 0.1192 - val_loss: 0.0600 - val_mse: 0.0254 - val_mae: 0.1203 - lr: 0.0010\n",
      "Epoch 470/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0570 - mse: 0.0246 - mae: 0.1193 - val_loss: 0.0600 - val_mse: 0.0254 - val_mae: 0.1201 - lr: 0.0010\n",
      "Epoch 471/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0570 - mse: 0.0246 - mae: 0.1192 - val_loss: 0.0598 - val_mse: 0.0253 - val_mae: 0.1200 - lr: 0.0010\n",
      "Epoch 472/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0569 - mse: 0.0246 - mae: 0.1190 - val_loss: 0.0597 - val_mse: 0.0253 - val_mae: 0.1200 - lr: 0.0010\n",
      "Epoch 473/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0568 - mse: 0.0246 - mae: 0.1191 - val_loss: 0.0598 - val_mse: 0.0253 - val_mae: 0.1201 - lr: 0.0010\n",
      "Epoch 474/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0567 - mse: 0.0245 - mae: 0.1188 - val_loss: 0.0596 - val_mse: 0.0252 - val_mae: 0.1196 - lr: 0.0010\n",
      "Epoch 475/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0567 - mse: 0.0245 - mae: 0.1188 - val_loss: 0.0595 - val_mse: 0.0252 - val_mae: 0.1197 - lr: 0.0010\n",
      "Epoch 476/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0566 - mse: 0.0245 - mae: 0.1187 - val_loss: 0.0595 - val_mse: 0.0251 - val_mae: 0.1196 - lr: 0.0010\n",
      "Epoch 477/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0566 - mse: 0.0244 - mae: 0.1186 - val_loss: 0.0595 - val_mse: 0.0252 - val_mae: 0.1197 - lr: 0.0010\n",
      "Epoch 478/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0565 - mse: 0.0244 - mae: 0.1184 - val_loss: 0.0593 - val_mse: 0.0251 - val_mae: 0.1194 - lr: 0.0010\n",
      "Epoch 479/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0565 - mse: 0.0244 - mae: 0.1184 - val_loss: 0.0593 - val_mse: 0.0251 - val_mae: 0.1195 - lr: 0.0010\n",
      "Epoch 480/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0565 - mse: 0.0244 - mae: 0.1186 - val_loss: 0.0593 - val_mse: 0.0251 - val_mae: 0.1193 - lr: 0.0010\n",
      "Epoch 481/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0564 - mse: 0.0243 - mae: 0.1182 - val_loss: 0.0593 - val_mse: 0.0251 - val_mae: 0.1195 - lr: 0.0010\n",
      "Epoch 482/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0563 - mse: 0.0243 - mae: 0.1183 - val_loss: 0.0592 - val_mse: 0.0250 - val_mae: 0.1194 - lr: 0.0010\n",
      "Epoch 483/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0562 - mse: 0.0243 - mae: 0.1183 - val_loss: 0.0590 - val_mse: 0.0250 - val_mae: 0.1191 - lr: 0.0010\n",
      "Epoch 484/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0562 - mse: 0.0242 - mae: 0.1181 - val_loss: 0.0592 - val_mse: 0.0251 - val_mae: 0.1196 - lr: 0.0010\n",
      "Epoch 485/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0561 - mse: 0.0242 - mae: 0.1181 - val_loss: 0.0591 - val_mse: 0.0250 - val_mae: 0.1192 - lr: 0.0010\n",
      "Epoch 486/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0561 - mse: 0.0242 - mae: 0.1179 - val_loss: 0.0588 - val_mse: 0.0249 - val_mae: 0.1189 - lr: 0.0010\n",
      "Epoch 487/500\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.0560 - mse: 0.0242 - mae: 0.1179 - val_loss: 0.0587 - val_mse: 0.0249 - val_mae: 0.1188 - lr: 0.0010\n",
      "Epoch 488/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0559 - mse: 0.0242 - mae: 0.1179 - val_loss: 0.0588 - val_mse: 0.0248 - val_mae: 0.1187 - lr: 0.0010\n",
      "Epoch 489/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0559 - mse: 0.0241 - mae: 0.1176 - val_loss: 0.0587 - val_mse: 0.0248 - val_mae: 0.1186 - lr: 0.0010\n",
      "Epoch 490/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0559 - mse: 0.0241 - mae: 0.1176 - val_loss: 0.0587 - val_mse: 0.0248 - val_mae: 0.1188 - lr: 0.0010\n",
      "Epoch 491/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0558 - mse: 0.0241 - mae: 0.1176 - val_loss: 0.0585 - val_mse: 0.0248 - val_mae: 0.1185 - lr: 0.0010\n",
      "Epoch 492/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0557 - mse: 0.0240 - mae: 0.1173 - val_loss: 0.0585 - val_mse: 0.0248 - val_mae: 0.1185 - lr: 0.0010\n",
      "Epoch 493/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0558 - mse: 0.0241 - mae: 0.1174 - val_loss: 0.0586 - val_mse: 0.0248 - val_mae: 0.1183 - lr: 0.0010\n",
      "Epoch 494/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0557 - mse: 0.0240 - mae: 0.1173 - val_loss: 0.0585 - val_mse: 0.0248 - val_mae: 0.1185 - lr: 0.0010\n",
      "Epoch 495/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0556 - mse: 0.0240 - mae: 0.1175 - val_loss: 0.0584 - val_mse: 0.0247 - val_mae: 0.1182 - lr: 0.0010\n",
      "Epoch 496/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0556 - mse: 0.0240 - mae: 0.1174 - val_loss: 0.0583 - val_mse: 0.0246 - val_mae: 0.1181 - lr: 0.0010\n",
      "Epoch 497/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0555 - mse: 0.0239 - mae: 0.1172 - val_loss: 0.0582 - val_mse: 0.0246 - val_mae: 0.1182 - lr: 0.0010\n",
      "Epoch 498/500\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0554 - mse: 0.0239 - mae: 0.1170 - val_loss: 0.0582 - val_mse: 0.0246 - val_mae: 0.1180 - lr: 0.0010\n",
      "Epoch 499/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0553 - mse: 0.0238 - mae: 0.1169 - val_loss: 0.0581 - val_mse: 0.0246 - val_mae: 0.1180 - lr: 0.0010\n",
      "Epoch 500/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0553 - mse: 0.0239 - mae: 0.1170 - val_loss: 0.0581 - val_mse: 0.0245 - val_mae: 0.1179 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [X_real_train, X_imag_train],\n",
    "    y_train,\n",
    "    epochs=500,\n",
    "    batch_size=16,\n",
    "    validation_data=([X_real_test, X_imag_test], y_test),\n",
    "    callbacks=[adjust_lr, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0581 - mse: 0.0245 - mae: 0.1179\n",
      "Test MSE: 0.0245, Test MAE: 0.1179\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_mse, test_mae = model.evaluate([X_real_test, X_imag_test], y_test)\n",
    "print(f\"Test MSE: {test_mse:.4f}, Test MAE: {test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear layer input: (None, 32)\n",
      "linear layer output: (None, 16)\n",
      "7/7 [==============================] - 1s 2ms/step\n",
      "Comparison of predictions and ground truth:\n",
      "Sample 1:\n",
      "  Predicted:    [1. 1. 1. 1. 1. 0. 3. 1. 1. 2. 1. 3. 1. 1. 4. 2.]\n",
      "  Ground Truth: [1. 0. 1. 1. 1. 1. 3. 0. 1. 3. 1. 2. 1. 1. 4. 2.]\n",
      "----------------------------------------\n",
      "Sample 2:\n",
      "  Predicted:    [4. 1. 3. 2. 2. 0. 3. 2. 1. 2. 1. 3. 3. 1. 1. 1.]\n",
      "  Ground Truth: [4. 0. 2. 0. 2. 0. 3. 2. 1. 2. 2. 2. 3. 0. 1. 0.]\n",
      "----------------------------------------\n",
      "Sample 3:\n",
      "  Predicted:    [3. 1. 1. 1. 2. 0. 4. 3. 3. 3. 0. 2. 4. 3. 0. 1.]\n",
      "  Ground Truth: [3. 2. 0. 0. 2. 0. 4. 4. 3. 3. 1. 3. 4. 3. 0. 1.]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = model.predict([X_real_test, X_imag_test])\n",
    "y_pred_test_rescaled = y_pred_test * (q - 1)\n",
    "# y_pred_test_rescaled = (y_pred_test * y_std) + y_mean\n",
    "y_test_rescaled = y_test * (q - 1)\n",
    "\n",
    "print(\"Comparison of predictions and ground truth:\")\n",
    "for i in range(3):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  Predicted:    {np.round(y_pred_test_rescaled[i])}\")\n",
    "    print(f\"  Ground Truth: {np.round(y_test_rescaled[i])}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoded Unseen Data (Real and Imaginary Parts):\n",
      "Real Part Shape: (5, 16)\n",
      "Imaginary Part Shape: (5, 16)\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "\n",
      "Comparison of Unseen Data and Predictions:\n",
      "============================================================\n",
      "Sample 1:\n",
      "  Ground Truth: [2 3 2 1 0 0 0 4 4 1 2 4 2 2 4 4]\n",
      "  Prediction  : [2 3 1 3 0 0 0 3 4 2 3 3 2 2 4 3]\n",
      "------------------------------------------------------------\n",
      "Sample 2:\n",
      "  Ground Truth: [1 3 0 4 4 2 3 4 3 0 2 1 0 3 3 0]\n",
      "  Prediction  : [1 1 1 3 4 3 3 4 3 1 1 1 0 2 3 0]\n",
      "------------------------------------------------------------\n",
      "Sample 3:\n",
      "  Ground Truth: [3 1 1 3 3 1 3 4 2 4 3 3 2 3 2 1]\n",
      "  Prediction  : [3 1 1 2 3 1 3 3 2 4 2 3 2 3 2 1]\n",
      "------------------------------------------------------------\n",
      "Sample 4:\n",
      "  Ground Truth: [2 2 4 3 1 0 2 0 3 2 0 4 0 4 1 3]\n",
      "  Prediction  : [2 3 3 3 1 2 2 0 3 1 1 3 0 3 1 3]\n",
      "------------------------------------------------------------\n",
      "Sample 5:\n",
      "  Ground Truth: [0 2 3 0 2 2 2 2 0 1 4 4 4 1 2 2]\n",
      "  Prediction  : [0 2 3 1 2 1 2 2 0 1 4 3 4 2 2 2]\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_unseen_samples = 5\n",
    "unseen_data = np.random.randint(0, q, size=(num_unseen_samples, n))\n",
    "\n",
    "unseen_data_normalized = unseen_data.astype(np.float32) / (q - 1)\n",
    "\n",
    "# print(\"Unseen Input Data (Before Encoding):\")\n",
    "# print(unseen_data)\n",
    "\n",
    "unseen_encoded = np.array([dft(message, n) for message in unseen_data])\n",
    "unseen_encoded[np.abs(unseen_encoded) < 1e-10] = 0\n",
    "unseen_encoded = np.round(unseen_encoded, decimals=10)\n",
    "\n",
    "X_real_unseen = np.real(unseen_encoded).astype(np.float32)\n",
    "X_imag_unseen = np.imag(unseen_encoded).astype(np.float32)\n",
    "\n",
    "print(\"\\nEncoded Unseen Data (Real and Imaginary Parts):\")\n",
    "print(\"Real Part Shape:\", X_real_unseen.shape)\n",
    "print(\"Imaginary Part Shape:\", X_imag_unseen.shape)\n",
    "\n",
    "y_pred_unseen = model.predict([X_real_unseen, X_imag_unseen])\n",
    "\n",
    "y_pred_unseen_rescaled = y_pred_unseen * (q - 1)\n",
    "\n",
    "y_pred_unseen_final = np.mod(np.round(y_pred_unseen_rescaled), q)\n",
    "\n",
    "print(\"\\nComparison of Unseen Data and Predictions:\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(num_unseen_samples):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  Ground Truth: {unseen_data[i]}\")\n",
    "    print(f\"  Prediction  : {y_pred_unseen_final[i].astype(int)}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check gradient flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real_input has no trainable variables.\n",
      "imag_input has no trainable variables.\n",
      "real_layer1 Gradient Mean: 0.7311445\n",
      "imag_layer1 Gradient Mean: 0.49622643\n",
      "leaky_re_lu has no trainable variables.\n",
      "leaky_re_lu_3 has no trainable variables.\n",
      "real_support_layer_1 Gradient Mean: 2.6891472\n",
      "imag_support_layer_1 Gradient Mean: 1.015368\n",
      "leaky_re_lu_1 has no trainable variables.\n",
      "leaky_re_lu_4 has no trainable variables.\n",
      "real_layer2 Gradient Mean: 1.8199035\n",
      "imag_layer2 Gradient Mean: 1.1226095\n",
      "leaky_re_lu_2 has no trainable variables.\n",
      "leaky_re_lu_5 has no trainable variables.\n",
      "merge_real_imag has no trainable variables.\n",
      "output_layer Gradient Mean: 2.4537137\n",
      "activation has no trainable variables.\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    if layer.trainable_variables:\n",
    "        grads = tf.reduce_mean(tf.abs(layer.trainable_variables[0]))\n",
    "        print(layer.name, \"Gradient Mean:\", grads.numpy())\n",
    "    else:\n",
    "        print(layer.name, \"has no trainable variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear layer input: (4, 32)\n",
      "linear layer output: (4, 16)\n",
      " Gradient OK for real_layer1/kernel_b1:0, mean: -0.0003058232250623405\n",
      " Gradient OK for real_layer1/kernel_b2:0, mean: -0.0018490756629034877\n",
      " Gradient OK for real_layer1/kernel_d1:0, mean: -0.0016712784999981523\n",
      " Gradient OK for real_layer1/kernel_d2:0, mean: -0.0006350121111609042\n",
      " Gradient OK for real_layer1/bias:0, mean: -0.002307265531271696\n",
      " Gradient OK for imag_layer1/kernel_b1:0, mean: -0.005752933211624622\n",
      " Gradient OK for imag_layer1/kernel_b2:0, mean: 0.0029827088583260775\n",
      " Gradient OK for imag_layer1/kernel_d1:0, mean: -0.0005948604666627944\n",
      " Gradient OK for imag_layer1/kernel_d2:0, mean: 0.0026720885653048754\n",
      " Gradient OK for imag_layer1/bias:0, mean: -0.013041270896792412\n",
      " Gradient OK for real_support_layer_1/kernel_m:0, mean: -0.01654862053692341\n",
      " Gradient OK for real_support_layer_1/bias:0, mean: -0.0005753248115070164\n",
      " Gradient OK for imag_support_layer_1/kernel_m:0, mean: -0.021153796464204788\n",
      " Gradient OK for imag_support_layer_1/bias:0, mean: -0.029882721602916718\n",
      " Gradient OK for real_layer2/kernel_d1:0, mean: 0.0001237084943568334\n",
      " Gradient OK for real_layer2/bias:0, mean: -0.004611015785485506\n",
      " Gradient OK for imag_layer2/kernel_d1:0, mean: 0.009626083076000214\n",
      " Gradient OK for imag_layer2/bias:0, mean: -0.04361357539892197\n",
      " Gradient OK for output_layer/kernel_m_1:0, mean: -0.027082305401563644\n",
      " Gradient OK for output_layer/bias:0, mean: -0.020479612052440643\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4 \n",
    "X_real_dummy = np.random.rand(batch_size, X_real_train.shape[1]).astype(np.float32)\n",
    "X_imag_dummy = np.random.rand(batch_size, X_imag_train.shape[1]).astype(np.float32)\n",
    "y_dummy = np.random.rand(batch_size, y_train.shape[1]).astype(np.float32)\n",
    "\n",
    "def check_gradients(model, X_real_dummy, X_imag_dummy, y_dummy):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(model.trainable_variables)\n",
    "        predictions = model([X_real_dummy, X_imag_dummy], training=True)\n",
    "        loss = tf.keras.losses.mean_squared_error(y_dummy, predictions)\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    for i, (grad, var) in enumerate(zip(grads, model.trainable_variables)):\n",
    "        if grad is None:\n",
    "            print(f\" Gradient is None for {var.name} at index {i}\")\n",
    "        else:\n",
    "            print(f\" Gradient OK for {var.name}, mean: {tf.reduce_mean(grad).numpy()}\")\n",
    "\n",
    "check_gradients(model, X_real_dummy, X_imag_dummy, y_dummy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
