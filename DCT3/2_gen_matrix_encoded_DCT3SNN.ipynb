{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCT3SNN - DCT-2 Encoded Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is encoded using the padded generator matrix. the imaginary and real values are parellel processed until concatenation. The parallel execution mimics the DCT-III SNN. One additional layer is included to account for the scaling diagonal matrix $\\hat{D}_n$ in the classical algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (r + 1) | (q - 1)\n",
    "# (r + 1) | n\n",
    "# q is a prime number\n",
    "# n = 2^t\n",
    "# r < n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample test on logic used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we don't know what x is: [2 3 3 4 1 1 0 0 1 1 4 0 4 4 3 0]\n",
      "\n",
      "what we are given:  [31.        -0.j          6.94075665+4.13918073j -5.53553391-6.12132034j\n",
      " -6.11600343-1.63664851j -2.        -5.j          5.2875763 -1.29350276j\n",
      "  1.53553391+1.87867966j -2.11232953-7.51767352j  5.        -0.j\n",
      " -2.11232953+7.51767352j  1.53553391-1.87867966j  5.2875763 +1.29350276j\n",
      " -2.        +5.j         -6.11600343+1.63664851j -5.53553391+6.12132034j\n",
      "  6.94075665-4.13918073j]\n",
      "\n",
      "DCT coefficients constructed using given fft:  [ 7.75        2.58555209 -2.34171864 -2.23719547 -1.32977661  1.43311912\n",
      "  0.8204165  -2.2634539   1.25        1.58080529 -0.25065808  1.28457041\n",
      "  1.36260566 -0.07396588  1.74081621 -1.2158476 ]\n",
      "\n",
      "Reconstructed signal:  [ 2.  0.  3.  3.  3.  4.  4.  4.  1. -0.  1.  4. -0.  1.  0.  1.]\n",
      "\n",
      "After permutation:  [ 2.  3.  3.  4.  1.  1. -0.  0.  1.  1.  4. -0.  4.  4.  3.  0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.fft import fft, ifft\n",
    "from scipy.fftpack import dct\n",
    "\n",
    "x = np.random.randint(0, 5, 16)\n",
    "print(\"we don't know what x is:\", x)\n",
    "N = len(x)\n",
    "X_dft = fft(x)\n",
    "print(\"\\nwhat we are given: \", X_dft)\n",
    "\n",
    "k = np.arange(N)\n",
    "phase_shift = np.exp(-1j * np.pi * k / (2 * N))\n",
    "alpha_k = np.where(k == 0, np.sqrt(1/N), np.sqrt(2/N))\n",
    "X_dct = np.real(alpha_k * phase_shift * X_dft)\n",
    "print(\"\\nDCT coefficients constructed using given fft: \", X_dct)\n",
    "\n",
    "X = dct(X_dct, type=3, norm='ortho')\n",
    "print(\"\\nReconstructed signal: \", np.round(X))\n",
    "\n",
    "perm = np.concatenate((X[::2], X[1::2][::-1])) # even indexes + odd indexes - reversed order\n",
    "print(\"\\nAfter permutation: \", np.round(perm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset -f\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (x, y, z, w) --> (1, 2, 3, 4)\n",
    "global z0\n",
    "global w0\n",
    "\n",
    "z0 = 3\n",
    "w0 = 4\n",
    "\n",
    "n = 16\n",
    "q = 5\n",
    "\n",
    "global r\n",
    "r = 3\n",
    "\n",
    "num_samples = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original n: 16\n",
      "Padded n: 16\n",
      "Generated dataset shape: (1000, 16)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def next_power_of_two(x):\n",
    "    return 1 if x == 0 else 2**(x - 1).bit_length()\n",
    "\n",
    "n_padded = next_power_of_two(n)\n",
    "\n",
    "dataset = np.random.randint(0, q, size=(num_samples, n))\n",
    "\n",
    "if n_padded > n:\n",
    "    pad_width = n_padded - n\n",
    "    dataset = np.pad(dataset, ((0, 0), (0, pad_width)), mode='constant', constant_values=0)\n",
    "\n",
    "print(\"Original n:\", n)\n",
    "print(\"Padded n:\", n_padded)\n",
    "print(\"Generated dataset shape:\", dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 4 2 ... 1 3 1]\n",
      " [3 4 0 ... 3 2 3]\n",
      " [3 0 2 ... 0 1 4]\n",
      " ...\n",
      " [4 4 4 ... 0 2 2]\n",
      " [4 2 4 ... 4 0 0]\n",
      " [2 2 2 ... 0 4 2]]\n",
      "(1000, 16)\n",
      "(16,)\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(dataset.shape)\n",
    "print(dataset[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\tilde{M}_{kj} = \\left[ \\left( \\frac{w_0}{z_0} \\right)^j \\zeta^{kj} \\right]_{k,j=0}^{n-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padded_generator_matrix(N, w0, z0):\n",
    "    n = np.arange(N)\n",
    "    k = n.reshape((N, 1))\n",
    "    zeta = np.exp(-2j * np.pi / N)\n",
    "    M_tilde = ((w0 / z0) ** n) * (zeta ** (k * n))\n",
    "    return M_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 16)\n"
     ]
    }
   ],
   "source": [
    "M_tilde = padded_generator_matrix(n_padded, w0, z0)\n",
    "print(M_tilde.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 649.89449698  +0.j           69.97998169+371.13335899j\n",
      "   -83.92481787+137.40000024j ...  -88.37954911-103.22398682j\n",
      "   -83.92481787-137.40000024j   69.97998169-371.13335899j]\n",
      " [ 695.87003951  +0.j          268.59479107+394.57072111j\n",
      "   -36.50274968+309.16102475j ...  -47.36788425-179.88976988j\n",
      "   -36.50274968-309.16102475j  268.59479107-394.57072111j]\n",
      " [ 540.63797786  +0.j          235.29809241+209.31041098j\n",
      "   176.66145652+204.71795948j ...  108.87153914-320.22828928j\n",
      "   176.66145652-204.71795948j  235.29809241-209.31041098j]\n",
      " ...\n",
      " [ 460.45123158  +0.j          162.64910275+168.11969395j\n",
      "    54.45039471+219.81284778j ...  -26.91067254-124.72256855j\n",
      "    54.45039471-219.81284778j  162.64910275-168.11969395j]\n",
      " [ 558.17613119  +0.j          -92.41727316+362.85897707j\n",
      "  -228.64175513 +13.85662093j ...  -76.51498185+176.39278765j\n",
      "  -228.64175513 -13.85662093j  -92.41727316-362.85897707j]\n",
      " [ 671.99178181  +0.j          161.97417378+352.04621278j\n",
      "    43.09272997+244.01831359j ...  -56.20864468-237.97753496j\n",
      "    43.09272997-244.01831359j  161.97417378-352.04621278j]]\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset = np.array([np.dot(M_tilde, x) for x in dataset])\n",
    "encoded_dataset[np.abs(encoded_dataset) < 1e-10] = 0\n",
    "encoded_dataset = np.round(encoded_dataset, decimals=10)\n",
    "print(encoded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 16)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 2 4 4 1 2 2 2 4 3 2 4 1 3 1]\n",
      "[ 649.89449698  +0.j           69.97998169+371.13335899j\n",
      "  -83.92481787+137.40000024j  -88.37954911+103.22398682j\n",
      "  -74.53901813 +41.73652084j  -97.48254893 +37.96448017j\n",
      " -147.95858217-108.15626408j   47.97234649-148.66691513j\n",
      "  146.76987906  -0.j           47.97234649+148.66691513j\n",
      " -147.95858217+108.15626408j  -97.48254893 -37.96448017j\n",
      "  -74.53901813 -41.73652084j  -88.37954911-103.22398682j\n",
      "  -83.92481787-137.40000024j   69.97998169-371.13335899j]\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])\n",
    "print(encoded_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 16)\n",
      "[ 162.47362425   37.48388585  -19.6246229   -19.30741408  -18.70056955\n",
      "  -24.06835946  -64.73967959  -20.23391967   36.69246976   51.39054587\n",
      "    2.73200003  -28.08438553  -23.71790458  -43.99421304  -53.43352006\n",
      " -128.1585124 ]\n"
     ]
    }
   ],
   "source": [
    "k = np.arange(n_padded)\n",
    "phase_shift = np.exp(-1j * np.pi * k / (2 * n_padded))\n",
    "alpha_k = np.where(k == 0, np.sqrt(1/n_padded), np.sqrt(2/n_padded))\n",
    "\n",
    "dct2_dataset = np.real(encoded_dataset * phase_shift) * alpha_k  # Apply transformation to entire dataset\n",
    "dct2_dataset = np.round(dct2_dataset, 10)  # Round for numerical stability\n",
    "\n",
    "print(dct2_dataset.shape)\n",
    "print(dct2_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.31321882 -0.59372357 -0.75737619 -0.39345835 -0.21874733 -0.32403293\n",
      " -1.52746419 -0.03438707  1.4241565   1.49536555  0.66904188  0.11344868\n",
      "  0.38536916  0.22891408  0.51751361 -0.53307882]\n"
     ]
    }
   ],
   "source": [
    "mean_real = np.mean(dct2_dataset, axis=0)\n",
    "std_real = np.std(dct2_dataset, axis=0) + 1e-8  # Avoid division by zero\n",
    "dct2_dataset = (dct2_dataset - mean_real) / std_real\n",
    "\n",
    "print(dct2_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: \n",
      "[3. 4. 2. 4. 4. 1. 2. 2. 2. 4. 3. 2. 4. 1. 3. 1.]\n",
      "\n",
      "y_normalized: \n",
      "[0.75 1.   0.5  1.   1.   0.25 0.5  0.5  0.5  1.   0.75 0.5  1.   0.25\n",
      " 0.75 0.25]\n"
     ]
    }
   ],
   "source": [
    "# normalize target data (integers 0-q to [0, 1])\n",
    "y = dataset.astype(np.float32)\n",
    "# y[:, :r] = 0\n",
    "y_normalized = y / (q - 1)  # Scale to [0, 1]\n",
    "print(\"y: \")\n",
    "print(y[0])\n",
    "print(\"\\ny_normalized: \")\n",
    "print(y_normalized[0])\n",
    "\n",
    "labels = y_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shapes: (800, 16) y: (800, 16)\n",
      "Testing data shapes: (200, 16) y: (200, 16)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dct2_dataset, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training data shapes:\", X_train.shape, \"y:\", y_train.shape)\n",
    "print(\"Testing data shapes:\", X_test.shape, \"y:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "output_dim = y_train.shape[1]\n",
    "print(input_dim)\n",
    "print(output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DCT-3 Structure Imposed Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, units, kernel_initializer='he_normal', bias_initializer='zeros', use_bias=True, **kwargs):\n",
    "    super(FirstLayer, self).__init__(**kwargs)\n",
    "    self.units = units # features/neurons\n",
    "    self.kernel_initializer = kernel_initializer\n",
    "    self.bias_initializer = bias_initializer\n",
    "    self.use_bias = use_bias\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    # input_shape --> (batch_size, input_dim)\n",
    "    n = self.units\n",
    "    n1 = n // 2\n",
    "    \n",
    "    self.d_1 = self.add_weight(name =\"kernel_d1\",\n",
    "                               shape=(n1,),\n",
    "                               initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                              #  regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                               trainable=True)\n",
    "    self.d_2 = self.add_weight(name =\"kernel_d2\",\n",
    "                               shape=(n1,),\n",
    "                               initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                              #  regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                               trainable=True)\n",
    "    if self.use_bias:\n",
    "      self.bias = self.add_weight(name =\"bias\",\n",
    "                                  shape=(self.units,),\n",
    "                                  initializer=tf.keras.initializers.get(self.bias_initializer),\n",
    "                                  # regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                                  trainable=True)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # print(\"Enters first layer\")\n",
    "    P_n = tf.concat([inputs[:, ::2], inputs[:, 1::2]], axis=1) # permutation [x0, x2, x4, ..., xn-2, x1, x3, x5, ..., xn-1]\n",
    "    out1 = P_n[:, :int(P_n.shape[1]/2)] # even indices [x0 x2 x4 ...]\n",
    "    out2 = P_n[:, int(P_n.shape[1]/2):] # odd indices [x1 x3 x5 ...]\n",
    "    \n",
    "    # expected output for rearrange out2 after bidiagonal matrix ---> out2_n = [sqrt(2).x1, x1+x3, x3+x5+ ..., xn-3+xn-1]   \n",
    "    \n",
    "    out3 = tf.add(\n",
    "      tf.multiply(out2, self.d_1), # diagonal\n",
    "      tf.multiply(tf.concat([tf.zeros_like(out2[:, :1]), out2[:, :-1]], axis=1), self.d_2) # super diagonal\n",
    "    )\n",
    "    \n",
    "    out = tf.concat([out1, out3], axis=1) # [x0 x2 x4 ... sqrt(2).x1, x1+x3, x3+x5+ ..., xn-3+xn-1]\n",
    "\n",
    "    if self.use_bias:\n",
    "      out += self.bias\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, kernel_initializer='he_normal', bias_initializer='zeros', use_bias=True, **kwargs):\n",
    "        super(SecondLayer, self).__init__(**kwargs)\n",
    "        self.units = units  # Number of neurons/features\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n = self.units\n",
    "        n1 = n // 2\n",
    "        num_blocks = n1 // 2  # Number of 2×2 blocks needed for c1 and c2 (for 16 we need 4 blocks)\n",
    "\n",
    "        self.d_1 = self.add_weight(name=\"kernel_d1\",\n",
    "                                   shape=(n1 - 2,),\n",
    "                                   initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                #    regularizer=tf.keras.regularizers.l2(1e-5),\n",
    "                                   trainable=True)\n",
    "        self.d_2 = self.add_weight(name=\"kernel_d2\",\n",
    "                                   shape=(n1 - 2,),\n",
    "                                   initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                #    regularizer=tf.keras.regularizers.l2(1e-5),\n",
    "                                   trainable=True)\n",
    "        self.w = self.add_weight(name=\"kernel_w\",\n",
    "                                 shape=(n1 - 2,),\n",
    "                                 initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                #  regularizer=tf.keras.regularizers.l2(1e-5),\n",
    "                                 trainable=True)\n",
    "    \n",
    "        self.C_1 = self.add_weight(name=\"kernel_C_1\",\n",
    "                                   shape=(num_blocks, 2, 2),\n",
    "                                   initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                #    regularizer=tf.keras.regularizers.l2(1e-5),\n",
    "                                   trainable=True)\n",
    "        self.C_2 = self.add_weight(name=\"kernel_C_2\",\n",
    "                                   shape=(num_blocks, 2, 2),\n",
    "                                   initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                #    regularizer=tf.keras.regularizers.l2(1e-5),\n",
    "                                   trainable=True)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name=\"bias\",\n",
    "                                        shape=(self.units,),\n",
    "                                        initializer=tf.keras.initializers.get(self.bias_initializer),\n",
    "                                        # regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                                        trainable=True)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # print(\"Enters second layer\")\n",
    "        \n",
    "        # def build_final_matrix(C, n1):\n",
    "        #     \"\"\"Constructs an n1 x n1 block-diagonal matrix from 2x2 blocks in C.\"\"\"\n",
    "        #     final_C = tf.eye(n1)  # initialize as an identity matrix\n",
    "        #     num_blocks = n1 // 2\n",
    "\n",
    "        #     for i in range(num_blocks):\n",
    "        #         final_C = tf.tensor_scatter_nd_update(\n",
    "        #             final_C,\n",
    "        #             indices=[[2*i, 2*i], [2*i, 2*i+1], [2*i+1, 2*i], [2*i+1, 2*i+1]],\n",
    "        #             updates=tf.reshape(C[i], (-1,))\n",
    "        #         )\n",
    "\n",
    "        #     return final_C\n",
    "\n",
    "        def recursiveDCTIII(inputVector, d_1, d_2, w, C, level):\n",
    "            n = inputVector.shape[1]\n",
    "\n",
    "            if n == 2:\n",
    "                out = tf.matmul(inputVector, C[level])  # select correct 2×2 matrix\n",
    "                return out\n",
    "            else:\n",
    "                P_n = tf.concat([inputVector[:, ::2], inputVector[:, 1::2]], axis=1)  # Permutation\n",
    "                out1 = P_n[:, :n // 2]\n",
    "                out2 = P_n[:, n // 2:]\n",
    "\n",
    "                # Apply diagonal and superdiagonal scaling\n",
    "                d1 = tf.multiply(out2, tf.reshape(d_1[:(n // 2)], (1, -1)))\n",
    "                d2 = tf.concat([tf.zeros_like(out2[:, :1]), tf.multiply(out2[:, :-1], tf.reshape(d_2[:(n // 2) - 1], (1, -1)))], axis=1)\n",
    "                out2_bn = tf.add(d1, d2)\n",
    "\n",
    "                # Recursively apply DCT-III\n",
    "                out1_n = recursiveDCTIII(out1, d_1[n // 2:], d_2[n // 2:], w[n // 2:], C, level + 1)\n",
    "                out2_n = recursiveDCTIII(out2_bn, d_1[n // 2:], d_2[n // 2:], w[n // 2:], C, level + 1)\n",
    "\n",
    "                out2_wn = tf.multiply(out2_n, tf.reshape(w[:n // 2], (1, -1)))\n",
    "\n",
    "                out = (1 / tf.sqrt(tf.constant(2.0, dtype=tf.float32))) * tf.concat(\n",
    "                    [(out1_n + out2_wn), tf.reverse((out1_n - out2_wn), axis=[1])], axis=1)\n",
    "\n",
    "                return out\n",
    "\n",
    "        input1 = inputs[:, :self.units // 2]\n",
    "        input2 = inputs[:, self.units // 2:]\n",
    "\n",
    "        z1 = recursiveDCTIII(input1, self.d_1, self.d_2, self.w, self.C_1, level=0)\n",
    "        z2 = recursiveDCTIII(input2, self.d_1, self.d_2, self.w, self.C_2, level=0)\n",
    "        \n",
    "        # final_C1 = build_final_matrix(self.C_1, self.units // 2)\n",
    "        # final_C2 = build_final_matrix(self.C_2, self.units // 2)\n",
    "\n",
    "        # z1 = tf.matmul(z1, final_C1)\n",
    "        # z2 = tf.matmul(z2, final_C2)\n",
    "\n",
    "        out = tf.concat([z1, z2], axis=1)\n",
    "\n",
    "        if self.use_bias:\n",
    "            out += self.bias\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThirdLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, kernel_initializer='he_normal', bias_initializer='zeros', use_bias=True, **kwargs):\n",
    "        super(ThirdLayer, self).__init__(**kwargs)\n",
    "        self.units = units # features/neurons\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n = self.units\n",
    "        n1 = n // 2\n",
    "        \n",
    "        self.w = self.add_weight(name =\"kernel_w\",\n",
    "                                      shape=(n1,),\n",
    "                                      initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                    #   regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                                      trainable=True)\n",
    "        # self.h = self.add_weight(name =\"kernel_h\",\n",
    "        #                               shape = (2,2),\n",
    "        #                               initializer='glorot_normal',\n",
    "        #                               trainable=True)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name =\"bias\",\n",
    "                                       shape=(self.units,),\n",
    "                                       initializer=tf.keras.initializers.get(self.bias_initializer),\n",
    "                                    #    regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                                       trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # print(\"Enters third layer\")\n",
    "        \n",
    "        out1 = inputs[:, :int(inputs.shape[1] / 2)]\n",
    "        out2 = inputs[:, int(inputs.shape[1] / 2):]\n",
    "\n",
    "        # out1 - stays as is\n",
    "        out3 = tf.multiply(out2, self.w)\n",
    "        \n",
    "        # TODO: make H trainable instead of hardcoded\n",
    "        \n",
    "        out = (1 / tf.sqrt(tf.constant(2.0, dtype=tf.float32))) * tf.concat([(out1 + out3), tf.reverse((out1 - out3), axis=[1])], axis=1)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            out += self.bias\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, kernel_initializer='he_normal', bias_initializer='zeros', use_bias=True, **kwargs):\n",
    "        super(CustomLayer, self).__init__(**kwargs)\n",
    "        self.units = units # features/neurons\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n = self.units\n",
    "        \n",
    "        self.m = self.add_weight(name =\"kernel_m\",\n",
    "                                      shape=(n,),\n",
    "                                      initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                    #   regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                                      trainable=True)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name =\"bias\",\n",
    "                                       shape=(self.units,),\n",
    "                                       initializer=tf.keras.initializers.get(self.bias_initializer),\n",
    "                                    #    regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                                       trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # print(\"Enters custom layer\")\n",
    "\n",
    "        out = tf.multiply(inputs, self.m)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            out += self.bias\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLayer2(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, kernel_initializer='he_normal', bias_initializer='zeros', use_bias=True, **kwargs):\n",
    "        super(CustomLayer2, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n = self.units\n",
    "\n",
    "        self.m = self.add_weight(name=\"kernel_m\",\n",
    "                                 shape=(n,),\n",
    "                                 initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                 trainable=True)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name=\"bias\",\n",
    "                                        shape=(self.units,),\n",
    "                                        initializer=tf.keras.initializers.get(self.bias_initializer),\n",
    "                                        trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_dim = tf.shape(inputs)[1]\n",
    "        \n",
    "        perm = tf.concat([inputs[:, ::2], inputs[:, 1::2][:, ::-1]], axis=1) # even indexes + odd indexes - reversed order\n",
    "        \n",
    "        D_hat_n = tf.pow(tf.constant(z0, dtype=tf.float32) / tf.constant(w0, dtype=tf.float32),\n",
    "                         tf.cast(tf.range(input_dim), dtype=tf.float32))\n",
    "        \n",
    "        decoded = D_hat_n * perm\n",
    "\n",
    "        out = tf.multiply(decoded, self.m)\n",
    "\n",
    "        if self.use_bias:\n",
    "            out += self.bias\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, kernel_initializer='ones', bias_initializer='zeros', use_bias=True, **kwargs):\n",
    "        super(LinearLayer, self).__init__(**kwargs)\n",
    "        self.units = units  # features/neurons\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n = self.units\n",
    "\n",
    "        self.m = self.add_weight(name=\"kernel_m\",\n",
    "                                 shape=(n,),\n",
    "                                 initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                 trainable=True)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name=\"bias\",\n",
    "                                        shape=(self.units,),\n",
    "                                        initializer=tf.keras.initializers.get(self.bias_initializer),\n",
    "                                        trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        input_dim = tf.shape(inputs)[1]\n",
    "        \n",
    "        perm = tf.concat([inputs[:, ::2], inputs[:, 1::2][:, ::-1]], axis=1) # even indexes + odd indexes - reversed order\n",
    "        \n",
    "        D_hat_n = tf.pow(tf.constant(z0, dtype=tf.float32) / tf.constant(w0, dtype=tf.float32),\n",
    "                         tf.cast(tf.range(input_dim), dtype=tf.float32))\n",
    "        \n",
    "        decoded = D_hat_n * perm\n",
    "\n",
    "        out = tf.multiply(decoded, self.m)\n",
    "        \n",
    "        # out = tf.multiply(inputs, self.m)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            out += self.bias\n",
    "            \n",
    "        # J = [0_r | I_(n-r)]\n",
    "        # batch_size = tf.shape(out)[0]  # get dynamic batch size\n",
    "        # indices = tf.stack(tf.meshgrid(tf.range(batch_size), tf.range(self.r), indexing='ij'), axis=-1)  # Shape (batch_size, r, 2)\n",
    "\n",
    "        # updates = tf.zeros((batch_size, self.r), dtype=out.dtype)\n",
    "        \n",
    "        # out = tf.tensor_scatter_nd_update(out, indices, updates) # apply the updates to zero out first `r` elements in each sample\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_initializer(shape, dtype=None):\n",
    "    n = shape[0]\n",
    "    values = np.array([(z0 / w0) ** k for k in range(n)], dtype=np.float32)\n",
    "    return tf.convert_to_tensor(values, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 16)]              0         \n",
      "                                                                 \n",
      " real_layer_1 (FirstLayer)   (None, 16)                32        \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 16)                0         \n",
      "                                                                 \n",
      " real_support_layer_1 (Cust  (None, 16)                32        \n",
      " omLayer)                                                        \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 16)                0         \n",
      "                                                                 \n",
      " real_layer_2 (SecondLayer)  (None, 16)                66        \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 16)                0         \n",
      "                                                                 \n",
      " real_support_layer_2 (Cust  (None, 16)                32        \n",
      " omLayer)                                                        \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 16)                0         \n",
      "                                                                 \n",
      " real_layer_3 (ThirdLayer)   (None, 16)                24        \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 16)                0         \n",
      "                                                                 \n",
      " output_layer (LinearLayer)  (None, 16)                32        \n",
      "                                                                 \n",
      " activation (Activation)     (None, 16)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 218 (872.00 Byte)\n",
      "Trainable params: 218 (872.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Concatenate, LeakyReLU, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def cosine_loss(y_true, y_pred):\n",
    "    y_true_norm = tf.nn.l2_normalize(y_true, axis=-1)\n",
    "    y_pred_norm = tf.nn.l2_normalize(y_pred, axis=-1)\n",
    "    return 1 - tf.reduce_mean(tf.reduce_sum(y_true_norm * y_pred_norm, axis=-1))\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    return mse\n",
    "\n",
    "def hybrid_loss(y_true, y_pred):\n",
    "    mse_part = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    cos_part = cosine_loss(y_true, y_pred)\n",
    "    return 0.5 * mse_part + 0.5 * cos_part\n",
    "\n",
    "\n",
    "def structured_NN(input_dim, output_dim):\n",
    "    input_ = Input(shape=(input_dim,), name=\"input\")\n",
    "    \n",
    "    # he_normal\n",
    "    # glorot_normal\n",
    "    # glorot_uniform\n",
    "    \n",
    "    x = FirstLayer(units=input_dim, kernel_initializer='glorot_normal', bias_initializer='glorot_uniform', name=\"real_layer_1\")(input_)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = CustomLayer(units=input_dim, kernel_initializer='glorot_normal', bias_initializer='glorot_uniform', name=\"real_support_layer_1\")(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = SecondLayer(units=input_dim, kernel_initializer='glorot_normal', bias_initializer='glorot_uniform', name=\"real_layer_2\")(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = CustomLayer(units=input_dim, kernel_initializer='glorot_normal', bias_initializer='glorot_uniform', name=\"real_support_layer_2\")(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = ThirdLayer(units=input_dim, kernel_initializer='glorot_normal', bias_initializer='glorot_uniform', name=\"real_layer_3\")(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    \n",
    "    # random_initializer = tf.keras.initializers.RandomUniform(minval=0.5, maxval=1.5) # Avoid very small values\n",
    "    # x = CustomLayer2(units=input_dim, kernel_initializer='glorot_normal', bias_initializer='zeros', name=\"diagonal_scaling_layer\")(x)\n",
    "    # x = LeakyReLU(alpha=0.1)(x)\n",
    "    \n",
    "    output = LinearLayer(units=output_dim, kernel_initializer='ones', bias_initializer='zeros', name=\"output_layer\")(x)\n",
    "    output = Activation('linear')(output) #sigmoid #tanh #linear\n",
    "\n",
    "    model = Model(inputs=input_, outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss=hybrid_loss, metrics=['mse', 'mae']) #hybrid_loss \n",
    "\n",
    "    return model\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = y_train.shape[1]\n",
    "model = structured_NN(input_dim, output_dim)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input [(None, 16)]\n",
      "real_layer_1 (None, 16)\n",
      "leaky_re_lu (None, 16)\n",
      "real_support_layer_1 (None, 16)\n",
      "leaky_re_lu_1 (None, 16)\n",
      "real_layer_2 (None, 16)\n",
      "leaky_re_lu_2 (None, 16)\n",
      "real_support_layer_2 (None, 16)\n",
      "leaky_re_lu_3 (None, 16)\n",
      "real_layer_3 (None, 16)\n",
      "leaky_re_lu_4 (None, 16)\n",
      "output_layer (None, 16)\n",
      "activation (None, 16)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name, layer.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "adjust_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "50/50 [==============================] - 6s 25ms/step - loss: 0.3621 - mse: 0.3310 - mae: 0.4715 - val_loss: 0.2680 - val_mse: 0.3089 - val_mae: 0.4557 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2530 - mse: 0.3123 - mae: 0.4594 - val_loss: 0.2439 - val_mse: 0.3006 - val_mae: 0.4504 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2438 - mse: 0.3047 - mae: 0.4541 - val_loss: 0.2395 - val_mse: 0.2921 - val_mae: 0.4445 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2390 - mse: 0.2951 - mae: 0.4472 - val_loss: 0.2345 - val_mse: 0.2823 - val_mae: 0.4374 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2338 - mse: 0.2849 - mae: 0.4396 - val_loss: 0.2295 - val_mse: 0.2716 - val_mae: 0.4294 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2283 - mse: 0.2736 - mae: 0.4308 - val_loss: 0.2238 - val_mse: 0.2602 - val_mae: 0.4205 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2224 - mse: 0.2615 - mae: 0.4212 - val_loss: 0.2181 - val_mse: 0.2485 - val_mae: 0.4110 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2164 - mse: 0.2499 - mae: 0.4114 - val_loss: 0.2124 - val_mse: 0.2369 - val_mae: 0.4010 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2105 - mse: 0.2375 - mae: 0.4004 - val_loss: 0.2066 - val_mse: 0.2252 - val_mae: 0.3905 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2046 - mse: 0.2258 - mae: 0.3895 - val_loss: 0.2008 - val_mse: 0.2134 - val_mae: 0.3792 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1987 - mse: 0.2133 - mae: 0.3770 - val_loss: 0.1951 - val_mse: 0.2017 - val_mae: 0.3673 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1930 - mse: 0.2021 - mae: 0.3655 - val_loss: 0.1897 - val_mse: 0.1906 - val_mae: 0.3567 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1874 - mse: 0.1906 - mae: 0.3557 - val_loss: 0.1845 - val_mse: 0.1800 - val_mae: 0.3488 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1824 - mse: 0.1799 - mae: 0.3483 - val_loss: 0.1799 - val_mse: 0.1703 - val_mae: 0.3428 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1778 - mse: 0.1712 - mae: 0.3438 - val_loss: 0.1756 - val_mse: 0.1624 - val_mae: 0.3389 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1738 - mse: 0.1629 - mae: 0.3396 - val_loss: 0.1721 - val_mse: 0.1553 - val_mae: 0.3349 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1703 - mse: 0.1562 - mae: 0.3360 - val_loss: 0.1689 - val_mse: 0.1494 - val_mae: 0.3312 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1673 - mse: 0.1508 - mae: 0.3327 - val_loss: 0.1663 - val_mse: 0.1446 - val_mae: 0.3279 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1648 - mse: 0.1460 - mae: 0.3294 - val_loss: 0.1643 - val_mse: 0.1407 - val_mae: 0.3248 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1627 - mse: 0.1419 - mae: 0.3261 - val_loss: 0.1624 - val_mse: 0.1375 - val_mae: 0.3220 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1610 - mse: 0.1391 - mae: 0.3239 - val_loss: 0.1609 - val_mse: 0.1350 - val_mae: 0.3196 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1595 - mse: 0.1366 - mae: 0.3216 - val_loss: 0.1597 - val_mse: 0.1328 - val_mae: 0.3172 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1584 - mse: 0.1344 - mae: 0.3192 - val_loss: 0.1589 - val_mse: 0.1313 - val_mae: 0.3154 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1574 - mse: 0.1328 - mae: 0.3174 - val_loss: 0.1580 - val_mse: 0.1301 - val_mae: 0.3138 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1567 - mse: 0.1316 - mae: 0.3159 - val_loss: 0.1575 - val_mse: 0.1290 - val_mae: 0.3122 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1561 - mse: 0.1305 - mae: 0.3142 - val_loss: 0.1569 - val_mse: 0.1283 - val_mae: 0.3109 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1556 - mse: 0.1297 - mae: 0.3130 - val_loss: 0.1565 - val_mse: 0.1277 - val_mae: 0.3098 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1552 - mse: 0.1291 - mae: 0.3118 - val_loss: 0.1563 - val_mse: 0.1272 - val_mae: 0.3088 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1548 - mse: 0.1286 - mae: 0.3109 - val_loss: 0.1561 - val_mse: 0.1269 - val_mae: 0.3080 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1546 - mse: 0.1281 - mae: 0.3099 - val_loss: 0.1558 - val_mse: 0.1266 - val_mae: 0.3071 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1544 - mse: 0.1279 - mae: 0.3093 - val_loss: 0.1557 - val_mse: 0.1264 - val_mae: 0.3065 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1542 - mse: 0.1276 - mae: 0.3085 - val_loss: 0.1556 - val_mse: 0.1262 - val_mae: 0.3059 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1541 - mse: 0.1274 - mae: 0.3081 - val_loss: 0.1555 - val_mse: 0.1261 - val_mae: 0.3053 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1540 - mse: 0.1272 - mae: 0.3075 - val_loss: 0.1555 - val_mse: 0.1261 - val_mae: 0.3050 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1539 - mse: 0.1271 - mae: 0.3070 - val_loss: 0.1554 - val_mse: 0.1260 - val_mae: 0.3046 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1538 - mse: 0.1270 - mae: 0.3066 - val_loss: 0.1552 - val_mse: 0.1259 - val_mae: 0.3044 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1537 - mse: 0.1269 - mae: 0.3064 - val_loss: 0.1552 - val_mse: 0.1259 - val_mae: 0.3042 - lr: 0.0010\n",
      "Epoch 38/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1536 - mse: 0.1268 - mae: 0.3063 - val_loss: 0.1552 - val_mse: 0.1259 - val_mae: 0.3041 - lr: 0.0010\n",
      "Epoch 39/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1536 - mse: 0.1268 - mae: 0.3061 - val_loss: 0.1551 - val_mse: 0.1258 - val_mae: 0.3040 - lr: 0.0010\n",
      "Epoch 40/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1535 - mse: 0.1267 - mae: 0.3061 - val_loss: 0.1551 - val_mse: 0.1258 - val_mae: 0.3040 - lr: 0.0010\n",
      "Epoch 41/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1535 - mse: 0.1267 - mae: 0.3060 - val_loss: 0.1551 - val_mse: 0.1258 - val_mae: 0.3040 - lr: 0.0010\n",
      "Epoch 42/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1534 - mse: 0.1266 - mae: 0.3059 - val_loss: 0.1550 - val_mse: 0.1258 - val_mae: 0.3039 - lr: 0.0010\n",
      "Epoch 43/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1533 - mse: 0.1265 - mae: 0.3059 - val_loss: 0.1550 - val_mse: 0.1257 - val_mae: 0.3039 - lr: 0.0010\n",
      "Epoch 44/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1533 - mse: 0.1265 - mae: 0.3059 - val_loss: 0.1549 - val_mse: 0.1256 - val_mae: 0.3039 - lr: 0.0010\n",
      "Epoch 45/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1532 - mse: 0.1265 - mae: 0.3059 - val_loss: 0.1548 - val_mse: 0.1256 - val_mae: 0.3039 - lr: 0.0010\n",
      "Epoch 46/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1531 - mse: 0.1264 - mae: 0.3059 - val_loss: 0.1548 - val_mse: 0.1257 - val_mae: 0.3040 - lr: 0.0010\n",
      "Epoch 47/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1530 - mse: 0.1263 - mae: 0.3059 - val_loss: 0.1547 - val_mse: 0.1256 - val_mae: 0.3040 - lr: 0.0010\n",
      "Epoch 48/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1531 - mse: 0.1263 - mae: 0.3060 - val_loss: 0.1548 - val_mse: 0.1257 - val_mae: 0.3041 - lr: 0.0010\n",
      "Epoch 49/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1530 - mse: 0.1263 - mae: 0.3059 - val_loss: 0.1547 - val_mse: 0.1256 - val_mae: 0.3039 - lr: 0.0010\n",
      "Epoch 50/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1529 - mse: 0.1262 - mae: 0.3059 - val_loss: 0.1546 - val_mse: 0.1255 - val_mae: 0.3041 - lr: 0.0010\n",
      "Epoch 51/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1529 - mse: 0.1262 - mae: 0.3059 - val_loss: 0.1546 - val_mse: 0.1256 - val_mae: 0.3041 - lr: 0.0010\n",
      "Epoch 52/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1528 - mse: 0.1262 - mae: 0.3059 - val_loss: 0.1546 - val_mse: 0.1255 - val_mae: 0.3041 - lr: 0.0010\n",
      "Epoch 53/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1528 - mse: 0.1261 - mae: 0.3059 - val_loss: 0.1546 - val_mse: 0.1255 - val_mae: 0.3041 - lr: 0.0010\n",
      "Epoch 54/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1527 - mse: 0.1261 - mae: 0.3059 - val_loss: 0.1545 - val_mse: 0.1255 - val_mae: 0.3040 - lr: 0.0010\n",
      "Epoch 55/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1527 - mse: 0.1261 - mae: 0.3059 - val_loss: 0.1545 - val_mse: 0.1255 - val_mae: 0.3040 - lr: 0.0010\n",
      "Epoch 56/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1526 - mse: 0.1260 - mae: 0.3059 - val_loss: 0.1544 - val_mse: 0.1254 - val_mae: 0.3040 - lr: 0.0010\n",
      "Epoch 57/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1526 - mse: 0.1260 - mae: 0.3057 - val_loss: 0.1542 - val_mse: 0.1252 - val_mae: 0.3040 - lr: 0.0010\n",
      "Epoch 58/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1525 - mse: 0.1259 - mae: 0.3059 - val_loss: 0.1544 - val_mse: 0.1254 - val_mae: 0.3041 - lr: 0.0010\n",
      "Epoch 59/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1525 - mse: 0.1259 - mae: 0.3058 - val_loss: 0.1543 - val_mse: 0.1253 - val_mae: 0.3041 - lr: 0.0010\n",
      "Epoch 60/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1524 - mse: 0.1258 - mae: 0.3058 - val_loss: 0.1543 - val_mse: 0.1253 - val_mae: 0.3040 - lr: 0.0010\n",
      "Epoch 61/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1523 - mse: 0.1258 - mae: 0.3058 - val_loss: 0.1542 - val_mse: 0.1252 - val_mae: 0.3040 - lr: 0.0010\n",
      "Epoch 62/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1522 - mse: 0.1257 - mae: 0.3057 - val_loss: 0.1541 - val_mse: 0.1252 - val_mae: 0.3039 - lr: 0.0010\n",
      "Epoch 63/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1522 - mse: 0.1257 - mae: 0.3056 - val_loss: 0.1539 - val_mse: 0.1251 - val_mae: 0.3039 - lr: 0.0010\n",
      "Epoch 64/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1521 - mse: 0.1256 - mae: 0.3057 - val_loss: 0.1539 - val_mse: 0.1250 - val_mae: 0.3039 - lr: 0.0010\n",
      "Epoch 65/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1521 - mse: 0.1256 - mae: 0.3056 - val_loss: 0.1538 - val_mse: 0.1249 - val_mae: 0.3039 - lr: 0.0010\n",
      "Epoch 66/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1521 - mse: 0.1256 - mae: 0.3056 - val_loss: 0.1538 - val_mse: 0.1250 - val_mae: 0.3038 - lr: 0.0010\n",
      "Epoch 67/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1519 - mse: 0.1255 - mae: 0.3054 - val_loss: 0.1536 - val_mse: 0.1249 - val_mae: 0.3036 - lr: 0.0010\n",
      "Epoch 68/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1519 - mse: 0.1255 - mae: 0.3055 - val_loss: 0.1537 - val_mse: 0.1249 - val_mae: 0.3037 - lr: 0.0010\n",
      "Epoch 69/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1518 - mse: 0.1254 - mae: 0.3054 - val_loss: 0.1536 - val_mse: 0.1248 - val_mae: 0.3036 - lr: 0.0010\n",
      "Epoch 70/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1517 - mse: 0.1253 - mae: 0.3052 - val_loss: 0.1535 - val_mse: 0.1247 - val_mae: 0.3036 - lr: 0.0010\n",
      "Epoch 71/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1516 - mse: 0.1252 - mae: 0.3052 - val_loss: 0.1534 - val_mse: 0.1247 - val_mae: 0.3035 - lr: 0.0010\n",
      "Epoch 72/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1515 - mse: 0.1251 - mae: 0.3051 - val_loss: 0.1533 - val_mse: 0.1246 - val_mae: 0.3035 - lr: 0.0010\n",
      "Epoch 73/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1514 - mse: 0.1251 - mae: 0.3050 - val_loss: 0.1532 - val_mse: 0.1245 - val_mae: 0.3034 - lr: 0.0010\n",
      "Epoch 74/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1513 - mse: 0.1250 - mae: 0.3050 - val_loss: 0.1532 - val_mse: 0.1246 - val_mae: 0.3034 - lr: 0.0010\n",
      "Epoch 75/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1512 - mse: 0.1249 - mae: 0.3049 - val_loss: 0.1530 - val_mse: 0.1244 - val_mae: 0.3034 - lr: 0.0010\n",
      "Epoch 76/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1511 - mse: 0.1248 - mae: 0.3047 - val_loss: 0.1529 - val_mse: 0.1243 - val_mae: 0.3033 - lr: 0.0010\n",
      "Epoch 77/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1510 - mse: 0.1248 - mae: 0.3047 - val_loss: 0.1528 - val_mse: 0.1243 - val_mae: 0.3032 - lr: 0.0010\n",
      "Epoch 78/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1508 - mse: 0.1246 - mae: 0.3045 - val_loss: 0.1527 - val_mse: 0.1242 - val_mae: 0.3032 - lr: 0.0010\n",
      "Epoch 79/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1507 - mse: 0.1245 - mae: 0.3044 - val_loss: 0.1527 - val_mse: 0.1242 - val_mae: 0.3033 - lr: 0.0010\n",
      "Epoch 80/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1505 - mse: 0.1244 - mae: 0.3043 - val_loss: 0.1525 - val_mse: 0.1241 - val_mae: 0.3031 - lr: 0.0010\n",
      "Epoch 81/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1504 - mse: 0.1243 - mae: 0.3042 - val_loss: 0.1523 - val_mse: 0.1239 - val_mae: 0.3029 - lr: 0.0010\n",
      "Epoch 82/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1503 - mse: 0.1242 - mae: 0.3040 - val_loss: 0.1523 - val_mse: 0.1239 - val_mae: 0.3029 - lr: 0.0010\n",
      "Epoch 83/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1501 - mse: 0.1241 - mae: 0.3038 - val_loss: 0.1520 - val_mse: 0.1237 - val_mae: 0.3028 - lr: 0.0010\n",
      "Epoch 84/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1499 - mse: 0.1240 - mae: 0.3037 - val_loss: 0.1518 - val_mse: 0.1235 - val_mae: 0.3026 - lr: 0.0010\n",
      "Epoch 85/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1496 - mse: 0.1238 - mae: 0.3034 - val_loss: 0.1516 - val_mse: 0.1234 - val_mae: 0.3024 - lr: 0.0010\n",
      "Epoch 86/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1494 - mse: 0.1236 - mae: 0.3032 - val_loss: 0.1516 - val_mse: 0.1234 - val_mae: 0.3024 - lr: 0.0010\n",
      "Epoch 87/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1492 - mse: 0.1234 - mae: 0.3029 - val_loss: 0.1514 - val_mse: 0.1233 - val_mae: 0.3022 - lr: 0.0010\n",
      "Epoch 88/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1489 - mse: 0.1233 - mae: 0.3027 - val_loss: 0.1512 - val_mse: 0.1232 - val_mae: 0.3020 - lr: 0.0010\n",
      "Epoch 89/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1486 - mse: 0.1230 - mae: 0.3024 - val_loss: 0.1509 - val_mse: 0.1230 - val_mae: 0.3018 - lr: 0.0010\n",
      "Epoch 90/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1482 - mse: 0.1228 - mae: 0.3021 - val_loss: 0.1505 - val_mse: 0.1228 - val_mae: 0.3014 - lr: 0.0010\n",
      "Epoch 91/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1478 - mse: 0.1225 - mae: 0.3018 - val_loss: 0.1501 - val_mse: 0.1225 - val_mae: 0.3010 - lr: 0.0010\n",
      "Epoch 92/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1474 - mse: 0.1223 - mae: 0.3013 - val_loss: 0.1496 - val_mse: 0.1222 - val_mae: 0.3005 - lr: 0.0010\n",
      "Epoch 93/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1470 - mse: 0.1220 - mae: 0.3010 - val_loss: 0.1493 - val_mse: 0.1220 - val_mae: 0.3002 - lr: 0.0010\n",
      "Epoch 94/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1467 - mse: 0.1218 - mae: 0.3006 - val_loss: 0.1490 - val_mse: 0.1219 - val_mae: 0.2998 - lr: 0.0010\n",
      "Epoch 95/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1464 - mse: 0.1216 - mae: 0.3003 - val_loss: 0.1486 - val_mse: 0.1216 - val_mae: 0.2994 - lr: 0.0010\n",
      "Epoch 96/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1461 - mse: 0.1213 - mae: 0.2999 - val_loss: 0.1482 - val_mse: 0.1213 - val_mae: 0.2989 - lr: 0.0010\n",
      "Epoch 97/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1457 - mse: 0.1211 - mae: 0.2995 - val_loss: 0.1478 - val_mse: 0.1210 - val_mae: 0.2985 - lr: 0.0010\n",
      "Epoch 98/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1454 - mse: 0.1208 - mae: 0.2989 - val_loss: 0.1474 - val_mse: 0.1207 - val_mae: 0.2980 - lr: 0.0010\n",
      "Epoch 99/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1449 - mse: 0.1205 - mae: 0.2985 - val_loss: 0.1470 - val_mse: 0.1203 - val_mae: 0.2975 - lr: 0.0010\n",
      "Epoch 100/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1445 - mse: 0.1202 - mae: 0.2980 - val_loss: 0.1467 - val_mse: 0.1201 - val_mae: 0.2972 - lr: 0.0010\n",
      "Epoch 101/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1442 - mse: 0.1199 - mae: 0.2975 - val_loss: 0.1461 - val_mse: 0.1197 - val_mae: 0.2965 - lr: 0.0010\n",
      "Epoch 102/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1437 - mse: 0.1195 - mae: 0.2969 - val_loss: 0.1457 - val_mse: 0.1194 - val_mae: 0.2961 - lr: 0.0010\n",
      "Epoch 103/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1433 - mse: 0.1192 - mae: 0.2964 - val_loss: 0.1452 - val_mse: 0.1190 - val_mae: 0.2954 - lr: 0.0010\n",
      "Epoch 104/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1429 - mse: 0.1188 - mae: 0.2958 - val_loss: 0.1449 - val_mse: 0.1187 - val_mae: 0.2949 - lr: 0.0010\n",
      "Epoch 105/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1426 - mse: 0.1185 - mae: 0.2954 - val_loss: 0.1443 - val_mse: 0.1182 - val_mae: 0.2942 - lr: 0.0010\n",
      "Epoch 106/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1421 - mse: 0.1181 - mae: 0.2947 - val_loss: 0.1438 - val_mse: 0.1178 - val_mae: 0.2935 - lr: 0.0010\n",
      "Epoch 107/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1416 - mse: 0.1177 - mae: 0.2941 - val_loss: 0.1432 - val_mse: 0.1174 - val_mae: 0.2927 - lr: 0.0010\n",
      "Epoch 108/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1411 - mse: 0.1173 - mae: 0.2933 - val_loss: 0.1426 - val_mse: 0.1169 - val_mae: 0.2920 - lr: 0.0010\n",
      "Epoch 109/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1406 - mse: 0.1169 - mae: 0.2925 - val_loss: 0.1422 - val_mse: 0.1165 - val_mae: 0.2913 - lr: 0.0010\n",
      "Epoch 110/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1402 - mse: 0.1165 - mae: 0.2919 - val_loss: 0.1416 - val_mse: 0.1161 - val_mae: 0.2905 - lr: 0.0010\n",
      "Epoch 111/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1396 - mse: 0.1161 - mae: 0.2911 - val_loss: 0.1410 - val_mse: 0.1155 - val_mae: 0.2898 - lr: 0.0010\n",
      "Epoch 112/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1391 - mse: 0.1156 - mae: 0.2903 - val_loss: 0.1404 - val_mse: 0.1150 - val_mae: 0.2887 - lr: 0.0010\n",
      "Epoch 113/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1385 - mse: 0.1151 - mae: 0.2894 - val_loss: 0.1398 - val_mse: 0.1146 - val_mae: 0.2881 - lr: 0.0010\n",
      "Epoch 114/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1381 - mse: 0.1147 - mae: 0.2886 - val_loss: 0.1392 - val_mse: 0.1140 - val_mae: 0.2871 - lr: 0.0010\n",
      "Epoch 115/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1375 - mse: 0.1142 - mae: 0.2878 - val_loss: 0.1387 - val_mse: 0.1136 - val_mae: 0.2863 - lr: 0.0010\n",
      "Epoch 116/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1370 - mse: 0.1137 - mae: 0.2868 - val_loss: 0.1381 - val_mse: 0.1131 - val_mae: 0.2854 - lr: 0.0010\n",
      "Epoch 117/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1365 - mse: 0.1133 - mae: 0.2860 - val_loss: 0.1376 - val_mse: 0.1128 - val_mae: 0.2846 - lr: 0.0010\n",
      "Epoch 118/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1360 - mse: 0.1129 - mae: 0.2851 - val_loss: 0.1371 - val_mse: 0.1122 - val_mae: 0.2837 - lr: 0.0010\n",
      "Epoch 119/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1355 - mse: 0.1124 - mae: 0.2843 - val_loss: 0.1364 - val_mse: 0.1116 - val_mae: 0.2828 - lr: 0.0010\n",
      "Epoch 120/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1350 - mse: 0.1119 - mae: 0.2834 - val_loss: 0.1359 - val_mse: 0.1113 - val_mae: 0.2820 - lr: 0.0010\n",
      "Epoch 121/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1345 - mse: 0.1116 - mae: 0.2826 - val_loss: 0.1354 - val_mse: 0.1108 - val_mae: 0.2813 - lr: 0.0010\n",
      "Epoch 122/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1341 - mse: 0.1112 - mae: 0.2819 - val_loss: 0.1350 - val_mse: 0.1106 - val_mae: 0.2807 - lr: 0.0010\n",
      "Epoch 123/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1337 - mse: 0.1108 - mae: 0.2812 - val_loss: 0.1343 - val_mse: 0.1099 - val_mae: 0.2797 - lr: 0.0010\n",
      "Epoch 124/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1331 - mse: 0.1103 - mae: 0.2804 - val_loss: 0.1337 - val_mse: 0.1094 - val_mae: 0.2788 - lr: 0.0010\n",
      "Epoch 125/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1327 - mse: 0.1100 - mae: 0.2797 - val_loss: 0.1333 - val_mse: 0.1091 - val_mae: 0.2781 - lr: 0.0010\n",
      "Epoch 126/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1323 - mse: 0.1096 - mae: 0.2791 - val_loss: 0.1328 - val_mse: 0.1087 - val_mae: 0.2776 - lr: 0.0010\n",
      "Epoch 127/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1318 - mse: 0.1092 - mae: 0.2784 - val_loss: 0.1323 - val_mse: 0.1082 - val_mae: 0.2767 - lr: 0.0010\n",
      "Epoch 128/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1313 - mse: 0.1088 - mae: 0.2777 - val_loss: 0.1319 - val_mse: 0.1079 - val_mae: 0.2761 - lr: 0.0010\n",
      "Epoch 129/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1310 - mse: 0.1085 - mae: 0.2772 - val_loss: 0.1314 - val_mse: 0.1075 - val_mae: 0.2753 - lr: 0.0010\n",
      "Epoch 130/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1305 - mse: 0.1081 - mae: 0.2765 - val_loss: 0.1308 - val_mse: 0.1071 - val_mae: 0.2745 - lr: 0.0010\n",
      "Epoch 131/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1302 - mse: 0.1078 - mae: 0.2759 - val_loss: 0.1305 - val_mse: 0.1068 - val_mae: 0.2739 - lr: 0.0010\n",
      "Epoch 132/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1296 - mse: 0.1074 - mae: 0.2751 - val_loss: 0.1298 - val_mse: 0.1063 - val_mae: 0.2731 - lr: 0.0010\n",
      "Epoch 133/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1290 - mse: 0.1069 - mae: 0.2744 - val_loss: 0.1293 - val_mse: 0.1059 - val_mae: 0.2724 - lr: 0.0010\n",
      "Epoch 134/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1286 - mse: 0.1066 - mae: 0.2737 - val_loss: 0.1287 - val_mse: 0.1054 - val_mae: 0.2717 - lr: 0.0010\n",
      "Epoch 135/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1282 - mse: 0.1062 - mae: 0.2731 - val_loss: 0.1282 - val_mse: 0.1050 - val_mae: 0.2708 - lr: 0.0010\n",
      "Epoch 136/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1276 - mse: 0.1058 - mae: 0.2723 - val_loss: 0.1277 - val_mse: 0.1047 - val_mae: 0.2702 - lr: 0.0010\n",
      "Epoch 137/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1272 - mse: 0.1055 - mae: 0.2717 - val_loss: 0.1273 - val_mse: 0.1044 - val_mae: 0.2696 - lr: 0.0010\n",
      "Epoch 138/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1267 - mse: 0.1051 - mae: 0.2711 - val_loss: 0.1268 - val_mse: 0.1040 - val_mae: 0.2687 - lr: 0.0010\n",
      "Epoch 139/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1262 - mse: 0.1047 - mae: 0.2703 - val_loss: 0.1264 - val_mse: 0.1037 - val_mae: 0.2681 - lr: 0.0010\n",
      "Epoch 140/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1258 - mse: 0.1044 - mae: 0.2698 - val_loss: 0.1259 - val_mse: 0.1033 - val_mae: 0.2673 - lr: 0.0010\n",
      "Epoch 141/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1254 - mse: 0.1040 - mae: 0.2691 - val_loss: 0.1256 - val_mse: 0.1031 - val_mae: 0.2667 - lr: 0.0010\n",
      "Epoch 142/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1249 - mse: 0.1037 - mae: 0.2686 - val_loss: 0.1250 - val_mse: 0.1027 - val_mae: 0.2659 - lr: 0.0010\n",
      "Epoch 143/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1245 - mse: 0.1034 - mae: 0.2679 - val_loss: 0.1247 - val_mse: 0.1025 - val_mae: 0.2654 - lr: 0.0010\n",
      "Epoch 144/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1241 - mse: 0.1031 - mae: 0.2674 - val_loss: 0.1244 - val_mse: 0.1023 - val_mae: 0.2651 - lr: 0.0010\n",
      "Epoch 145/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1237 - mse: 0.1028 - mae: 0.2669 - val_loss: 0.1241 - val_mse: 0.1020 - val_mae: 0.2646 - lr: 0.0010\n",
      "Epoch 146/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1234 - mse: 0.1025 - mae: 0.2665 - val_loss: 0.1239 - val_mse: 0.1018 - val_mae: 0.2643 - lr: 0.0010\n",
      "Epoch 147/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1230 - mse: 0.1022 - mae: 0.2658 - val_loss: 0.1234 - val_mse: 0.1014 - val_mae: 0.2637 - lr: 0.0010\n",
      "Epoch 148/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1228 - mse: 0.1020 - mae: 0.2656 - val_loss: 0.1232 - val_mse: 0.1013 - val_mae: 0.2633 - lr: 0.0010\n",
      "Epoch 149/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1225 - mse: 0.1018 - mae: 0.2651 - val_loss: 0.1228 - val_mse: 0.1010 - val_mae: 0.2628 - lr: 0.0010\n",
      "Epoch 150/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1221 - mse: 0.1015 - mae: 0.2646 - val_loss: 0.1226 - val_mse: 0.1008 - val_mae: 0.2624 - lr: 0.0010\n",
      "Epoch 151/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1218 - mse: 0.1012 - mae: 0.2642 - val_loss: 0.1224 - val_mse: 0.1007 - val_mae: 0.2620 - lr: 0.0010\n",
      "Epoch 152/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1214 - mse: 0.1010 - mae: 0.2637 - val_loss: 0.1220 - val_mse: 0.1003 - val_mae: 0.2616 - lr: 0.0010\n",
      "Epoch 153/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1212 - mse: 0.1008 - mae: 0.2633 - val_loss: 0.1217 - val_mse: 0.1001 - val_mae: 0.2611 - lr: 0.0010\n",
      "Epoch 154/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1209 - mse: 0.1005 - mae: 0.2629 - val_loss: 0.1214 - val_mse: 0.0998 - val_mae: 0.2608 - lr: 0.0010\n",
      "Epoch 155/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1206 - mse: 0.1002 - mae: 0.2625 - val_loss: 0.1213 - val_mse: 0.0998 - val_mae: 0.2605 - lr: 0.0010\n",
      "Epoch 156/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1203 - mse: 0.1000 - mae: 0.2621 - val_loss: 0.1209 - val_mse: 0.0994 - val_mae: 0.2600 - lr: 0.0010\n",
      "Epoch 157/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1201 - mse: 0.0998 - mae: 0.2617 - val_loss: 0.1205 - val_mse: 0.0991 - val_mae: 0.2595 - lr: 0.0010\n",
      "Epoch 158/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1197 - mse: 0.0995 - mae: 0.2613 - val_loss: 0.1206 - val_mse: 0.0992 - val_mae: 0.2592 - lr: 0.0010\n",
      "Epoch 159/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1194 - mse: 0.0993 - mae: 0.2608 - val_loss: 0.1200 - val_mse: 0.0987 - val_mae: 0.2586 - lr: 0.0010\n",
      "Epoch 160/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1191 - mse: 0.0990 - mae: 0.2603 - val_loss: 0.1198 - val_mse: 0.0985 - val_mae: 0.2582 - lr: 0.0010\n",
      "Epoch 161/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1188 - mse: 0.0987 - mae: 0.2599 - val_loss: 0.1197 - val_mse: 0.0983 - val_mae: 0.2578 - lr: 0.0010\n",
      "Epoch 162/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1185 - mse: 0.0985 - mae: 0.2595 - val_loss: 0.1195 - val_mse: 0.0982 - val_mae: 0.2577 - lr: 0.0010\n",
      "Epoch 163/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1182 - mse: 0.0982 - mae: 0.2590 - val_loss: 0.1192 - val_mse: 0.0979 - val_mae: 0.2571 - lr: 0.0010\n",
      "Epoch 164/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1178 - mse: 0.0979 - mae: 0.2585 - val_loss: 0.1189 - val_mse: 0.0976 - val_mae: 0.2566 - lr: 0.0010\n",
      "Epoch 165/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1175 - mse: 0.0976 - mae: 0.2579 - val_loss: 0.1186 - val_mse: 0.0974 - val_mae: 0.2561 - lr: 0.0010\n",
      "Epoch 166/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1172 - mse: 0.0974 - mae: 0.2576 - val_loss: 0.1184 - val_mse: 0.0971 - val_mae: 0.2557 - lr: 0.0010\n",
      "Epoch 167/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1169 - mse: 0.0971 - mae: 0.2570 - val_loss: 0.1181 - val_mse: 0.0969 - val_mae: 0.2553 - lr: 0.0010\n",
      "Epoch 168/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1164 - mse: 0.0967 - mae: 0.2564 - val_loss: 0.1178 - val_mse: 0.0966 - val_mae: 0.2547 - lr: 0.0010\n",
      "Epoch 169/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1160 - mse: 0.0964 - mae: 0.2558 - val_loss: 0.1174 - val_mse: 0.0963 - val_mae: 0.2541 - lr: 0.0010\n",
      "Epoch 170/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1157 - mse: 0.0961 - mae: 0.2553 - val_loss: 0.1172 - val_mse: 0.0961 - val_mae: 0.2537 - lr: 0.0010\n",
      "Epoch 171/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1154 - mse: 0.0958 - mae: 0.2548 - val_loss: 0.1170 - val_mse: 0.0959 - val_mae: 0.2532 - lr: 0.0010\n",
      "Epoch 172/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1151 - mse: 0.0955 - mae: 0.2541 - val_loss: 0.1167 - val_mse: 0.0956 - val_mae: 0.2528 - lr: 0.0010\n",
      "Epoch 173/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1148 - mse: 0.0953 - mae: 0.2536 - val_loss: 0.1166 - val_mse: 0.0955 - val_mae: 0.2525 - lr: 0.0010\n",
      "Epoch 174/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1144 - mse: 0.0949 - mae: 0.2530 - val_loss: 0.1163 - val_mse: 0.0952 - val_mae: 0.2519 - lr: 0.0010\n",
      "Epoch 175/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1141 - mse: 0.0947 - mae: 0.2524 - val_loss: 0.1159 - val_mse: 0.0949 - val_mae: 0.2512 - lr: 0.0010\n",
      "Epoch 176/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1137 - mse: 0.0944 - mae: 0.2519 - val_loss: 0.1157 - val_mse: 0.0947 - val_mae: 0.2508 - lr: 0.0010\n",
      "Epoch 177/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1135 - mse: 0.0941 - mae: 0.2513 - val_loss: 0.1154 - val_mse: 0.0944 - val_mae: 0.2502 - lr: 0.0010\n",
      "Epoch 178/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1131 - mse: 0.0939 - mae: 0.2507 - val_loss: 0.1149 - val_mse: 0.0940 - val_mae: 0.2495 - lr: 0.0010\n",
      "Epoch 179/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1128 - mse: 0.0936 - mae: 0.2502 - val_loss: 0.1148 - val_mse: 0.0939 - val_mae: 0.2491 - lr: 0.0010\n",
      "Epoch 180/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1125 - mse: 0.0933 - mae: 0.2496 - val_loss: 0.1146 - val_mse: 0.0937 - val_mae: 0.2487 - lr: 0.0010\n",
      "Epoch 181/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1122 - mse: 0.0930 - mae: 0.2491 - val_loss: 0.1142 - val_mse: 0.0934 - val_mae: 0.2482 - lr: 0.0010\n",
      "Epoch 182/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1119 - mse: 0.0928 - mae: 0.2486 - val_loss: 0.1140 - val_mse: 0.0932 - val_mae: 0.2477 - lr: 0.0010\n",
      "Epoch 183/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1115 - mse: 0.0925 - mae: 0.2480 - val_loss: 0.1136 - val_mse: 0.0928 - val_mae: 0.2471 - lr: 0.0010\n",
      "Epoch 184/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1113 - mse: 0.0922 - mae: 0.2475 - val_loss: 0.1132 - val_mse: 0.0925 - val_mae: 0.2466 - lr: 0.0010\n",
      "Epoch 185/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1110 - mse: 0.0920 - mae: 0.2471 - val_loss: 0.1130 - val_mse: 0.0923 - val_mae: 0.2462 - lr: 0.0010\n",
      "Epoch 186/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1107 - mse: 0.0918 - mae: 0.2466 - val_loss: 0.1128 - val_mse: 0.0922 - val_mae: 0.2458 - lr: 0.0010\n",
      "Epoch 187/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1104 - mse: 0.0916 - mae: 0.2462 - val_loss: 0.1124 - val_mse: 0.0918 - val_mae: 0.2452 - lr: 0.0010\n",
      "Epoch 188/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1101 - mse: 0.0913 - mae: 0.2456 - val_loss: 0.1120 - val_mse: 0.0915 - val_mae: 0.2448 - lr: 0.0010\n",
      "Epoch 189/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1097 - mse: 0.0910 - mae: 0.2452 - val_loss: 0.1119 - val_mse: 0.0914 - val_mae: 0.2445 - lr: 0.0010\n",
      "Epoch 190/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1094 - mse: 0.0907 - mae: 0.2446 - val_loss: 0.1114 - val_mse: 0.0910 - val_mae: 0.2437 - lr: 0.0010\n",
      "Epoch 191/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1091 - mse: 0.0905 - mae: 0.2442 - val_loss: 0.1113 - val_mse: 0.0909 - val_mae: 0.2435 - lr: 0.0010\n",
      "Epoch 192/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1088 - mse: 0.0902 - mae: 0.2436 - val_loss: 0.1109 - val_mse: 0.0905 - val_mae: 0.2429 - lr: 0.0010\n",
      "Epoch 193/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1085 - mse: 0.0900 - mae: 0.2432 - val_loss: 0.1104 - val_mse: 0.0902 - val_mae: 0.2424 - lr: 0.0010\n",
      "Epoch 194/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1082 - mse: 0.0897 - mae: 0.2427 - val_loss: 0.1102 - val_mse: 0.0900 - val_mae: 0.2419 - lr: 0.0010\n",
      "Epoch 195/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1079 - mse: 0.0894 - mae: 0.2422 - val_loss: 0.1100 - val_mse: 0.0899 - val_mae: 0.2416 - lr: 0.0010\n",
      "Epoch 196/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1075 - mse: 0.0892 - mae: 0.2415 - val_loss: 0.1097 - val_mse: 0.0896 - val_mae: 0.2410 - lr: 0.0010\n",
      "Epoch 197/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1072 - mse: 0.0889 - mae: 0.2410 - val_loss: 0.1092 - val_mse: 0.0892 - val_mae: 0.2404 - lr: 0.0010\n",
      "Epoch 198/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1068 - mse: 0.0886 - mae: 0.2404 - val_loss: 0.1090 - val_mse: 0.0890 - val_mae: 0.2399 - lr: 0.0010\n",
      "Epoch 199/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1065 - mse: 0.0884 - mae: 0.2399 - val_loss: 0.1087 - val_mse: 0.0887 - val_mae: 0.2394 - lr: 0.0010\n",
      "Epoch 200/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1062 - mse: 0.0881 - mae: 0.2394 - val_loss: 0.1083 - val_mse: 0.0884 - val_mae: 0.2388 - lr: 0.0010\n",
      "Epoch 201/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1059 - mse: 0.0879 - mae: 0.2388 - val_loss: 0.1079 - val_mse: 0.0882 - val_mae: 0.2383 - lr: 0.0010\n",
      "Epoch 202/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1056 - mse: 0.0876 - mae: 0.2382 - val_loss: 0.1077 - val_mse: 0.0880 - val_mae: 0.2379 - lr: 0.0010\n",
      "Epoch 203/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1052 - mse: 0.0873 - mae: 0.2376 - val_loss: 0.1073 - val_mse: 0.0876 - val_mae: 0.2369 - lr: 0.0010\n",
      "Epoch 204/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1049 - mse: 0.0871 - mae: 0.2371 - val_loss: 0.1068 - val_mse: 0.0872 - val_mae: 0.2363 - lr: 0.0010\n",
      "Epoch 205/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1045 - mse: 0.0868 - mae: 0.2365 - val_loss: 0.1066 - val_mse: 0.0871 - val_mae: 0.2359 - lr: 0.0010\n",
      "Epoch 206/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1042 - mse: 0.0865 - mae: 0.2359 - val_loss: 0.1063 - val_mse: 0.0868 - val_mae: 0.2355 - lr: 0.0010\n",
      "Epoch 207/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.1040 - mse: 0.0863 - mae: 0.2356 - val_loss: 0.1059 - val_mse: 0.0865 - val_mae: 0.2349 - lr: 0.0010\n",
      "Epoch 208/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1037 - mse: 0.0861 - mae: 0.2351 - val_loss: 0.1055 - val_mse: 0.0862 - val_mae: 0.2343 - lr: 0.0010\n",
      "Epoch 209/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1033 - mse: 0.0858 - mae: 0.2345 - val_loss: 0.1052 - val_mse: 0.0860 - val_mae: 0.2338 - lr: 0.0010\n",
      "Epoch 210/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1030 - mse: 0.0856 - mae: 0.2340 - val_loss: 0.1049 - val_mse: 0.0857 - val_mae: 0.2333 - lr: 0.0010\n",
      "Epoch 211/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1028 - mse: 0.0855 - mae: 0.2338 - val_loss: 0.1047 - val_mse: 0.0856 - val_mae: 0.2329 - lr: 0.0010\n",
      "Epoch 212/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1026 - mse: 0.0852 - mae: 0.2332 - val_loss: 0.1042 - val_mse: 0.0852 - val_mae: 0.2325 - lr: 0.0010\n",
      "Epoch 213/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1022 - mse: 0.0850 - mae: 0.2328 - val_loss: 0.1040 - val_mse: 0.0850 - val_mae: 0.2317 - lr: 0.0010\n",
      "Epoch 214/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1020 - mse: 0.0848 - mae: 0.2325 - val_loss: 0.1037 - val_mse: 0.0848 - val_mae: 0.2315 - lr: 0.0010\n",
      "Epoch 215/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1017 - mse: 0.0845 - mae: 0.2319 - val_loss: 0.1036 - val_mse: 0.0847 - val_mae: 0.2311 - lr: 0.0010\n",
      "Epoch 216/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1014 - mse: 0.0843 - mae: 0.2315 - val_loss: 0.1034 - val_mse: 0.0845 - val_mae: 0.2308 - lr: 0.0010\n",
      "Epoch 217/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1013 - mse: 0.0842 - mae: 0.2313 - val_loss: 0.1031 - val_mse: 0.0842 - val_mae: 0.2302 - lr: 0.0010\n",
      "Epoch 218/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1010 - mse: 0.0840 - mae: 0.2309 - val_loss: 0.1029 - val_mse: 0.0841 - val_mae: 0.2299 - lr: 0.0010\n",
      "Epoch 219/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1009 - mse: 0.0839 - mae: 0.2305 - val_loss: 0.1026 - val_mse: 0.0839 - val_mae: 0.2293 - lr: 0.0010\n",
      "Epoch 220/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1007 - mse: 0.0837 - mae: 0.2302 - val_loss: 0.1022 - val_mse: 0.0836 - val_mae: 0.2289 - lr: 0.0010\n",
      "Epoch 221/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1005 - mse: 0.0835 - mae: 0.2298 - val_loss: 0.1021 - val_mse: 0.0835 - val_mae: 0.2287 - lr: 0.0010\n",
      "Epoch 222/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1003 - mse: 0.0835 - mae: 0.2297 - val_loss: 0.1019 - val_mse: 0.0834 - val_mae: 0.2281 - lr: 0.0010\n",
      "Epoch 223/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1001 - mse: 0.0833 - mae: 0.2292 - val_loss: 0.1016 - val_mse: 0.0831 - val_mae: 0.2277 - lr: 0.0010\n",
      "Epoch 224/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0999 - mse: 0.0831 - mae: 0.2288 - val_loss: 0.1013 - val_mse: 0.0829 - val_mae: 0.2271 - lr: 0.0010\n",
      "Epoch 225/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0997 - mse: 0.0829 - mae: 0.2285 - val_loss: 0.1016 - val_mse: 0.0831 - val_mae: 0.2276 - lr: 0.0010\n",
      "Epoch 226/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0996 - mse: 0.0829 - mae: 0.2282 - val_loss: 0.1009 - val_mse: 0.0826 - val_mae: 0.2266 - lr: 0.0010\n",
      "Epoch 227/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0994 - mse: 0.0828 - mae: 0.2280 - val_loss: 0.1008 - val_mse: 0.0825 - val_mae: 0.2263 - lr: 0.0010\n",
      "Epoch 228/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0991 - mse: 0.0825 - mae: 0.2275 - val_loss: 0.1007 - val_mse: 0.0824 - val_mae: 0.2262 - lr: 0.0010\n",
      "Epoch 229/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0990 - mse: 0.0824 - mae: 0.2271 - val_loss: 0.1003 - val_mse: 0.0821 - val_mae: 0.2256 - lr: 0.0010\n",
      "Epoch 230/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0988 - mse: 0.0822 - mae: 0.2267 - val_loss: 0.1003 - val_mse: 0.0821 - val_mae: 0.2255 - lr: 0.0010\n",
      "Epoch 231/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0987 - mse: 0.0822 - mae: 0.2264 - val_loss: 0.0999 - val_mse: 0.0818 - val_mae: 0.2250 - lr: 0.0010\n",
      "Epoch 232/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0985 - mse: 0.0820 - mae: 0.2261 - val_loss: 0.0999 - val_mse: 0.0818 - val_mae: 0.2247 - lr: 0.0010\n",
      "Epoch 233/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0983 - mse: 0.0819 - mae: 0.2258 - val_loss: 0.0998 - val_mse: 0.0817 - val_mae: 0.2242 - lr: 0.0010\n",
      "Epoch 234/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0982 - mse: 0.0818 - mae: 0.2255 - val_loss: 0.0993 - val_mse: 0.0813 - val_mae: 0.2236 - lr: 0.0010\n",
      "Epoch 235/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0980 - mse: 0.0816 - mae: 0.2252 - val_loss: 0.0994 - val_mse: 0.0814 - val_mae: 0.2238 - lr: 0.0010\n",
      "Epoch 236/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0979 - mse: 0.0816 - mae: 0.2249 - val_loss: 0.0992 - val_mse: 0.0812 - val_mae: 0.2234 - lr: 0.0010\n",
      "Epoch 237/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0978 - mse: 0.0815 - mae: 0.2246 - val_loss: 0.0990 - val_mse: 0.0810 - val_mae: 0.2229 - lr: 0.0010\n",
      "Epoch 238/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0975 - mse: 0.0813 - mae: 0.2241 - val_loss: 0.0987 - val_mse: 0.0808 - val_mae: 0.2223 - lr: 0.0010\n",
      "Epoch 239/500\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0974 - mse: 0.0812 - mae: 0.2238 - val_loss: 0.0985 - val_mse: 0.0807 - val_mae: 0.2220 - lr: 0.0010\n",
      "Epoch 240/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0973 - mse: 0.0811 - mae: 0.2235 - val_loss: 0.0983 - val_mse: 0.0805 - val_mae: 0.2217 - lr: 0.0010\n",
      "Epoch 241/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0971 - mse: 0.0809 - mae: 0.2232 - val_loss: 0.0983 - val_mse: 0.0805 - val_mae: 0.2215 - lr: 0.0010\n",
      "Epoch 242/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0970 - mse: 0.0809 - mae: 0.2228 - val_loss: 0.0982 - val_mse: 0.0804 - val_mae: 0.2209 - lr: 0.0010\n",
      "Epoch 243/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0968 - mse: 0.0806 - mae: 0.2223 - val_loss: 0.0979 - val_mse: 0.0802 - val_mae: 0.2207 - lr: 0.0010\n",
      "Epoch 244/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0967 - mse: 0.0806 - mae: 0.2221 - val_loss: 0.0978 - val_mse: 0.0801 - val_mae: 0.2203 - lr: 0.0010\n",
      "Epoch 245/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0966 - mse: 0.0805 - mae: 0.2218 - val_loss: 0.0977 - val_mse: 0.0801 - val_mae: 0.2202 - lr: 0.0010\n",
      "Epoch 246/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0964 - mse: 0.0804 - mae: 0.2215 - val_loss: 0.0977 - val_mse: 0.0800 - val_mae: 0.2198 - lr: 0.0010\n",
      "Epoch 247/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0963 - mse: 0.0803 - mae: 0.2212 - val_loss: 0.0977 - val_mse: 0.0800 - val_mae: 0.2196 - lr: 0.0010\n",
      "Epoch 248/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0962 - mse: 0.0802 - mae: 0.2209 - val_loss: 0.0974 - val_mse: 0.0798 - val_mae: 0.2191 - lr: 0.0010\n",
      "Epoch 249/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0961 - mse: 0.0802 - mae: 0.2206 - val_loss: 0.0973 - val_mse: 0.0797 - val_mae: 0.2189 - lr: 0.0010\n",
      "Epoch 250/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0961 - mse: 0.0801 - mae: 0.2204 - val_loss: 0.0972 - val_mse: 0.0796 - val_mae: 0.2187 - lr: 0.0010\n",
      "Epoch 251/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0960 - mse: 0.0800 - mae: 0.2201 - val_loss: 0.0971 - val_mse: 0.0796 - val_mae: 0.2184 - lr: 0.0010\n",
      "Epoch 252/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0959 - mse: 0.0800 - mae: 0.2198 - val_loss: 0.0969 - val_mse: 0.0794 - val_mae: 0.2181 - lr: 0.0010\n",
      "Epoch 253/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0958 - mse: 0.0799 - mae: 0.2196 - val_loss: 0.0968 - val_mse: 0.0793 - val_mae: 0.2176 - lr: 0.0010\n",
      "Epoch 254/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0957 - mse: 0.0798 - mae: 0.2194 - val_loss: 0.0968 - val_mse: 0.0793 - val_mae: 0.2176 - lr: 0.0010\n",
      "Epoch 255/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0956 - mse: 0.0797 - mae: 0.2189 - val_loss: 0.0967 - val_mse: 0.0792 - val_mae: 0.2170 - lr: 0.0010\n",
      "Epoch 256/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0955 - mse: 0.0796 - mae: 0.2187 - val_loss: 0.0966 - val_mse: 0.0791 - val_mae: 0.2168 - lr: 0.0010\n",
      "Epoch 257/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0955 - mse: 0.0797 - mae: 0.2186 - val_loss: 0.0965 - val_mse: 0.0790 - val_mae: 0.2165 - lr: 0.0010\n",
      "Epoch 258/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0953 - mse: 0.0795 - mae: 0.2182 - val_loss: 0.0963 - val_mse: 0.0789 - val_mae: 0.2166 - lr: 0.0010\n",
      "Epoch 259/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0954 - mse: 0.0795 - mae: 0.2182 - val_loss: 0.0962 - val_mse: 0.0788 - val_mae: 0.2160 - lr: 0.0010\n",
      "Epoch 260/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0952 - mse: 0.0794 - mae: 0.2179 - val_loss: 0.0963 - val_mse: 0.0789 - val_mae: 0.2160 - lr: 0.0010\n",
      "Epoch 261/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0951 - mse: 0.0793 - mae: 0.2175 - val_loss: 0.0961 - val_mse: 0.0787 - val_mae: 0.2158 - lr: 0.0010\n",
      "Epoch 262/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0951 - mse: 0.0793 - mae: 0.2173 - val_loss: 0.0960 - val_mse: 0.0787 - val_mae: 0.2153 - lr: 0.0010\n",
      "Epoch 263/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0949 - mse: 0.0792 - mae: 0.2170 - val_loss: 0.0960 - val_mse: 0.0786 - val_mae: 0.2155 - lr: 0.0010\n",
      "Epoch 264/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0948 - mse: 0.0791 - mae: 0.2167 - val_loss: 0.0958 - val_mse: 0.0785 - val_mae: 0.2147 - lr: 0.0010\n",
      "Epoch 265/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0948 - mse: 0.0791 - mae: 0.2166 - val_loss: 0.0958 - val_mse: 0.0785 - val_mae: 0.2148 - lr: 0.0010\n",
      "Epoch 266/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0947 - mse: 0.0790 - mae: 0.2163 - val_loss: 0.0958 - val_mse: 0.0785 - val_mae: 0.2146 - lr: 0.0010\n",
      "Epoch 267/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0947 - mse: 0.0789 - mae: 0.2161 - val_loss: 0.0955 - val_mse: 0.0783 - val_mae: 0.2140 - lr: 0.0010\n",
      "Epoch 268/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0946 - mse: 0.0789 - mae: 0.2158 - val_loss: 0.0956 - val_mse: 0.0783 - val_mae: 0.2140 - lr: 0.0010\n",
      "Epoch 269/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0946 - mse: 0.0788 - mae: 0.2158 - val_loss: 0.0955 - val_mse: 0.0782 - val_mae: 0.2138 - lr: 0.0010\n",
      "Epoch 270/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0944 - mse: 0.0787 - mae: 0.2155 - val_loss: 0.0953 - val_mse: 0.0781 - val_mae: 0.2136 - lr: 0.0010\n",
      "Epoch 271/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0944 - mse: 0.0787 - mae: 0.2152 - val_loss: 0.0954 - val_mse: 0.0782 - val_mae: 0.2136 - lr: 0.0010\n",
      "Epoch 272/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0943 - mse: 0.0786 - mae: 0.2151 - val_loss: 0.0954 - val_mse: 0.0782 - val_mae: 0.2135 - lr: 0.0010\n",
      "Epoch 273/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0942 - mse: 0.0785 - mae: 0.2148 - val_loss: 0.0954 - val_mse: 0.0782 - val_mae: 0.2135 - lr: 0.0010\n",
      "Epoch 274/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0943 - mse: 0.0786 - mae: 0.2148 - val_loss: 0.0952 - val_mse: 0.0779 - val_mae: 0.2128 - lr: 0.0010\n",
      "Epoch 275/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0942 - mse: 0.0785 - mae: 0.2146 - val_loss: 0.0949 - val_mse: 0.0778 - val_mae: 0.2126 - lr: 0.0010\n",
      "Epoch 276/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0940 - mse: 0.0784 - mae: 0.2142 - val_loss: 0.0951 - val_mse: 0.0779 - val_mae: 0.2128 - lr: 0.0010\n",
      "Epoch 277/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0940 - mse: 0.0784 - mae: 0.2142 - val_loss: 0.0949 - val_mse: 0.0777 - val_mae: 0.2120 - lr: 0.0010\n",
      "Epoch 278/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0939 - mse: 0.0783 - mae: 0.2138 - val_loss: 0.0949 - val_mse: 0.0777 - val_mae: 0.2122 - lr: 0.0010\n",
      "Epoch 279/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0939 - mse: 0.0783 - mae: 0.2140 - val_loss: 0.0951 - val_mse: 0.0779 - val_mae: 0.2125 - lr: 0.0010\n",
      "Epoch 280/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0939 - mse: 0.0783 - mae: 0.2138 - val_loss: 0.0948 - val_mse: 0.0777 - val_mae: 0.2122 - lr: 0.0010\n",
      "Epoch 281/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0939 - mse: 0.0782 - mae: 0.2137 - val_loss: 0.0947 - val_mse: 0.0776 - val_mae: 0.2117 - lr: 0.0010\n",
      "Epoch 282/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0938 - mse: 0.0782 - mae: 0.2136 - val_loss: 0.0947 - val_mse: 0.0776 - val_mae: 0.2117 - lr: 0.0010\n",
      "Epoch 283/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0937 - mse: 0.0781 - mae: 0.2132 - val_loss: 0.0947 - val_mse: 0.0775 - val_mae: 0.2111 - lr: 0.0010\n",
      "Epoch 284/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0936 - mse: 0.0780 - mae: 0.2130 - val_loss: 0.0946 - val_mse: 0.0775 - val_mae: 0.2113 - lr: 0.0010\n",
      "Epoch 285/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0937 - mse: 0.0780 - mae: 0.2130 - val_loss: 0.0947 - val_mse: 0.0775 - val_mae: 0.2114 - lr: 0.0010\n",
      "Epoch 286/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0936 - mse: 0.0780 - mae: 0.2129 - val_loss: 0.0945 - val_mse: 0.0774 - val_mae: 0.2111 - lr: 0.0010\n",
      "Epoch 287/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0935 - mse: 0.0780 - mae: 0.2129 - val_loss: 0.0945 - val_mse: 0.0774 - val_mae: 0.2111 - lr: 0.0010\n",
      "Epoch 288/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0935 - mse: 0.0779 - mae: 0.2128 - val_loss: 0.0943 - val_mse: 0.0773 - val_mae: 0.2110 - lr: 0.0010\n",
      "Epoch 289/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0935 - mse: 0.0779 - mae: 0.2126 - val_loss: 0.0944 - val_mse: 0.0773 - val_mae: 0.2107 - lr: 0.0010\n",
      "Epoch 290/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0934 - mse: 0.0778 - mae: 0.2123 - val_loss: 0.0943 - val_mse: 0.0773 - val_mae: 0.2106 - lr: 0.0010\n",
      "Epoch 291/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0933 - mse: 0.0778 - mae: 0.2123 - val_loss: 0.0943 - val_mse: 0.0772 - val_mae: 0.2102 - lr: 0.0010\n",
      "Epoch 292/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0933 - mse: 0.0778 - mae: 0.2121 - val_loss: 0.0941 - val_mse: 0.0771 - val_mae: 0.2101 - lr: 0.0010\n",
      "Epoch 293/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0933 - mse: 0.0778 - mae: 0.2119 - val_loss: 0.0941 - val_mse: 0.0770 - val_mae: 0.2098 - lr: 0.0010\n",
      "Epoch 294/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0933 - mse: 0.0777 - mae: 0.2119 - val_loss: 0.0942 - val_mse: 0.0772 - val_mae: 0.2101 - lr: 0.0010\n",
      "Epoch 295/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0932 - mse: 0.0777 - mae: 0.2117 - val_loss: 0.0941 - val_mse: 0.0771 - val_mae: 0.2100 - lr: 0.0010\n",
      "Epoch 296/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0932 - mse: 0.0776 - mae: 0.2116 - val_loss: 0.0940 - val_mse: 0.0770 - val_mae: 0.2095 - lr: 0.0010\n",
      "Epoch 297/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0931 - mse: 0.0776 - mae: 0.2114 - val_loss: 0.0940 - val_mse: 0.0770 - val_mae: 0.2095 - lr: 0.0010\n",
      "Epoch 298/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0931 - mse: 0.0776 - mae: 0.2114 - val_loss: 0.0939 - val_mse: 0.0769 - val_mae: 0.2096 - lr: 0.0010\n",
      "Epoch 299/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0930 - mse: 0.0775 - mae: 0.2112 - val_loss: 0.0938 - val_mse: 0.0768 - val_mae: 0.2092 - lr: 0.0010\n",
      "Epoch 300/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0930 - mse: 0.0775 - mae: 0.2112 - val_loss: 0.0939 - val_mse: 0.0768 - val_mae: 0.2089 - lr: 0.0010\n",
      "Epoch 301/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0929 - mse: 0.0774 - mae: 0.2109 - val_loss: 0.0938 - val_mse: 0.0768 - val_mae: 0.2091 - lr: 0.0010\n",
      "Epoch 302/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0929 - mse: 0.0774 - mae: 0.2109 - val_loss: 0.0938 - val_mse: 0.0768 - val_mae: 0.2090 - lr: 0.0010\n",
      "Epoch 303/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0930 - mse: 0.0775 - mae: 0.2110 - val_loss: 0.0938 - val_mse: 0.0767 - val_mae: 0.2088 - lr: 0.0010\n",
      "Epoch 304/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0929 - mse: 0.0774 - mae: 0.2108 - val_loss: 0.0935 - val_mse: 0.0766 - val_mae: 0.2085 - lr: 0.0010\n",
      "Epoch 305/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0928 - mse: 0.0773 - mae: 0.2107 - val_loss: 0.0936 - val_mse: 0.0767 - val_mae: 0.2086 - lr: 0.0010\n",
      "Epoch 306/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0927 - mse: 0.0773 - mae: 0.2106 - val_loss: 0.0936 - val_mse: 0.0767 - val_mae: 0.2087 - lr: 0.0010\n",
      "Epoch 307/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0927 - mse: 0.0772 - mae: 0.2104 - val_loss: 0.0936 - val_mse: 0.0766 - val_mae: 0.2085 - lr: 0.0010\n",
      "Epoch 308/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0926 - mse: 0.0772 - mae: 0.2104 - val_loss: 0.0935 - val_mse: 0.0766 - val_mae: 0.2082 - lr: 0.0010\n",
      "Epoch 309/500\n",
      "35/50 [====================>.........] - ETA: 0s - loss: 0.0923 - mse: 0.0767 - mae: 0.2099\n",
      "Epoch 309: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0927 - mse: 0.0772 - mae: 0.2105 - val_loss: 0.0936 - val_mse: 0.0766 - val_mae: 0.2083 - lr: 0.0010\n",
      "Epoch 310/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0925 - mse: 0.0771 - mae: 0.2101 - val_loss: 0.0935 - val_mse: 0.0765 - val_mae: 0.2081 - lr: 5.0000e-04\n",
      "Epoch 311/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0926 - mse: 0.0771 - mae: 0.2101 - val_loss: 0.0934 - val_mse: 0.0765 - val_mae: 0.2081 - lr: 5.0000e-04\n",
      "Epoch 312/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0924 - mse: 0.0770 - mae: 0.2099 - val_loss: 0.0936 - val_mse: 0.0766 - val_mae: 0.2082 - lr: 5.0000e-04\n",
      "Epoch 313/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0925 - mse: 0.0771 - mae: 0.2101 - val_loss: 0.0935 - val_mse: 0.0765 - val_mae: 0.2081 - lr: 5.0000e-04\n",
      "Epoch 314/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0924 - mse: 0.0770 - mae: 0.2099 - val_loss: 0.0934 - val_mse: 0.0765 - val_mae: 0.2080 - lr: 5.0000e-04\n",
      "Epoch 315/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0925 - mse: 0.0770 - mae: 0.2100 - val_loss: 0.0934 - val_mse: 0.0764 - val_mae: 0.2079 - lr: 5.0000e-04\n",
      "Epoch 316/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0924 - mse: 0.0770 - mae: 0.2098 - val_loss: 0.0932 - val_mse: 0.0763 - val_mae: 0.2077 - lr: 5.0000e-04\n",
      "Epoch 317/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0924 - mse: 0.0770 - mae: 0.2098 - val_loss: 0.0933 - val_mse: 0.0764 - val_mae: 0.2078 - lr: 5.0000e-04\n",
      "Epoch 318/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0924 - mse: 0.0770 - mae: 0.2098 - val_loss: 0.0933 - val_mse: 0.0764 - val_mae: 0.2076 - lr: 5.0000e-04\n",
      "Epoch 319/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0924 - mse: 0.0770 - mae: 0.2097 - val_loss: 0.0934 - val_mse: 0.0764 - val_mae: 0.2079 - lr: 5.0000e-04\n",
      "Epoch 320/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0924 - mse: 0.0769 - mae: 0.2097 - val_loss: 0.0932 - val_mse: 0.0763 - val_mae: 0.2076 - lr: 5.0000e-04\n",
      "Epoch 321/500\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0927 - mse: 0.0771 - mae: 0.2100\n",
      "Epoch 321: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0923 - mse: 0.0769 - mae: 0.2097 - val_loss: 0.0932 - val_mse: 0.0763 - val_mae: 0.2076 - lr: 5.0000e-04\n",
      "Epoch 322/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0923 - mse: 0.0769 - mae: 0.2095 - val_loss: 0.0932 - val_mse: 0.0763 - val_mae: 0.2075 - lr: 2.5000e-04\n",
      "Epoch 323/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0922 - mse: 0.0769 - mae: 0.2095 - val_loss: 0.0932 - val_mse: 0.0763 - val_mae: 0.2075 - lr: 2.5000e-04\n",
      "Epoch 324/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0922 - mse: 0.0769 - mae: 0.2095 - val_loss: 0.0932 - val_mse: 0.0763 - val_mae: 0.2076 - lr: 2.5000e-04\n",
      "Epoch 325/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0922 - mse: 0.0768 - mae: 0.2095 - val_loss: 0.0932 - val_mse: 0.0763 - val_mae: 0.2075 - lr: 2.5000e-04\n",
      "Epoch 326/500\n",
      "39/50 [======================>.......] - ETA: 0s - loss: 0.0929 - mse: 0.0771 - mae: 0.2100\n",
      "Epoch 326: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0923 - mse: 0.0769 - mae: 0.2095 - val_loss: 0.0932 - val_mse: 0.0763 - val_mae: 0.2075 - lr: 2.5000e-04\n",
      "Epoch 327/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0922 - mse: 0.0768 - mae: 0.2094 - val_loss: 0.0931 - val_mse: 0.0763 - val_mae: 0.2074 - lr: 1.2500e-04\n",
      "Epoch 328/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0922 - mse: 0.0768 - mae: 0.2094 - val_loss: 0.0931 - val_mse: 0.0763 - val_mae: 0.2075 - lr: 1.2500e-04\n",
      "Epoch 329/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0922 - mse: 0.0768 - mae: 0.2094 - val_loss: 0.0931 - val_mse: 0.0763 - val_mae: 0.2074 - lr: 1.2500e-04\n",
      "Epoch 330/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0922 - mse: 0.0768 - mae: 0.2093 - val_loss: 0.0931 - val_mse: 0.0763 - val_mae: 0.2074 - lr: 1.2500e-04\n",
      "Epoch 331/500\n",
      "37/50 [=====================>........] - ETA: 0s - loss: 0.0931 - mse: 0.0775 - mae: 0.2106\n",
      "Epoch 331: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0922 - mse: 0.0768 - mae: 0.2093 - val_loss: 0.0931 - val_mse: 0.0762 - val_mae: 0.2074 - lr: 1.2500e-04\n",
      "Epoch 332/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0921 - mse: 0.0768 - mae: 0.2093 - val_loss: 0.0931 - val_mse: 0.0762 - val_mae: 0.2074 - lr: 6.2500e-05\n",
      "Epoch 333/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0921 - mse: 0.0768 - mae: 0.2093 - val_loss: 0.0931 - val_mse: 0.0762 - val_mae: 0.2074 - lr: 6.2500e-05\n",
      "Epoch 334/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0921 - mse: 0.0768 - mae: 0.2093 - val_loss: 0.0931 - val_mse: 0.0762 - val_mae: 0.2074 - lr: 6.2500e-05\n",
      "Epoch 335/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0921 - mse: 0.0768 - mae: 0.2093 - val_loss: 0.0931 - val_mse: 0.0762 - val_mae: 0.2074 - lr: 6.2500e-05\n",
      "Epoch 336/500\n",
      "42/50 [========================>.....] - ETA: 0s - loss: 0.0916 - mse: 0.0766 - mae: 0.2089\n",
      "Epoch 336: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0921 - mse: 0.0768 - mae: 0.2093 - val_loss: 0.0931 - val_mse: 0.0762 - val_mae: 0.2074 - lr: 6.2500e-05\n",
      "Epoch 337/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0921 - mse: 0.0768 - mae: 0.2093 - val_loss: 0.0931 - val_mse: 0.0762 - val_mae: 0.2074 - lr: 3.1250e-05\n",
      "Epoch 338/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0921 - mse: 0.0768 - mae: 0.2093 - val_loss: 0.0931 - val_mse: 0.0762 - val_mae: 0.2074 - lr: 3.1250e-05\n",
      "Epoch 339/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0921 - mse: 0.0768 - mae: 0.2093 - val_loss: 0.0931 - val_mse: 0.0762 - val_mae: 0.2074 - lr: 3.1250e-05\n",
      "Epoch 340/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0921 - mse: 0.0768 - mae: 0.2093 - val_loss: 0.0931 - val_mse: 0.0762 - val_mae: 0.2074 - lr: 3.1250e-05\n",
      "Epoch 341/500\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0922 - mse: 0.0768 - mae: 0.2093\n",
      "Epoch 341: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0921 - mse: 0.0768 - mae: 0.2093 - val_loss: 0.0931 - val_mse: 0.0762 - val_mae: 0.2074 - lr: 3.1250e-05\n",
      "Epoch 342/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0921 - mse: 0.0767 - mae: 0.2093 - val_loss: 0.0931 - val_mse: 0.0762 - val_mae: 0.2074 - lr: 1.5625e-05\n",
      "Epoch 343/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0921 - mse: 0.0768 - mae: 0.2093 - val_loss: 0.0931 - val_mse: 0.0762 - val_mae: 0.2074 - lr: 1.5625e-05\n",
      "Epoch 344/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0921 - mse: 0.0767 - mae: 0.2092 - val_loss: 0.0931 - val_mse: 0.0762 - val_mae: 0.2074 - lr: 1.5625e-05\n",
      "Epoch 345/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0921 - mse: 0.0767 - mae: 0.2092 - val_loss: 0.0931 - val_mse: 0.0762 - val_mae: 0.2074 - lr: 1.5625e-05\n",
      "Epoch 346/500\n",
      "42/50 [========================>.....] - ETA: 0s - loss: 0.0922 - mse: 0.0769 - mae: 0.2095\n",
      "Epoch 346: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0921 - mse: 0.0767 - mae: 0.2092 - val_loss: 0.0931 - val_mse: 0.0762 - val_mae: 0.2074 - lr: 1.5625e-05\n",
      "Epoch 347/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0921 - mse: 0.0767 - mae: 0.2092 - val_loss: 0.0931 - val_mse: 0.0762 - val_mae: 0.2073 - lr: 7.8125e-06\n",
      "Epoch 348/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0921 - mse: 0.0767 - mae: 0.2092 - val_loss: 0.0931 - val_mse: 0.0762 - val_mae: 0.2074 - lr: 7.8125e-06\n",
      "Epoch 349/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0921 - mse: 0.0767 - mae: 0.2092 - val_loss: 0.0931 - val_mse: 0.0762 - val_mae: 0.2074 - lr: 7.8125e-06\n",
      "Epoch 350/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0921 - mse: 0.0767 - mae: 0.2092 - val_loss: 0.0931 - val_mse: 0.0762 - val_mae: 0.2074 - lr: 7.8125e-06\n",
      "Epoch 351/500\n",
      "43/50 [========================>.....] - ETA: 0s - loss: 0.0915 - mse: 0.0766 - mae: 0.2088\n",
      "Epoch 351: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0921 - mse: 0.0767 - mae: 0.2092 - val_loss: 0.0931 - val_mse: 0.0762 - val_mae: 0.2074 - lr: 7.8125e-06\n",
      "Epoch 352/500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0921 - mse: 0.0767 - mae: 0.2092 - val_loss: 0.0931 - val_mse: 0.0762 - val_mae: 0.2074 - lr: 3.9063e-06\n",
      "Epoch 353/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0921 - mse: 0.0767 - mae: 0.2092 - val_loss: 0.0931 - val_mse: 0.0762 - val_mae: 0.2074 - lr: 3.9063e-06\n",
      "Epoch 354/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0921 - mse: 0.0767 - mae: 0.2092 - val_loss: 0.0931 - val_mse: 0.0762 - val_mae: 0.2074 - lr: 3.9063e-06\n",
      "Epoch 355/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0921 - mse: 0.0767 - mae: 0.2092 - val_loss: 0.0931 - val_mse: 0.0762 - val_mae: 0.2073 - lr: 3.9063e-06\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=500,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[adjust_lr, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0931 - mse: 0.0762 - mae: 0.2074\n",
      "Test MSE: 0.0762, Test MAE: 0.2074\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_mse, test_mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test MSE: {test_mse:.4f}, Test MAE: {test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# path = r\"D:\\0000000DONE\\Masters\\error-correcting-codes\\final work\\results\"\n",
    "\n",
    "# history_df = pd.DataFrame(history.history)\n",
    "# pwd = os.getcwd()\n",
    "# file_path = os.path.join(path, 'model_gen_matrix_DCT3_history.csv')\n",
    "# history_df.to_csv(file_path, index=False)\n",
    "\n",
    "# print(f\"File saved at: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.plot(history.history['loss'], label='Training Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.yscale('log')  # Apply logarithmic scale to y-axis\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 2ms/step\n",
      "Comparison of predictions and ground truth:\n",
      "Sample 1:\n",
      "  Predicted:    [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1. 1. 4. 2.]\n",
      "  Ground Truth: [1. 0. 1. 1. 1. 1. 3. 0. 1. 3. 1. 2. 1. 1. 4. 2.]\n",
      "----------------------------------------\n",
      "Sample 2:\n",
      "  Predicted:    [2. 1. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 3. 0. 1. 0.]\n",
      "  Ground Truth: [4. 0. 2. 0. 2. 0. 3. 2. 1. 2. 2. 2. 3. 0. 1. 0.]\n",
      "----------------------------------------\n",
      "Sample 3:\n",
      "  Predicted:    [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 3. 4. 4. 0. 1.]\n",
      "  Ground Truth: [3. 2. 0. 0. 2. 0. 4. 4. 3. 3. 1. 3. 4. 3. 0. 1.]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = model.predict(X_test)\n",
    "y_pred_test_rescaled = y_pred_test * (q - 1)\n",
    "y_test_rescaled = y_test * (q - 1)\n",
    "\n",
    "print(\"Comparison of predictions and ground truth:\")\n",
    "for i in range(3):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    # print(f\"predicted as is: \", y_pred_test[i])\n",
    "    print(f\"  Predicted:    {np.round(np.abs(y_pred_test_rescaled[i]))}\")\n",
    "    print(f\"  Ground Truth: {np.round(np.abs(y_test_rescaled[i]))}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 42ms/step\n",
      "\n",
      "Comparison of Unseen Data and Predictions:\n",
      "============================================================\n",
      "Sample 1:\n",
      "  Prediction  : [2 2 2 2 2 2 2 1 1 2 3 2 3 1 2 0]\n",
      "  Ground Truth: [3 4 2 4 4 1 2 2 2 4 3 2 4 1 3 1]\n",
      "------------------------------------------------------------\n",
      "Sample 2:\n",
      "  Prediction  : [2 2 2 2 0 2 2 3 2 2 2 3 3 3 1 0]\n",
      "  Ground Truth: [1 3 2 4 2 1 3 4 3 2 1 4 2 3 1 4]\n",
      "------------------------------------------------------------\n",
      "Sample 3:\n",
      "  Prediction  : [2 2 2 2 4 2 2 2 2 2 0 1 0 2 3 1]\n",
      "  Ground Truth: [4 2 3 1 3 4 2 1 4 3 2 1 3 2 4 1]\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_unseen_samples = 3\n",
    "# unseen_data = np.random.randint(0, q, size=(num_unseen_samples, n))\n",
    "# unseen_data = np.array([3, 4, 2, 4, 4, 1, 2, 2, 2, 4, 3, 2, 4, 1, 3, 1])\n",
    "\n",
    "unseen_data = np.array([\n",
    "    [3, 4, 2, 4, 4, 1, 2, 2, 2, 4, 3, 2, 4, 1, 3, 1],\n",
    "    [1, 3, 2, 4, 2, 1, 3, 4, 3, 2, 1, 4, 2, 3, 1, 4],\n",
    "    [4, 2, 3, 1, 3, 4, 2, 1, 4, 3, 2, 1, 3, 2, 4, 1]\n",
    "])  # Manually specified data\n",
    "\n",
    "\n",
    "padded_unseen_data = np.hstack((unseen_data, np.zeros((num_unseen_samples, n_padded - n))))\n",
    "\n",
    "unseen_data_normalized = unseen_data.astype(np.float32) / (q - 1)\n",
    "\n",
    "# print(\"Unseen Input Data (Before Encoding):\")\n",
    "# print(unseen_data)\n",
    "\n",
    "unseen_encoded = np.array([np.dot(M_tilde, x) for x in padded_unseen_data])\n",
    "unseen_encoded[np.abs(unseen_encoded) < 1e-10] = 0\n",
    "unseen_encoded = np.round(unseen_encoded, decimals=10)\n",
    "\n",
    "k = np.arange(n_padded)\n",
    "phase_shift = np.exp(-1j * np.pi * k / (2 * n_padded))\n",
    "alpha_k = np.where(k == 0, np.sqrt(1/n_padded), np.sqrt(2/n_padded))\n",
    "\n",
    "dct2_dataset_unseen = np.real(unseen_encoded * phase_shift) * alpha_k\n",
    "dct2_dataset_unseen = np.round(dct2_dataset_unseen, 10)\n",
    "\n",
    "mean_real = np.mean(dct2_dataset_unseen, axis=0)\n",
    "std_real = np.std(dct2_dataset_unseen, axis=0) + 1e-8\n",
    "dct2_dataset_unseen = (dct2_dataset_unseen - mean_real) / std_real\n",
    "\n",
    "y_pred_unseen = model.predict(dct2_dataset_unseen)\n",
    "\n",
    "y_pred_unseen_rescaled = y_pred_unseen * (q - 1)\n",
    "\n",
    "y_pred_unseen_final = np.mod(np.round(y_pred_unseen_rescaled), q)\n",
    "\n",
    "print(\"\\nComparison of Unseen Data and Predictions:\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(num_unseen_samples):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  Prediction  : {y_pred_unseen_final[i].astype(int)}\")\n",
    "    print(f\"  Ground Truth: {padded_unseen_data[i].astype(int)}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Gradient Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input has no trainable variables.\n",
      "real_layer_1 Gradient Mean: 1.1781706\n",
      "leaky_re_lu has no trainable variables.\n",
      "real_support_layer_1 Gradient Mean: 1.7026582\n",
      "leaky_re_lu_1 has no trainable variables.\n",
      "real_layer_2 Gradient Mean: 1.3373591\n",
      "leaky_re_lu_2 has no trainable variables.\n",
      "real_support_layer_2 Gradient Mean: 1.2227685\n",
      "leaky_re_lu_3 has no trainable variables.\n",
      "real_layer_3 Gradient Mean: 1.1078273\n",
      "leaky_re_lu_4 has no trainable variables.\n",
      "output_layer Gradient Mean: 2.429224\n",
      "activation has no trainable variables.\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    if layer.trainable_variables:\n",
    "        grads = tf.reduce_mean(tf.abs(layer.trainable_variables[0]))\n",
    "        print(layer.name, \"Gradient Mean:\", grads.numpy())\n",
    "    else:\n",
    "        print(layer.name, \"has no trainable variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Gradient OK for real_layer_1/kernel_d1:0, mean: -0.006991651840507984\n",
      "✅ Gradient OK for real_layer_1/kernel_d2:0, mean: -0.009233171120285988\n",
      "✅ Gradient OK for real_layer_1/bias:0, mean: -0.00024196412414312363\n",
      "✅ Gradient OK for real_support_layer_1/kernel_m:0, mean: 0.00516069121658802\n",
      "✅ Gradient OK for real_support_layer_1/bias:0, mean: 0.0058286855928599834\n",
      "✅ Gradient OK for real_layer_2/kernel_d1:0, mean: 0.004125332925468683\n",
      "✅ Gradient OK for real_layer_2/kernel_d2:0, mean: 0.018778635188937187\n",
      "✅ Gradient OK for real_layer_2/kernel_w:0, mean: -0.059736695140600204\n",
      "✅ Gradient OK for real_layer_2/kernel_C_1:0, mean: 0.02814575470983982\n",
      "✅ Gradient OK for real_layer_2/kernel_C_2:0, mean: 0.02111365832388401\n",
      "✅ Gradient OK for real_layer_2/bias:0, mean: -0.0006694813491776586\n",
      "✅ Gradient OK for real_support_layer_2/kernel_m:0, mean: 0.006077839992940426\n",
      "✅ Gradient OK for real_support_layer_2/bias:0, mean: -0.0007773307152092457\n",
      "✅ Gradient OK for real_layer_3/kernel_w:0, mean: -0.011825831606984138\n",
      "✅ Gradient OK for real_layer_3/bias:0, mean: 0.0046003046445548534\n",
      "✅ Gradient OK for output_layer/kernel_m:0, mean: 0.01882244646549225\n",
      "✅ Gradient OK for output_layer/bias:0, mean: 0.014941285364329815\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4 \n",
    "X_dummy = np.random.rand(batch_size, X_train.shape[1]).astype(np.float32)\n",
    "y_dummy = np.random.rand(batch_size, y_train.shape[1]).astype(np.float32)\n",
    "\n",
    "def check_gradients(model, X_dummy, y_dummy):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(model.trainable_variables)\n",
    "        predictions = model(X_dummy, training=True)\n",
    "        loss = tf.keras.losses.mean_squared_error(y_dummy, predictions)\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    for i, (grad, var) in enumerate(zip(grads, model.trainable_variables)):\n",
    "        if grad is None:\n",
    "            print(f\"🚨 Gradient is None for {var.name} at index {i}\")\n",
    "        else:\n",
    "            print(f\"✅ Gradient OK for {var.name}, mean: {tf.reduce_mean(grad).numpy()}\")\n",
    "\n",
    "check_gradients(model, X_dummy, y_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
