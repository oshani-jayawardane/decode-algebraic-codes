{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCT3SNN - DCT-2 Encoded Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is encoded using the recursive radix-2 DCT-II algorithm. The encodings are real valued. </br>\n",
    "Results demonstrate perfect reconstruction of the original codeword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (r + 1) | (q - 1)\n",
    "# (r + 1) | n\n",
    "# q is a prime number\n",
    "# n = 2^t\n",
    "# r < n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset -f\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original n: 16\n",
      "Padded n: 16\n",
      "Generated dataset shape: (1000, 16)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n = 16\n",
    "q = 5\n",
    "num_samples = 1000\n",
    "\n",
    "def next_power_of_two(x):\n",
    "    return 1 if x == 0 else 2**(x - 1).bit_length()\n",
    "\n",
    "n_padded = next_power_of_two(n)\n",
    "\n",
    "dataset = np.random.randint(0, q, size=(num_samples, n))\n",
    "\n",
    "if n_padded > n:\n",
    "    pad_width = n_padded - n\n",
    "    dataset = np.pad(dataset, ((0, 0), (0, pad_width)), mode='constant', constant_values=0)\n",
    "\n",
    "print(\"Original n:\", n)\n",
    "print(\"Padded n:\", n_padded)\n",
    "print(\"Generated dataset shape:\", dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 4 2 ... 1 3 1]\n",
      " [3 4 0 ... 3 2 3]\n",
      " [3 0 2 ... 0 1 4]\n",
      " ...\n",
      " [4 4 4 ... 0 2 2]\n",
      " [4 2 4 ... 4 0 0]\n",
      " [2 2 2 ... 0 4 2]]\n",
      "(1000, 16)\n",
      "(16,)\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(dataset.shape)\n",
    "print(dataset[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos2(x, n):\n",
    "    if n == 2:\n",
    "        H_2 = (1 / np.sqrt(2)) * np.array([[1, 1], [1, -1]])\n",
    "        return H_2 @ x\n",
    "    else:\n",
    "        n1 = n // 2\n",
    "        \n",
    "        I_half = np.eye(n1)\n",
    "        I_tilde_half = np.flip(I_half, axis=1)\n",
    "        H = np.block([\n",
    "            [I_half, I_tilde_half], \n",
    "            [I_half, -I_tilde_half]\n",
    "        ])\n",
    "        H_n = (1 / np.sqrt(2)) * H\n",
    "        u = H_n @ x\n",
    "        \n",
    "        W_entries = [1] * n1 + [ (1 / np.cos((2*k - 1)*np.pi / (2*n))) / 2 for k in range(1, n1+1) ]\n",
    "        W_c = np.diag(W_entries)\n",
    "        v = W_c @ u\n",
    "        \n",
    "        z1 = cos2(v[:n1], n1)\n",
    "        z2 = cos2(v[n1:], n1)\n",
    "        \n",
    "        B = np.eye(n1)\n",
    "        np.fill_diagonal(B[:-1, 1:], 1)\n",
    "        B[0, 0] = np.sqrt(2)\n",
    "        B_c = np.block([\n",
    "            [np.eye(n1), np.zeros((n1, n1))], \n",
    "            [np.zeros((n1, n1)), B]\n",
    "        ])\n",
    "        w = B_c @ np.concatenate([z1, z2])\n",
    "        \n",
    "        perm = np.arange(n).reshape(2, -1).T.flatten()\n",
    "        y = w[perm]\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.5         1.26383132  0.23549485 ...  0.99641577 -1.18391256\n",
      "   0.62689581]\n",
      " [ 8.5         0.1771992   2.06115683 ... -1.75713705 -2.05152122\n",
      "  -0.99053701]\n",
      " [ 7.25        1.0749807   0.49007978 ... -0.53638835 -2.15286363\n",
      "   0.39362506]\n",
      " ...\n",
      " [ 9.25        4.32802462  1.51024903 ...  0.43115947 -1.18524598\n",
      "   0.36995936]\n",
      " [10.          0.58621259 -1.25590356 ...  1.41169444  1.93868323\n",
      "  -0.37231064]\n",
      " [ 8.75       -0.52259211 -0.66741344 ...  0.07678143  2.07385623\n",
      "   1.58517901]]\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset = np.array([cos2(message, n_padded) for message in dataset])\n",
    "encoded_dataset[np.abs(encoded_dataset) < 1e-10] = 0\n",
    "encoded_dataset = np.round(encoded_dataset, decimals=10)\n",
    "print(encoded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 2 4 4 1 2 2 2 4 3 2 4 1 3 1]\n",
      "[10.5         1.26383132  0.23549485  1.56229549 -1.1480503  -0.97258159\n",
      " -0.11506236  1.16365963  0.5         0.30816899  0.172203   -1.08679973\n",
      " -2.7716386   0.99641577 -1.18391256  0.62689581]\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])\n",
    "print(encoded_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.75 1.   0.5  1.   1.   0.25 0.5  0.5  0.5  1.   0.75 0.5  1.   0.25\n",
      " 0.75 0.25]\n"
     ]
    }
   ],
   "source": [
    "# normalize target data\n",
    "y_normalized = dataset.astype(np.float32) / (q - 1)  # Scale to [0, 1]\n",
    "print(y_normalized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (800, 16) (800, 16)\n",
      "Testing data shape: (200, 16) (200, 16)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    encoded_dataset, y_normalized, test_size=0.2, random_state=42\n",
    ")\n",
    "print(\"Training data shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing data shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "output_dim = y_train.shape[1]\n",
    "print(input_dim)\n",
    "print(output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DCT-3 Structure Imposed Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, units, kernel_initializer='he_normal', bias_initializer='zeros', use_bias=True, **kwargs):\n",
    "    super(FirstLayer, self).__init__(**kwargs)\n",
    "    self.units = units # features/neurons\n",
    "    self.kernel_initializer = kernel_initializer\n",
    "    self.bias_initializer = bias_initializer\n",
    "    self.use_bias = use_bias\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    # input_shape --> (batch_size, input_dim)\n",
    "    n = self.units\n",
    "    n1 = n // 2\n",
    "    \n",
    "    self.d_1 = self.add_weight(name =\"kernel_d1\",\n",
    "                               shape=(n1,),\n",
    "                               initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                              #  regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                               trainable=True)\n",
    "    self.d_2 = self.add_weight(name =\"kernel_d2\",\n",
    "                               shape=(n1,),\n",
    "                               initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                              #  regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                               trainable=True)\n",
    "    if self.use_bias:\n",
    "      self.bias = self.add_weight(name =\"bias\",\n",
    "                                  shape=(self.units,),\n",
    "                                  initializer=tf.keras.initializers.get(self.bias_initializer),\n",
    "                                  # regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                                  trainable=True)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # print(\"Enters first layer\")\n",
    "    P_n = tf.concat([inputs[:, ::2], inputs[:, 1::2]], axis=1) # permutation [x0, x2, x4, ..., xn-2, x1, x3, x5, ..., xn-1]\n",
    "    out1 = P_n[:, :int(P_n.shape[1]/2)] # even indices [x0 x2 x4 ...]\n",
    "    out2 = P_n[:, int(P_n.shape[1]/2):] # odd indices [x1 x3 x5 ...]\n",
    "    \n",
    "    # expected output for rearrange out2 after bidiagonal matrix ---> out2_n = [sqrt(2).x1, x1+x3, x3+x5+ ..., xn-3+xn-1]   \n",
    "    \n",
    "    out3 = tf.add(\n",
    "      tf.multiply(out2, self.d_1), # diagonal\n",
    "      tf.multiply(tf.concat([tf.zeros_like(out2[:, :1]), out2[:, :-1]], axis=1), self.d_2) # super diagonal\n",
    "    )\n",
    "    \n",
    "    out = tf.concat([out1, out3], axis=1) # [x0 x2 x4 ... sqrt(2).x1, x1+x3, x3+x5+ ..., xn-3+xn-1]\n",
    "\n",
    "    if self.use_bias:\n",
    "      out += self.bias\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, kernel_initializer='he_normal', bias_initializer='zeros', use_bias=True, **kwargs):\n",
    "        super(SecondLayer, self).__init__(**kwargs)\n",
    "        self.units = units  # Number of neurons/features\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n = self.units\n",
    "        n1 = n // 2\n",
    "        num_blocks = n1 // 2  # Number of 2×2 blocks needed for c1 and c2 (for 16 we need 4 blocks)\n",
    "\n",
    "        self.d_1 = self.add_weight(name=\"kernel_d1\",\n",
    "                                   shape=(n1 - 2,),\n",
    "                                   initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                #    regularizer=tf.keras.regularizers.l2(1e-5),\n",
    "                                   trainable=True)\n",
    "        self.d_2 = self.add_weight(name=\"kernel_d2\",\n",
    "                                   shape=(n1 - 2,),\n",
    "                                   initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                #    regularizer=tf.keras.regularizers.l2(1e-5),\n",
    "                                   trainable=True)\n",
    "        self.w = self.add_weight(name=\"kernel_w\",\n",
    "                                 shape=(n1 - 2,),\n",
    "                                 initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                #  regularizer=tf.keras.regularizers.l2(1e-5),\n",
    "                                 trainable=True)\n",
    "    \n",
    "        self.C_1 = self.add_weight(name=\"kernel_C_1\",\n",
    "                                   shape=(num_blocks, 2, 2),\n",
    "                                   initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                #    regularizer=tf.keras.regularizers.l2(1e-5),\n",
    "                                   trainable=True)\n",
    "        self.C_2 = self.add_weight(name=\"kernel_C_2\",\n",
    "                                   shape=(num_blocks, 2, 2),\n",
    "                                   initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                #    regularizer=tf.keras.regularizers.l2(1e-5),\n",
    "                                   trainable=True)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name=\"bias\",\n",
    "                                        shape=(self.units,),\n",
    "                                        initializer=tf.keras.initializers.get(self.bias_initializer),\n",
    "                                        # regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                                        trainable=True)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # print(\"Enters second layer\")\n",
    "        \n",
    "        def build_final_matrix(C, n1):\n",
    "            \"\"\"Constructs an n1 x n1 block-diagonal matrix from 2x2 blocks in C.\"\"\"\n",
    "            final_C = tf.eye(n1)  # initialize as an identity matrix\n",
    "            num_blocks = n1 // 2\n",
    "\n",
    "            for i in range(num_blocks):\n",
    "                final_C = tf.tensor_scatter_nd_update(\n",
    "                    final_C,\n",
    "                    indices=[[2*i, 2*i], [2*i, 2*i+1], [2*i+1, 2*i], [2*i+1, 2*i+1]],\n",
    "                    updates=tf.reshape(C[i], (-1,))\n",
    "                )\n",
    "\n",
    "            return final_C\n",
    "\n",
    "        def recursiveDCTIII(inputVector, d_1, d_2, w, C, level):\n",
    "            n = inputVector.shape[1]\n",
    "\n",
    "            if n == 2:\n",
    "                out = tf.matmul(inputVector, C[level])  # select correct 2×2 matrix\n",
    "                return out\n",
    "            else:\n",
    "                P_n = tf.concat([inputVector[:, ::2], inputVector[:, 1::2]], axis=1)  # Permutation\n",
    "                out1 = P_n[:, :n // 2]\n",
    "                out2 = P_n[:, n // 2:]\n",
    "\n",
    "                # Apply diagonal and superdiagonal scaling\n",
    "                d1 = tf.multiply(out2, tf.reshape(d_1[:(n // 2)], (1, -1)))\n",
    "                d2 = tf.concat([tf.zeros_like(out2[:, :1]), tf.multiply(out2[:, :-1], tf.reshape(d_2[:(n // 2) - 1], (1, -1)))], axis=1)\n",
    "                out2_bn = tf.add(d1, d2)\n",
    "\n",
    "                # Recursively apply DCT-III\n",
    "                out1_n = recursiveDCTIII(out1, d_1[n // 2:], d_2[n // 2:], w[n // 2:], C, level + 1)\n",
    "                out2_n = recursiveDCTIII(out2_bn, d_1[n // 2:], d_2[n // 2:], w[n // 2:], C, level + 1)\n",
    "\n",
    "                out2_wn = tf.multiply(out2_n, tf.reshape(w[:n // 2], (1, -1)))\n",
    "\n",
    "                out = (1 / tf.sqrt(tf.constant(2.0, dtype=tf.float32))) * tf.concat(\n",
    "                    [(out1_n + out2_wn), tf.reverse((out1_n - out2_wn), axis=[1])], axis=1)\n",
    "\n",
    "                return out\n",
    "\n",
    "        input1 = inputs[:, :self.units // 2]\n",
    "        input2 = inputs[:, self.units // 2:]\n",
    "\n",
    "        z1 = recursiveDCTIII(input1, self.d_1, self.d_2, self.w, self.C_1, level=0)\n",
    "        z2 = recursiveDCTIII(input2, self.d_1, self.d_2, self.w, self.C_2, level=0)\n",
    "        \n",
    "        final_C1 = build_final_matrix(self.C_1, self.units // 2)\n",
    "        final_C2 = build_final_matrix(self.C_2, self.units // 2)\n",
    "\n",
    "        z1 = tf.matmul(z1, final_C1)\n",
    "        z2 = tf.matmul(z2, final_C2)\n",
    "\n",
    "        out = tf.concat([z1, z2], axis=1)\n",
    "\n",
    "        if self.use_bias:\n",
    "            out += self.bias\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThirdLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, kernel_initializer='he_normal', bias_initializer='zeros', use_bias=True, **kwargs):\n",
    "        super(ThirdLayer, self).__init__(**kwargs)\n",
    "        self.units = units # features/neurons\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n = self.units\n",
    "        n1 = n // 2\n",
    "        \n",
    "        self.w = self.add_weight(name =\"kernel_w\",\n",
    "                                      shape=(n1,),\n",
    "                                      initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                    #   regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                                      trainable=True)\n",
    "        # self.h = self.add_weight(name =\"kernel_h\",\n",
    "        #                               shape = (2,2),\n",
    "        #                               initializer='glorot_normal',\n",
    "        #                               trainable=True)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name =\"bias\",\n",
    "                                       shape=(self.units,),\n",
    "                                       initializer=tf.keras.initializers.get(self.bias_initializer),\n",
    "                                    #    regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                                       trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # print(\"Enters third layer\")\n",
    "        \n",
    "        out1 = inputs[:, :int(inputs.shape[1] / 2)]\n",
    "        out2 = inputs[:, int(inputs.shape[1] / 2):]\n",
    "\n",
    "        # out1 - stays as is\n",
    "        out3 = tf.multiply(out2, self.w)\n",
    "        \n",
    "        # TODO: make H trainable instead of hardcoded\n",
    "        \n",
    "        out = (1 / tf.sqrt(tf.constant(2.0, dtype=tf.float32))) * tf.concat([(out1 + out3), tf.reverse((out1 - out3), axis=[1])], axis=1)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            out += self.bias\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, kernel_initializer='he_normal', bias_initializer='zeros', use_bias=True, **kwargs):\n",
    "        super(CustomLayer, self).__init__(**kwargs)\n",
    "        self.units = units # features/neurons\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n = self.units\n",
    "        \n",
    "        self.m = self.add_weight(name =\"kernel_m\",\n",
    "                                      shape=(n,),\n",
    "                                      initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                    #   regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                                      trainable=True)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name =\"bias\",\n",
    "                                       shape=(self.units,),\n",
    "                                       initializer=tf.keras.initializers.get(self.bias_initializer),\n",
    "                                    #    regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                                       trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # print(\"Enters custom layer\")\n",
    "\n",
    "        out = tf.multiply(inputs, self.m)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            out += self.bias\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only difference between the gen matrix encoded and dct encoded is the linear mapping (m weight matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, kernel_initializer='ones', bias_initializer='zeros', use_bias=True, **kwargs):\n",
    "        super(LinearLayer, self).__init__(**kwargs)\n",
    "        self.units = units # features/neurons\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n = self.units\n",
    "        \n",
    "        self.m = self.add_weight(name =\"kernel_m\",\n",
    "                                      shape=(n,),\n",
    "                                      initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                      trainable=True)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name =\"bias\",\n",
    "                                       shape=(n,),\n",
    "                                       initializer=tf.keras.initializers.get(self.bias_initializer),\n",
    "                                       trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # print(\"Enters last linear layer\")\n",
    "        # print(\"linear layer input shape:\", inputs.shape)\n",
    "\n",
    "        out = tf.multiply(inputs, self.m)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            out += self.bias\n",
    "            \n",
    "        # print(\"linear layer output shape:\", out.shape)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer1 (FirstLayer)         (None, 16)                32        \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 16)                0         \n",
      "                                                                 \n",
      " support_layer_1 (CustomLay  (None, 16)                32        \n",
      " er)                                                             \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 16)                0         \n",
      "                                                                 \n",
      " layer2 (SecondLayer)        (None, 16)                66        \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 16)                0         \n",
      "                                                                 \n",
      " support_layer_2 (CustomLay  (None, 16)                32        \n",
      " er)                                                             \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 16)                0         \n",
      "                                                                 \n",
      " layer3 (ThirdLayer)         (None, 16)                24        \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 16)                0         \n",
      "                                                                 \n",
      " output_layer (LinearLayer)  (None, 16)                32        \n",
      "                                                                 \n",
      " activation (Activation)     (None, 16)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 218 (872.00 Byte)\n",
      "Trainable params: 218 (872.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input, Concatenate, LeakyReLU, Activation\n",
    "\n",
    "def cosine_loss(y_true, y_pred):\n",
    "    y_true_norm = tf.nn.l2_normalize(y_true, axis=-1)\n",
    "    y_pred_norm = tf.nn.l2_normalize(y_pred, axis=-1)\n",
    "\n",
    "    return 1 - tf.reduce_mean(tf.reduce_sum(y_true_norm * y_pred_norm, axis=-1))  # 1 - Cosine similarity\n",
    "\n",
    "\n",
    "def hybrid_loss(y_true, y_pred):\n",
    "    mse_part = tf.reduce_mean(tf.square(y_true - y_pred))  # MSE\n",
    "    cos_part = cosine_loss(y_true, y_pred)\n",
    "    return 0.5 * mse_part + 0.5 * cos_part\n",
    "\n",
    "\n",
    "def structured_NN(input_dim, output_dim):\n",
    "    model = tf.keras.Sequential([\n",
    "        Input(shape=(input_dim,)),\n",
    "        FirstLayer(units=input_dim, kernel_initializer='he_normal', bias_initializer='zeros', name=\"layer1\"),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        CustomLayer(units=input_dim, kernel_initializer='he_normal', bias_initializer='zeros', name=\"support_layer_1\"),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        SecondLayer(units=input_dim, kernel_initializer='he_normal', bias_initializer='zeros', name=\"layer2\"),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        CustomLayer(units=input_dim, kernel_initializer='he_normal', bias_initializer='zeros', name=\"support_layer_2\"),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        ThirdLayer(units=input_dim, kernel_initializer='he_normal', bias_initializer='zeros', name=\"layer3\"),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        LinearLayer(units=output_dim, kernel_initializer='ones', bias_initializer='zeros', name=\"output_layer\"),\n",
    "        Activation('sigmoid') #sigmoid #linear #softmax \n",
    "        ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=hybrid_loss, #Huber(delta=0.5), #mean_squared_error',\n",
    "        metrics=['mse', 'mae']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = y_train.shape[1]\n",
    "model = structured_NN(input_dim, output_dim)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1 (None, 16)\n",
      "leaky_re_lu (None, 16)\n",
      "support_layer_1 (None, 16)\n",
      "leaky_re_lu_1 (None, 16)\n",
      "layer2 (None, 16)\n",
      "leaky_re_lu_2 (None, 16)\n",
      "support_layer_2 (None, 16)\n",
      "leaky_re_lu_3 (None, 16)\n",
      "layer3 (None, 16)\n",
      "leaky_re_lu_4 (None, 16)\n",
      "output_layer (None, 16)\n",
      "activation (None, 16)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name, layer.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "adjust_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "25/25 [==============================] - 6s 55ms/step - loss: 0.1543 - mse: 0.1265 - mae: 0.3038 - val_loss: 0.1556 - val_mse: 0.1254 - val_mae: 0.3014 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.1542 - mse: 0.1265 - mae: 0.3040 - val_loss: 0.1555 - val_mse: 0.1254 - val_mae: 0.3015 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.1541 - mse: 0.1264 - mae: 0.3041 - val_loss: 0.1555 - val_mse: 0.1254 - val_mae: 0.3017 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.1540 - mse: 0.1263 - mae: 0.3042 - val_loss: 0.1554 - val_mse: 0.1253 - val_mae: 0.3018 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.1538 - mse: 0.1261 - mae: 0.3042 - val_loss: 0.1552 - val_mse: 0.1251 - val_mae: 0.3019 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.1535 - mse: 0.1259 - mae: 0.3042 - val_loss: 0.1549 - val_mse: 0.1249 - val_mae: 0.3018 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.1531 - mse: 0.1256 - mae: 0.3041 - val_loss: 0.1544 - val_mse: 0.1246 - val_mae: 0.3017 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.1525 - mse: 0.1252 - mae: 0.3038 - val_loss: 0.1539 - val_mse: 0.1242 - val_mae: 0.3015 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1518 - mse: 0.1246 - mae: 0.3034 - val_loss: 0.1531 - val_mse: 0.1237 - val_mae: 0.3012 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1508 - mse: 0.1239 - mae: 0.3029 - val_loss: 0.1520 - val_mse: 0.1230 - val_mae: 0.3008 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1494 - mse: 0.1230 - mae: 0.3019 - val_loss: 0.1507 - val_mse: 0.1221 - val_mae: 0.3000 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1479 - mse: 0.1219 - mae: 0.3008 - val_loss: 0.1491 - val_mse: 0.1211 - val_mae: 0.2988 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.1464 - mse: 0.1208 - mae: 0.2992 - val_loss: 0.1477 - val_mse: 0.1202 - val_mae: 0.2977 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.1447 - mse: 0.1197 - mae: 0.2978 - val_loss: 0.1462 - val_mse: 0.1192 - val_mae: 0.2966 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1430 - mse: 0.1185 - mae: 0.2963 - val_loss: 0.1442 - val_mse: 0.1179 - val_mae: 0.2953 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1408 - mse: 0.1170 - mae: 0.2947 - val_loss: 0.1422 - val_mse: 0.1167 - val_mae: 0.2940 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1385 - mse: 0.1155 - mae: 0.2926 - val_loss: 0.1401 - val_mse: 0.1154 - val_mae: 0.2920 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1364 - mse: 0.1141 - mae: 0.2903 - val_loss: 0.1382 - val_mse: 0.1143 - val_mae: 0.2899 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1343 - mse: 0.1126 - mae: 0.2880 - val_loss: 0.1365 - val_mse: 0.1132 - val_mae: 0.2879 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1324 - mse: 0.1114 - mae: 0.2860 - val_loss: 0.1350 - val_mse: 0.1122 - val_mae: 0.2862 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1307 - mse: 0.1102 - mae: 0.2841 - val_loss: 0.1336 - val_mse: 0.1112 - val_mae: 0.2845 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1292 - mse: 0.1091 - mae: 0.2825 - val_loss: 0.1323 - val_mse: 0.1103 - val_mae: 0.2829 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.1278 - mse: 0.1079 - mae: 0.2808 - val_loss: 0.1311 - val_mse: 0.1092 - val_mae: 0.2813 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1263 - mse: 0.1067 - mae: 0.2791 - val_loss: 0.1297 - val_mse: 0.1081 - val_mae: 0.2797 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.1248 - mse: 0.1055 - mae: 0.2774 - val_loss: 0.1282 - val_mse: 0.1070 - val_mae: 0.2780 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1233 - mse: 0.1044 - mae: 0.2756 - val_loss: 0.1267 - val_mse: 0.1058 - val_mae: 0.2763 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1219 - mse: 0.1031 - mae: 0.2739 - val_loss: 0.1253 - val_mse: 0.1046 - val_mae: 0.2747 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1205 - mse: 0.1020 - mae: 0.2723 - val_loss: 0.1240 - val_mse: 0.1036 - val_mae: 0.2732 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1192 - mse: 0.1010 - mae: 0.2708 - val_loss: 0.1229 - val_mse: 0.1026 - val_mae: 0.2718 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1179 - mse: 0.0998 - mae: 0.2692 - val_loss: 0.1216 - val_mse: 0.1016 - val_mae: 0.2704 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1167 - mse: 0.0989 - mae: 0.2677 - val_loss: 0.1205 - val_mse: 0.1007 - val_mae: 0.2691 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1155 - mse: 0.0979 - mae: 0.2663 - val_loss: 0.1194 - val_mse: 0.0998 - val_mae: 0.2678 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1145 - mse: 0.0971 - mae: 0.2649 - val_loss: 0.1184 - val_mse: 0.0990 - val_mae: 0.2667 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1134 - mse: 0.0961 - mae: 0.2636 - val_loss: 0.1175 - val_mse: 0.0981 - val_mae: 0.2655 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.1125 - mse: 0.0954 - mae: 0.2623 - val_loss: 0.1164 - val_mse: 0.0972 - val_mae: 0.2642 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1113 - mse: 0.0944 - mae: 0.2608 - val_loss: 0.1152 - val_mse: 0.0963 - val_mae: 0.2628 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.1099 - mse: 0.0933 - mae: 0.2591 - val_loss: 0.1139 - val_mse: 0.0954 - val_mae: 0.2611 - lr: 0.0010\n",
      "Epoch 38/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.1087 - mse: 0.0924 - mae: 0.2576 - val_loss: 0.1128 - val_mse: 0.0944 - val_mae: 0.2596 - lr: 0.0010\n",
      "Epoch 39/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.1077 - mse: 0.0915 - mae: 0.2562 - val_loss: 0.1117 - val_mse: 0.0936 - val_mae: 0.2583 - lr: 0.0010\n",
      "Epoch 40/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.1068 - mse: 0.0908 - mae: 0.2549 - val_loss: 0.1109 - val_mse: 0.0929 - val_mae: 0.2572 - lr: 0.0010\n",
      "Epoch 41/500\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 0.1059 - mse: 0.0901 - mae: 0.2538 - val_loss: 0.1100 - val_mse: 0.0921 - val_mae: 0.2561 - lr: 0.0010\n",
      "Epoch 42/500\n",
      "25/25 [==============================] - 0s 12ms/step - loss: 0.1051 - mse: 0.0893 - mae: 0.2526 - val_loss: 0.1092 - val_mse: 0.0914 - val_mae: 0.2551 - lr: 0.0010\n",
      "Epoch 43/500\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 0.1042 - mse: 0.0886 - mae: 0.2514 - val_loss: 0.1085 - val_mse: 0.0907 - val_mae: 0.2540 - lr: 0.0010\n",
      "Epoch 44/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.1035 - mse: 0.0879 - mae: 0.2504 - val_loss: 0.1076 - val_mse: 0.0900 - val_mae: 0.2528 - lr: 0.0010\n",
      "Epoch 45/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.1027 - mse: 0.0873 - mae: 0.2493 - val_loss: 0.1069 - val_mse: 0.0894 - val_mae: 0.2519 - lr: 0.0010\n",
      "Epoch 46/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.1020 - mse: 0.0867 - mae: 0.2483 - val_loss: 0.1063 - val_mse: 0.0888 - val_mae: 0.2509 - lr: 0.0010\n",
      "Epoch 47/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.1014 - mse: 0.0861 - mae: 0.2474 - val_loss: 0.1056 - val_mse: 0.0883 - val_mae: 0.2500 - lr: 0.0010\n",
      "Epoch 48/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.1007 - mse: 0.0855 - mae: 0.2464 - val_loss: 0.1049 - val_mse: 0.0876 - val_mae: 0.2490 - lr: 0.0010\n",
      "Epoch 49/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.1001 - mse: 0.0849 - mae: 0.2454 - val_loss: 0.1042 - val_mse: 0.0869 - val_mae: 0.2480 - lr: 0.0010\n",
      "Epoch 50/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0994 - mse: 0.0843 - mae: 0.2445 - val_loss: 0.1035 - val_mse: 0.0863 - val_mae: 0.2470 - lr: 0.0010\n",
      "Epoch 51/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0989 - mse: 0.0838 - mae: 0.2436 - val_loss: 0.1028 - val_mse: 0.0858 - val_mae: 0.2460 - lr: 0.0010\n",
      "Epoch 52/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0982 - mse: 0.0833 - mae: 0.2427 - val_loss: 0.1025 - val_mse: 0.0854 - val_mae: 0.2454 - lr: 0.0010\n",
      "Epoch 53/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0976 - mse: 0.0827 - mae: 0.2417 - val_loss: 0.1018 - val_mse: 0.0848 - val_mae: 0.2444 - lr: 0.0010\n",
      "Epoch 54/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0971 - mse: 0.0822 - mae: 0.2408 - val_loss: 0.1011 - val_mse: 0.0843 - val_mae: 0.2434 - lr: 0.0010\n",
      "Epoch 55/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0964 - mse: 0.0817 - mae: 0.2399 - val_loss: 0.1004 - val_mse: 0.0836 - val_mae: 0.2423 - lr: 0.0010\n",
      "Epoch 56/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0958 - mse: 0.0811 - mae: 0.2390 - val_loss: 0.0999 - val_mse: 0.0831 - val_mae: 0.2416 - lr: 0.0010\n",
      "Epoch 57/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0953 - mse: 0.0806 - mae: 0.2381 - val_loss: 0.0993 - val_mse: 0.0826 - val_mae: 0.2406 - lr: 0.0010\n",
      "Epoch 58/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0947 - mse: 0.0801 - mae: 0.2372 - val_loss: 0.0988 - val_mse: 0.0822 - val_mae: 0.2399 - lr: 0.0010\n",
      "Epoch 59/500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0942 - mse: 0.0796 - mae: 0.2364 - val_loss: 0.0981 - val_mse: 0.0816 - val_mae: 0.2390 - lr: 0.0010\n",
      "Epoch 60/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.0936 - mse: 0.0792 - mae: 0.2355 - val_loss: 0.0977 - val_mse: 0.0812 - val_mae: 0.2382 - lr: 0.0010\n",
      "Epoch 61/500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0931 - mse: 0.0787 - mae: 0.2348 - val_loss: 0.0972 - val_mse: 0.0808 - val_mae: 0.2375 - lr: 0.0010\n",
      "Epoch 62/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0926 - mse: 0.0783 - mae: 0.2338 - val_loss: 0.0967 - val_mse: 0.0803 - val_mae: 0.2367 - lr: 0.0010\n",
      "Epoch 63/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0921 - mse: 0.0778 - mae: 0.2331 - val_loss: 0.0962 - val_mse: 0.0799 - val_mae: 0.2360 - lr: 0.0010\n",
      "Epoch 64/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0916 - mse: 0.0774 - mae: 0.2323 - val_loss: 0.0957 - val_mse: 0.0795 - val_mae: 0.2353 - lr: 0.0010\n",
      "Epoch 65/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0911 - mse: 0.0769 - mae: 0.2315 - val_loss: 0.0954 - val_mse: 0.0791 - val_mae: 0.2346 - lr: 0.0010\n",
      "Epoch 66/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0905 - mse: 0.0765 - mae: 0.2306 - val_loss: 0.0949 - val_mse: 0.0787 - val_mae: 0.2339 - lr: 0.0010\n",
      "Epoch 67/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0901 - mse: 0.0761 - mae: 0.2299 - val_loss: 0.0944 - val_mse: 0.0784 - val_mae: 0.2333 - lr: 0.0010\n",
      "Epoch 68/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.0896 - mse: 0.0757 - mae: 0.2291 - val_loss: 0.0940 - val_mse: 0.0779 - val_mae: 0.2326 - lr: 0.0010\n",
      "Epoch 69/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0892 - mse: 0.0753 - mae: 0.2284 - val_loss: 0.0936 - val_mse: 0.0776 - val_mae: 0.2320 - lr: 0.0010\n",
      "Epoch 70/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0888 - mse: 0.0750 - mae: 0.2277 - val_loss: 0.0931 - val_mse: 0.0772 - val_mae: 0.2312 - lr: 0.0010\n",
      "Epoch 71/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.0884 - mse: 0.0746 - mae: 0.2270 - val_loss: 0.0928 - val_mse: 0.0769 - val_mae: 0.2307 - lr: 0.0010\n",
      "Epoch 72/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0880 - mse: 0.0743 - mae: 0.2264 - val_loss: 0.0925 - val_mse: 0.0766 - val_mae: 0.2301 - lr: 0.0010\n",
      "Epoch 73/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.0876 - mse: 0.0740 - mae: 0.2257 - val_loss: 0.0920 - val_mse: 0.0763 - val_mae: 0.2295 - lr: 0.0010\n",
      "Epoch 74/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.0872 - mse: 0.0736 - mae: 0.2250 - val_loss: 0.0917 - val_mse: 0.0759 - val_mae: 0.2289 - lr: 0.0010\n",
      "Epoch 75/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0868 - mse: 0.0733 - mae: 0.2244 - val_loss: 0.0912 - val_mse: 0.0756 - val_mae: 0.2282 - lr: 0.0010\n",
      "Epoch 76/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.0864 - mse: 0.0729 - mae: 0.2237 - val_loss: 0.0907 - val_mse: 0.0752 - val_mae: 0.2276 - lr: 0.0010\n",
      "Epoch 77/500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0860 - mse: 0.0726 - mae: 0.2230 - val_loss: 0.0903 - val_mse: 0.0748 - val_mae: 0.2268 - lr: 0.0010\n",
      "Epoch 78/500\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 0.0856 - mse: 0.0723 - mae: 0.2223 - val_loss: 0.0899 - val_mse: 0.0745 - val_mae: 0.2261 - lr: 0.0010\n",
      "Epoch 79/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0852 - mse: 0.0719 - mae: 0.2217 - val_loss: 0.0894 - val_mse: 0.0741 - val_mae: 0.2254 - lr: 0.0010\n",
      "Epoch 80/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0848 - mse: 0.0715 - mae: 0.2209 - val_loss: 0.0890 - val_mse: 0.0737 - val_mae: 0.2248 - lr: 0.0010\n",
      "Epoch 81/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.0843 - mse: 0.0711 - mae: 0.2202 - val_loss: 0.0885 - val_mse: 0.0733 - val_mae: 0.2241 - lr: 0.0010\n",
      "Epoch 82/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0839 - mse: 0.0707 - mae: 0.2196 - val_loss: 0.0880 - val_mse: 0.0729 - val_mae: 0.2236 - lr: 0.0010\n",
      "Epoch 83/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0834 - mse: 0.0703 - mae: 0.2188 - val_loss: 0.0875 - val_mse: 0.0724 - val_mae: 0.2228 - lr: 0.0010\n",
      "Epoch 84/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0829 - mse: 0.0699 - mae: 0.2180 - val_loss: 0.0871 - val_mse: 0.0721 - val_mae: 0.2222 - lr: 0.0010\n",
      "Epoch 85/500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0823 - mse: 0.0695 - mae: 0.2173 - val_loss: 0.0865 - val_mse: 0.0716 - val_mae: 0.2213 - lr: 0.0010\n",
      "Epoch 86/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.0818 - mse: 0.0690 - mae: 0.2165 - val_loss: 0.0860 - val_mse: 0.0711 - val_mae: 0.2205 - lr: 0.0010\n",
      "Epoch 87/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0812 - mse: 0.0685 - mae: 0.2155 - val_loss: 0.0853 - val_mse: 0.0705 - val_mae: 0.2195 - lr: 0.0010\n",
      "Epoch 88/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0806 - mse: 0.0680 - mae: 0.2147 - val_loss: 0.0847 - val_mse: 0.0701 - val_mae: 0.2187 - lr: 0.0010\n",
      "Epoch 89/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0800 - mse: 0.0675 - mae: 0.2138 - val_loss: 0.0841 - val_mse: 0.0696 - val_mae: 0.2178 - lr: 0.0010\n",
      "Epoch 90/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0794 - mse: 0.0670 - mae: 0.2129 - val_loss: 0.0834 - val_mse: 0.0690 - val_mae: 0.2168 - lr: 0.0010\n",
      "Epoch 91/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0787 - mse: 0.0664 - mae: 0.2120 - val_loss: 0.0828 - val_mse: 0.0685 - val_mae: 0.2159 - lr: 0.0010\n",
      "Epoch 92/500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0781 - mse: 0.0659 - mae: 0.2110 - val_loss: 0.0820 - val_mse: 0.0679 - val_mae: 0.2149 - lr: 0.0010\n",
      "Epoch 93/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0774 - mse: 0.0653 - mae: 0.2100 - val_loss: 0.0814 - val_mse: 0.0674 - val_mae: 0.2140 - lr: 0.0010\n",
      "Epoch 94/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0768 - mse: 0.0648 - mae: 0.2091 - val_loss: 0.0808 - val_mse: 0.0669 - val_mae: 0.2132 - lr: 0.0010\n",
      "Epoch 95/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0762 - mse: 0.0643 - mae: 0.2081 - val_loss: 0.0802 - val_mse: 0.0664 - val_mae: 0.2124 - lr: 0.0010\n",
      "Epoch 96/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0756 - mse: 0.0638 - mae: 0.2073 - val_loss: 0.0796 - val_mse: 0.0659 - val_mae: 0.2115 - lr: 0.0010\n",
      "Epoch 97/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0750 - mse: 0.0634 - mae: 0.2064 - val_loss: 0.0790 - val_mse: 0.0655 - val_mae: 0.2107 - lr: 0.0010\n",
      "Epoch 98/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0745 - mse: 0.0629 - mae: 0.2056 - val_loss: 0.0785 - val_mse: 0.0650 - val_mae: 0.2101 - lr: 0.0010\n",
      "Epoch 99/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0740 - mse: 0.0625 - mae: 0.2048 - val_loss: 0.0778 - val_mse: 0.0645 - val_mae: 0.2091 - lr: 0.0010\n",
      "Epoch 100/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0735 - mse: 0.0621 - mae: 0.2041 - val_loss: 0.0774 - val_mse: 0.0641 - val_mae: 0.2086 - lr: 0.0010\n",
      "Epoch 101/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0730 - mse: 0.0617 - mae: 0.2034 - val_loss: 0.0769 - val_mse: 0.0638 - val_mae: 0.2078 - lr: 0.0010\n",
      "Epoch 102/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0726 - mse: 0.0614 - mae: 0.2027 - val_loss: 0.0764 - val_mse: 0.0633 - val_mae: 0.2071 - lr: 0.0010\n",
      "Epoch 103/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0722 - mse: 0.0610 - mae: 0.2021 - val_loss: 0.0759 - val_mse: 0.0630 - val_mae: 0.2064 - lr: 0.0010\n",
      "Epoch 104/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0718 - mse: 0.0607 - mae: 0.2014 - val_loss: 0.0755 - val_mse: 0.0626 - val_mae: 0.2058 - lr: 0.0010\n",
      "Epoch 105/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0714 - mse: 0.0604 - mae: 0.2008 - val_loss: 0.0751 - val_mse: 0.0623 - val_mae: 0.2051 - lr: 0.0010\n",
      "Epoch 106/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0711 - mse: 0.0601 - mae: 0.2003 - val_loss: 0.0747 - val_mse: 0.0620 - val_mae: 0.2047 - lr: 0.0010\n",
      "Epoch 107/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0707 - mse: 0.0598 - mae: 0.1998 - val_loss: 0.0744 - val_mse: 0.0617 - val_mae: 0.2042 - lr: 0.0010\n",
      "Epoch 108/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0703 - mse: 0.0595 - mae: 0.1991 - val_loss: 0.0739 - val_mse: 0.0613 - val_mae: 0.2035 - lr: 0.0010\n",
      "Epoch 109/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0700 - mse: 0.0592 - mae: 0.1986 - val_loss: 0.0736 - val_mse: 0.0610 - val_mae: 0.2029 - lr: 0.0010\n",
      "Epoch 110/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0696 - mse: 0.0589 - mae: 0.1980 - val_loss: 0.0731 - val_mse: 0.0607 - val_mae: 0.2022 - lr: 0.0010\n",
      "Epoch 111/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0693 - mse: 0.0586 - mae: 0.1974 - val_loss: 0.0728 - val_mse: 0.0604 - val_mae: 0.2018 - lr: 0.0010\n",
      "Epoch 112/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0689 - mse: 0.0583 - mae: 0.1969 - val_loss: 0.0725 - val_mse: 0.0602 - val_mae: 0.2013 - lr: 0.0010\n",
      "Epoch 113/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0686 - mse: 0.0580 - mae: 0.1963 - val_loss: 0.0720 - val_mse: 0.0598 - val_mae: 0.2005 - lr: 0.0010\n",
      "Epoch 114/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0683 - mse: 0.0577 - mae: 0.1958 - val_loss: 0.0717 - val_mse: 0.0595 - val_mae: 0.2000 - lr: 0.0010\n",
      "Epoch 115/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0680 - mse: 0.0575 - mae: 0.1953 - val_loss: 0.0714 - val_mse: 0.0593 - val_mae: 0.1994 - lr: 0.0010\n",
      "Epoch 116/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0677 - mse: 0.0572 - mae: 0.1948 - val_loss: 0.0711 - val_mse: 0.0591 - val_mae: 0.1989 - lr: 0.0010\n",
      "Epoch 117/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0674 - mse: 0.0570 - mae: 0.1943 - val_loss: 0.0708 - val_mse: 0.0588 - val_mae: 0.1985 - lr: 0.0010\n",
      "Epoch 118/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0671 - mse: 0.0567 - mae: 0.1938 - val_loss: 0.0705 - val_mse: 0.0585 - val_mae: 0.1978 - lr: 0.0010\n",
      "Epoch 119/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0668 - mse: 0.0565 - mae: 0.1933 - val_loss: 0.0701 - val_mse: 0.0582 - val_mae: 0.1972 - lr: 0.0010\n",
      "Epoch 120/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0665 - mse: 0.0562 - mae: 0.1929 - val_loss: 0.0699 - val_mse: 0.0581 - val_mae: 0.1968 - lr: 0.0010\n",
      "Epoch 121/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0662 - mse: 0.0560 - mae: 0.1924 - val_loss: 0.0695 - val_mse: 0.0577 - val_mae: 0.1961 - lr: 0.0010\n",
      "Epoch 122/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0659 - mse: 0.0557 - mae: 0.1920 - val_loss: 0.0692 - val_mse: 0.0575 - val_mae: 0.1956 - lr: 0.0010\n",
      "Epoch 123/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0657 - mse: 0.0555 - mae: 0.1916 - val_loss: 0.0688 - val_mse: 0.0572 - val_mae: 0.1950 - lr: 0.0010\n",
      "Epoch 124/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0654 - mse: 0.0553 - mae: 0.1910 - val_loss: 0.0685 - val_mse: 0.0569 - val_mae: 0.1944 - lr: 0.0010\n",
      "Epoch 125/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0651 - mse: 0.0551 - mae: 0.1907 - val_loss: 0.0683 - val_mse: 0.0567 - val_mae: 0.1941 - lr: 0.0010\n",
      "Epoch 126/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0649 - mse: 0.0548 - mae: 0.1902 - val_loss: 0.0679 - val_mse: 0.0564 - val_mae: 0.1935 - lr: 0.0010\n",
      "Epoch 127/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0646 - mse: 0.0546 - mae: 0.1897 - val_loss: 0.0675 - val_mse: 0.0561 - val_mae: 0.1930 - lr: 0.0010\n",
      "Epoch 128/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0643 - mse: 0.0543 - mae: 0.1892 - val_loss: 0.0672 - val_mse: 0.0559 - val_mae: 0.1926 - lr: 0.0010\n",
      "Epoch 129/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0640 - mse: 0.0541 - mae: 0.1888 - val_loss: 0.0669 - val_mse: 0.0556 - val_mae: 0.1920 - lr: 0.0010\n",
      "Epoch 130/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0637 - mse: 0.0538 - mae: 0.1883 - val_loss: 0.0665 - val_mse: 0.0553 - val_mae: 0.1914 - lr: 0.0010\n",
      "Epoch 131/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0634 - mse: 0.0536 - mae: 0.1878 - val_loss: 0.0662 - val_mse: 0.0550 - val_mae: 0.1908 - lr: 0.0010\n",
      "Epoch 132/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0632 - mse: 0.0534 - mae: 0.1874 - val_loss: 0.0659 - val_mse: 0.0548 - val_mae: 0.1904 - lr: 0.0010\n",
      "Epoch 133/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0628 - mse: 0.0531 - mae: 0.1868 - val_loss: 0.0655 - val_mse: 0.0545 - val_mae: 0.1898 - lr: 0.0010\n",
      "Epoch 134/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0625 - mse: 0.0528 - mae: 0.1865 - val_loss: 0.0653 - val_mse: 0.0543 - val_mae: 0.1894 - lr: 0.0010\n",
      "Epoch 135/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0622 - mse: 0.0525 - mae: 0.1859 - val_loss: 0.0649 - val_mse: 0.0539 - val_mae: 0.1887 - lr: 0.0010\n",
      "Epoch 136/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0620 - mse: 0.0524 - mae: 0.1855 - val_loss: 0.0646 - val_mse: 0.0537 - val_mae: 0.1880 - lr: 0.0010\n",
      "Epoch 137/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0617 - mse: 0.0521 - mae: 0.1849 - val_loss: 0.0643 - val_mse: 0.0534 - val_mae: 0.1877 - lr: 0.0010\n",
      "Epoch 138/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0614 - mse: 0.0519 - mae: 0.1846 - val_loss: 0.0641 - val_mse: 0.0532 - val_mae: 0.1871 - lr: 0.0010\n",
      "Epoch 139/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0612 - mse: 0.0516 - mae: 0.1841 - val_loss: 0.0638 - val_mse: 0.0530 - val_mae: 0.1867 - lr: 0.0010\n",
      "Epoch 140/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0609 - mse: 0.0515 - mae: 0.1837 - val_loss: 0.0633 - val_mse: 0.0526 - val_mae: 0.1859 - lr: 0.0010\n",
      "Epoch 141/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0606 - mse: 0.0512 - mae: 0.1832 - val_loss: 0.0630 - val_mse: 0.0524 - val_mae: 0.1855 - lr: 0.0010\n",
      "Epoch 142/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0604 - mse: 0.0510 - mae: 0.1828 - val_loss: 0.0626 - val_mse: 0.0521 - val_mae: 0.1849 - lr: 0.0010\n",
      "Epoch 143/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0602 - mse: 0.0508 - mae: 0.1824 - val_loss: 0.0624 - val_mse: 0.0519 - val_mae: 0.1844 - lr: 0.0010\n",
      "Epoch 144/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0600 - mse: 0.0506 - mae: 0.1820 - val_loss: 0.0620 - val_mse: 0.0515 - val_mae: 0.1837 - lr: 0.0010\n",
      "Epoch 145/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0597 - mse: 0.0504 - mae: 0.1817 - val_loss: 0.0617 - val_mse: 0.0513 - val_mae: 0.1834 - lr: 0.0010\n",
      "Epoch 146/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0595 - mse: 0.0503 - mae: 0.1812 - val_loss: 0.0615 - val_mse: 0.0511 - val_mae: 0.1828 - lr: 0.0010\n",
      "Epoch 147/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0592 - mse: 0.0500 - mae: 0.1808 - val_loss: 0.0611 - val_mse: 0.0508 - val_mae: 0.1822 - lr: 0.0010\n",
      "Epoch 148/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0590 - mse: 0.0498 - mae: 0.1804 - val_loss: 0.0608 - val_mse: 0.0506 - val_mae: 0.1819 - lr: 0.0010\n",
      "Epoch 149/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0588 - mse: 0.0496 - mae: 0.1800 - val_loss: 0.0606 - val_mse: 0.0504 - val_mae: 0.1815 - lr: 0.0010\n",
      "Epoch 150/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0585 - mse: 0.0494 - mae: 0.1796 - val_loss: 0.0602 - val_mse: 0.0501 - val_mae: 0.1808 - lr: 0.0010\n",
      "Epoch 151/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0582 - mse: 0.0492 - mae: 0.1791 - val_loss: 0.0597 - val_mse: 0.0497 - val_mae: 0.1801 - lr: 0.0010\n",
      "Epoch 152/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0580 - mse: 0.0490 - mae: 0.1787 - val_loss: 0.0595 - val_mse: 0.0495 - val_mae: 0.1797 - lr: 0.0010\n",
      "Epoch 153/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0578 - mse: 0.0488 - mae: 0.1783 - val_loss: 0.0592 - val_mse: 0.0492 - val_mae: 0.1791 - lr: 0.0010\n",
      "Epoch 154/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0574 - mse: 0.0485 - mae: 0.1777 - val_loss: 0.0588 - val_mse: 0.0489 - val_mae: 0.1787 - lr: 0.0010\n",
      "Epoch 155/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0572 - mse: 0.0483 - mae: 0.1773 - val_loss: 0.0585 - val_mse: 0.0487 - val_mae: 0.1781 - lr: 0.0010\n",
      "Epoch 156/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0570 - mse: 0.0482 - mae: 0.1770 - val_loss: 0.0581 - val_mse: 0.0484 - val_mae: 0.1776 - lr: 0.0010\n",
      "Epoch 157/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0567 - mse: 0.0479 - mae: 0.1765 - val_loss: 0.0577 - val_mse: 0.0481 - val_mae: 0.1769 - lr: 0.0010\n",
      "Epoch 158/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0565 - mse: 0.0477 - mae: 0.1760 - val_loss: 0.0575 - val_mse: 0.0479 - val_mae: 0.1765 - lr: 0.0010\n",
      "Epoch 159/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0562 - mse: 0.0475 - mae: 0.1756 - val_loss: 0.0571 - val_mse: 0.0475 - val_mae: 0.1758 - lr: 0.0010\n",
      "Epoch 160/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0560 - mse: 0.0473 - mae: 0.1752 - val_loss: 0.0568 - val_mse: 0.0473 - val_mae: 0.1754 - lr: 0.0010\n",
      "Epoch 161/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0557 - mse: 0.0471 - mae: 0.1748 - val_loss: 0.0565 - val_mse: 0.0471 - val_mae: 0.1748 - lr: 0.0010\n",
      "Epoch 162/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0556 - mse: 0.0470 - mae: 0.1744 - val_loss: 0.0562 - val_mse: 0.0468 - val_mae: 0.1743 - lr: 0.0010\n",
      "Epoch 163/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.0553 - mse: 0.0467 - mae: 0.1739 - val_loss: 0.0559 - val_mse: 0.0465 - val_mae: 0.1737 - lr: 0.0010\n",
      "Epoch 164/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0550 - mse: 0.0465 - mae: 0.1735 - val_loss: 0.0556 - val_mse: 0.0463 - val_mae: 0.1733 - lr: 0.0010\n",
      "Epoch 165/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0548 - mse: 0.0463 - mae: 0.1730 - val_loss: 0.0554 - val_mse: 0.0461 - val_mae: 0.1728 - lr: 0.0010\n",
      "Epoch 166/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0545 - mse: 0.0460 - mae: 0.1724 - val_loss: 0.0550 - val_mse: 0.0458 - val_mae: 0.1721 - lr: 0.0010\n",
      "Epoch 167/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0543 - mse: 0.0458 - mae: 0.1720 - val_loss: 0.0547 - val_mse: 0.0455 - val_mae: 0.1714 - lr: 0.0010\n",
      "Epoch 168/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0540 - mse: 0.0456 - mae: 0.1715 - val_loss: 0.0544 - val_mse: 0.0453 - val_mae: 0.1709 - lr: 0.0010\n",
      "Epoch 169/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0538 - mse: 0.0454 - mae: 0.1711 - val_loss: 0.0541 - val_mse: 0.0451 - val_mae: 0.1704 - lr: 0.0010\n",
      "Epoch 170/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0535 - mse: 0.0452 - mae: 0.1706 - val_loss: 0.0538 - val_mse: 0.0448 - val_mae: 0.1699 - lr: 0.0010\n",
      "Epoch 171/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0533 - mse: 0.0450 - mae: 0.1702 - val_loss: 0.0536 - val_mse: 0.0446 - val_mae: 0.1694 - lr: 0.0010\n",
      "Epoch 172/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0531 - mse: 0.0448 - mae: 0.1698 - val_loss: 0.0533 - val_mse: 0.0444 - val_mae: 0.1689 - lr: 0.0010\n",
      "Epoch 173/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0529 - mse: 0.0447 - mae: 0.1694 - val_loss: 0.0530 - val_mse: 0.0442 - val_mae: 0.1682 - lr: 0.0010\n",
      "Epoch 174/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0526 - mse: 0.0444 - mae: 0.1689 - val_loss: 0.0528 - val_mse: 0.0440 - val_mae: 0.1678 - lr: 0.0010\n",
      "Epoch 175/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0524 - mse: 0.0442 - mae: 0.1685 - val_loss: 0.0526 - val_mse: 0.0438 - val_mae: 0.1673 - lr: 0.0010\n",
      "Epoch 176/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0522 - mse: 0.0441 - mae: 0.1683 - val_loss: 0.0522 - val_mse: 0.0435 - val_mae: 0.1668 - lr: 0.0010\n",
      "Epoch 177/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0520 - mse: 0.0439 - mae: 0.1677 - val_loss: 0.0520 - val_mse: 0.0433 - val_mae: 0.1663 - lr: 0.0010\n",
      "Epoch 178/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0518 - mse: 0.0437 - mae: 0.1674 - val_loss: 0.0518 - val_mse: 0.0432 - val_mae: 0.1660 - lr: 0.0010\n",
      "Epoch 179/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0516 - mse: 0.0435 - mae: 0.1670 - val_loss: 0.0515 - val_mse: 0.0429 - val_mae: 0.1654 - lr: 0.0010\n",
      "Epoch 180/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0513 - mse: 0.0433 - mae: 0.1666 - val_loss: 0.0513 - val_mse: 0.0428 - val_mae: 0.1650 - lr: 0.0010\n",
      "Epoch 181/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0512 - mse: 0.0432 - mae: 0.1663 - val_loss: 0.0510 - val_mse: 0.0425 - val_mae: 0.1646 - lr: 0.0010\n",
      "Epoch 182/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0509 - mse: 0.0430 - mae: 0.1658 - val_loss: 0.0508 - val_mse: 0.0423 - val_mae: 0.1641 - lr: 0.0010\n",
      "Epoch 183/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0507 - mse: 0.0428 - mae: 0.1656 - val_loss: 0.0506 - val_mse: 0.0422 - val_mae: 0.1639 - lr: 0.0010\n",
      "Epoch 184/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0506 - mse: 0.0427 - mae: 0.1652 - val_loss: 0.0504 - val_mse: 0.0420 - val_mae: 0.1635 - lr: 0.0010\n",
      "Epoch 185/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0504 - mse: 0.0426 - mae: 0.1650 - val_loss: 0.0502 - val_mse: 0.0418 - val_mae: 0.1630 - lr: 0.0010\n",
      "Epoch 186/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0502 - mse: 0.0424 - mae: 0.1647 - val_loss: 0.0500 - val_mse: 0.0417 - val_mae: 0.1629 - lr: 0.0010\n",
      "Epoch 187/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0501 - mse: 0.0423 - mae: 0.1644 - val_loss: 0.0498 - val_mse: 0.0416 - val_mae: 0.1624 - lr: 0.0010\n",
      "Epoch 188/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0499 - mse: 0.0421 - mae: 0.1640 - val_loss: 0.0497 - val_mse: 0.0414 - val_mae: 0.1623 - lr: 0.0010\n",
      "Epoch 189/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0497 - mse: 0.0420 - mae: 0.1638 - val_loss: 0.0494 - val_mse: 0.0412 - val_mae: 0.1619 - lr: 0.0010\n",
      "Epoch 190/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0496 - mse: 0.0419 - mae: 0.1637 - val_loss: 0.0493 - val_mse: 0.0411 - val_mae: 0.1616 - lr: 0.0010\n",
      "Epoch 191/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0495 - mse: 0.0418 - mae: 0.1634 - val_loss: 0.0492 - val_mse: 0.0411 - val_mae: 0.1615 - lr: 0.0010\n",
      "Epoch 192/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0493 - mse: 0.0416 - mae: 0.1629 - val_loss: 0.0488 - val_mse: 0.0407 - val_mae: 0.1609 - lr: 0.0010\n",
      "Epoch 193/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0491 - mse: 0.0415 - mae: 0.1628 - val_loss: 0.0487 - val_mse: 0.0407 - val_mae: 0.1608 - lr: 0.0010\n",
      "Epoch 194/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0490 - mse: 0.0415 - mae: 0.1625 - val_loss: 0.0486 - val_mse: 0.0406 - val_mae: 0.1606 - lr: 0.0010\n",
      "Epoch 195/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0489 - mse: 0.0413 - mae: 0.1623 - val_loss: 0.0484 - val_mse: 0.0404 - val_mae: 0.1602 - lr: 0.0010\n",
      "Epoch 196/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0487 - mse: 0.0412 - mae: 0.1621 - val_loss: 0.0482 - val_mse: 0.0403 - val_mae: 0.1599 - lr: 0.0010\n",
      "Epoch 197/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0487 - mse: 0.0411 - mae: 0.1620 - val_loss: 0.0481 - val_mse: 0.0402 - val_mae: 0.1597 - lr: 0.0010\n",
      "Epoch 198/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0485 - mse: 0.0410 - mae: 0.1616 - val_loss: 0.0481 - val_mse: 0.0402 - val_mae: 0.1596 - lr: 0.0010\n",
      "Epoch 199/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0484 - mse: 0.0409 - mae: 0.1614 - val_loss: 0.0478 - val_mse: 0.0399 - val_mae: 0.1592 - lr: 0.0010\n",
      "Epoch 200/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0483 - mse: 0.0408 - mae: 0.1614 - val_loss: 0.0478 - val_mse: 0.0399 - val_mae: 0.1592 - lr: 0.0010\n",
      "Epoch 201/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0481 - mse: 0.0407 - mae: 0.1610 - val_loss: 0.0476 - val_mse: 0.0398 - val_mae: 0.1588 - lr: 0.0010\n",
      "Epoch 202/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0480 - mse: 0.0406 - mae: 0.1608 - val_loss: 0.0475 - val_mse: 0.0397 - val_mae: 0.1587 - lr: 0.0010\n",
      "Epoch 203/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0479 - mse: 0.0405 - mae: 0.1606 - val_loss: 0.0474 - val_mse: 0.0396 - val_mae: 0.1586 - lr: 0.0010\n",
      "Epoch 204/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0478 - mse: 0.0404 - mae: 0.1604 - val_loss: 0.0473 - val_mse: 0.0396 - val_mae: 0.1584 - lr: 0.0010\n",
      "Epoch 205/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0477 - mse: 0.0403 - mae: 0.1603 - val_loss: 0.0472 - val_mse: 0.0394 - val_mae: 0.1582 - lr: 0.0010\n",
      "Epoch 206/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0476 - mse: 0.0403 - mae: 0.1601 - val_loss: 0.0470 - val_mse: 0.0392 - val_mae: 0.1578 - lr: 0.0010\n",
      "Epoch 207/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0474 - mse: 0.0401 - mae: 0.1598 - val_loss: 0.0469 - val_mse: 0.0392 - val_mae: 0.1576 - lr: 0.0010\n",
      "Epoch 208/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0474 - mse: 0.0401 - mae: 0.1597 - val_loss: 0.0468 - val_mse: 0.0391 - val_mae: 0.1576 - lr: 0.0010\n",
      "Epoch 209/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0472 - mse: 0.0400 - mae: 0.1595 - val_loss: 0.0466 - val_mse: 0.0390 - val_mae: 0.1572 - lr: 0.0010\n",
      "Epoch 210/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0471 - mse: 0.0399 - mae: 0.1593 - val_loss: 0.0466 - val_mse: 0.0389 - val_mae: 0.1571 - lr: 0.0010\n",
      "Epoch 211/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0470 - mse: 0.0398 - mae: 0.1591 - val_loss: 0.0464 - val_mse: 0.0388 - val_mae: 0.1569 - lr: 0.0010\n",
      "Epoch 212/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0470 - mse: 0.0398 - mae: 0.1590 - val_loss: 0.0464 - val_mse: 0.0388 - val_mae: 0.1570 - lr: 0.0010\n",
      "Epoch 213/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0469 - mse: 0.0397 - mae: 0.1588 - val_loss: 0.0463 - val_mse: 0.0388 - val_mae: 0.1567 - lr: 0.0010\n",
      "Epoch 214/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0468 - mse: 0.0396 - mae: 0.1586 - val_loss: 0.0462 - val_mse: 0.0387 - val_mae: 0.1566 - lr: 0.0010\n",
      "Epoch 215/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0467 - mse: 0.0395 - mae: 0.1584 - val_loss: 0.0461 - val_mse: 0.0386 - val_mae: 0.1563 - lr: 0.0010\n",
      "Epoch 216/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0466 - mse: 0.0395 - mae: 0.1584 - val_loss: 0.0460 - val_mse: 0.0385 - val_mae: 0.1562 - lr: 0.0010\n",
      "Epoch 217/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0465 - mse: 0.0394 - mae: 0.1582 - val_loss: 0.0459 - val_mse: 0.0384 - val_mae: 0.1561 - lr: 0.0010\n",
      "Epoch 218/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0464 - mse: 0.0393 - mae: 0.1579 - val_loss: 0.0459 - val_mse: 0.0384 - val_mae: 0.1560 - lr: 0.0010\n",
      "Epoch 219/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0463 - mse: 0.0392 - mae: 0.1578 - val_loss: 0.0457 - val_mse: 0.0382 - val_mae: 0.1556 - lr: 0.0010\n",
      "Epoch 220/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0462 - mse: 0.0392 - mae: 0.1575 - val_loss: 0.0456 - val_mse: 0.0382 - val_mae: 0.1555 - lr: 0.0010\n",
      "Epoch 221/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0461 - mse: 0.0391 - mae: 0.1575 - val_loss: 0.0455 - val_mse: 0.0381 - val_mae: 0.1554 - lr: 0.0010\n",
      "Epoch 222/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0460 - mse: 0.0390 - mae: 0.1573 - val_loss: 0.0455 - val_mse: 0.0381 - val_mae: 0.1553 - lr: 0.0010\n",
      "Epoch 223/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0460 - mse: 0.0390 - mae: 0.1572 - val_loss: 0.0455 - val_mse: 0.0381 - val_mae: 0.1553 - lr: 0.0010\n",
      "Epoch 224/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0459 - mse: 0.0389 - mae: 0.1570 - val_loss: 0.0453 - val_mse: 0.0379 - val_mae: 0.1549 - lr: 0.0010\n",
      "Epoch 225/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0458 - mse: 0.0389 - mae: 0.1568 - val_loss: 0.0452 - val_mse: 0.0379 - val_mae: 0.1548 - lr: 0.0010\n",
      "Epoch 226/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0457 - mse: 0.0388 - mae: 0.1567 - val_loss: 0.0452 - val_mse: 0.0379 - val_mae: 0.1548 - lr: 0.0010\n",
      "Epoch 227/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0457 - mse: 0.0388 - mae: 0.1567 - val_loss: 0.0451 - val_mse: 0.0378 - val_mae: 0.1547 - lr: 0.0010\n",
      "Epoch 228/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0456 - mse: 0.0387 - mae: 0.1565 - val_loss: 0.0450 - val_mse: 0.0377 - val_mae: 0.1544 - lr: 0.0010\n",
      "Epoch 229/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0455 - mse: 0.0386 - mae: 0.1564 - val_loss: 0.0450 - val_mse: 0.0377 - val_mae: 0.1544 - lr: 0.0010\n",
      "Epoch 230/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0454 - mse: 0.0385 - mae: 0.1562 - val_loss: 0.0448 - val_mse: 0.0375 - val_mae: 0.1541 - lr: 0.0010\n",
      "Epoch 231/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0454 - mse: 0.0385 - mae: 0.1560 - val_loss: 0.0448 - val_mse: 0.0375 - val_mae: 0.1541 - lr: 0.0010\n",
      "Epoch 232/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0453 - mse: 0.0384 - mae: 0.1560 - val_loss: 0.0447 - val_mse: 0.0374 - val_mae: 0.1540 - lr: 0.0010\n",
      "Epoch 233/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0452 - mse: 0.0384 - mae: 0.1558 - val_loss: 0.0446 - val_mse: 0.0374 - val_mae: 0.1539 - lr: 0.0010\n",
      "Epoch 234/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0451 - mse: 0.0383 - mae: 0.1557 - val_loss: 0.0445 - val_mse: 0.0373 - val_mae: 0.1537 - lr: 0.0010\n",
      "Epoch 235/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0451 - mse: 0.0383 - mae: 0.1556 - val_loss: 0.0445 - val_mse: 0.0373 - val_mae: 0.1538 - lr: 0.0010\n",
      "Epoch 236/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0450 - mse: 0.0382 - mae: 0.1555 - val_loss: 0.0445 - val_mse: 0.0373 - val_mae: 0.1537 - lr: 0.0010\n",
      "Epoch 237/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0450 - mse: 0.0382 - mae: 0.1554 - val_loss: 0.0444 - val_mse: 0.0372 - val_mae: 0.1535 - lr: 0.0010\n",
      "Epoch 238/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0449 - mse: 0.0381 - mae: 0.1552 - val_loss: 0.0444 - val_mse: 0.0372 - val_mae: 0.1535 - lr: 0.0010\n",
      "Epoch 239/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.0448 - mse: 0.0381 - mae: 0.1551 - val_loss: 0.0442 - val_mse: 0.0371 - val_mae: 0.1532 - lr: 0.0010\n",
      "Epoch 240/500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0448 - mse: 0.0380 - mae: 0.1550 - val_loss: 0.0442 - val_mse: 0.0370 - val_mae: 0.1530 - lr: 0.0010\n",
      "Epoch 241/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.0447 - mse: 0.0380 - mae: 0.1549 - val_loss: 0.0442 - val_mse: 0.0370 - val_mae: 0.1530 - lr: 0.0010\n",
      "Epoch 242/500\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0447 - mse: 0.0380 - mae: 0.1548 - val_loss: 0.0441 - val_mse: 0.0370 - val_mae: 0.1528 - lr: 0.0010\n",
      "Epoch 243/500\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 0.0446 - mse: 0.0379 - mae: 0.1546 - val_loss: 0.0441 - val_mse: 0.0369 - val_mae: 0.1528 - lr: 0.0010\n",
      "Epoch 244/500\n",
      "25/25 [==============================] - 0s 12ms/step - loss: 0.0445 - mse: 0.0379 - mae: 0.1546 - val_loss: 0.0440 - val_mse: 0.0369 - val_mae: 0.1528 - lr: 0.0010\n",
      "Epoch 245/500\n",
      "25/25 [==============================] - 0s 12ms/step - loss: 0.0445 - mse: 0.0378 - mae: 0.1544 - val_loss: 0.0439 - val_mse: 0.0368 - val_mae: 0.1525 - lr: 0.0010\n",
      "Epoch 246/500\n",
      "25/25 [==============================] - 1s 23ms/step - loss: 0.0444 - mse: 0.0378 - mae: 0.1543 - val_loss: 0.0439 - val_mse: 0.0368 - val_mae: 0.1524 - lr: 0.0010\n",
      "Epoch 247/500\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0443 - mse: 0.0377 - mae: 0.1541 - val_loss: 0.0437 - val_mse: 0.0367 - val_mae: 0.1522 - lr: 0.0010\n",
      "Epoch 248/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0443 - mse: 0.0377 - mae: 0.1542 - val_loss: 0.0437 - val_mse: 0.0366 - val_mae: 0.1521 - lr: 0.0010\n",
      "Epoch 249/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0443 - mse: 0.0376 - mae: 0.1540 - val_loss: 0.0438 - val_mse: 0.0367 - val_mae: 0.1522 - lr: 0.0010\n",
      "Epoch 250/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0442 - mse: 0.0377 - mae: 0.1540 - val_loss: 0.0436 - val_mse: 0.0366 - val_mae: 0.1520 - lr: 0.0010\n",
      "Epoch 251/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0441 - mse: 0.0376 - mae: 0.1538 - val_loss: 0.0436 - val_mse: 0.0366 - val_mae: 0.1518 - lr: 0.0010\n",
      "Epoch 252/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0441 - mse: 0.0375 - mae: 0.1537 - val_loss: 0.0435 - val_mse: 0.0365 - val_mae: 0.1517 - lr: 0.0010\n",
      "Epoch 253/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0440 - mse: 0.0375 - mae: 0.1537 - val_loss: 0.0435 - val_mse: 0.0365 - val_mae: 0.1518 - lr: 0.0010\n",
      "Epoch 254/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0440 - mse: 0.0374 - mae: 0.1536 - val_loss: 0.0434 - val_mse: 0.0364 - val_mae: 0.1515 - lr: 0.0010\n",
      "Epoch 255/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0439 - mse: 0.0374 - mae: 0.1533 - val_loss: 0.0433 - val_mse: 0.0364 - val_mae: 0.1513 - lr: 0.0010\n",
      "Epoch 256/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0439 - mse: 0.0373 - mae: 0.1532 - val_loss: 0.0433 - val_mse: 0.0363 - val_mae: 0.1512 - lr: 0.0010\n",
      "Epoch 257/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0438 - mse: 0.0373 - mae: 0.1532 - val_loss: 0.0432 - val_mse: 0.0363 - val_mae: 0.1512 - lr: 0.0010\n",
      "Epoch 258/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0438 - mse: 0.0373 - mae: 0.1531 - val_loss: 0.0432 - val_mse: 0.0363 - val_mae: 0.1512 - lr: 0.0010\n",
      "Epoch 259/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0438 - mse: 0.0373 - mae: 0.1531 - val_loss: 0.0431 - val_mse: 0.0362 - val_mae: 0.1509 - lr: 0.0010\n",
      "Epoch 260/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0437 - mse: 0.0372 - mae: 0.1528 - val_loss: 0.0431 - val_mse: 0.0362 - val_mae: 0.1509 - lr: 0.0010\n",
      "Epoch 261/500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0436 - mse: 0.0372 - mae: 0.1529 - val_loss: 0.0430 - val_mse: 0.0361 - val_mae: 0.1508 - lr: 0.0010\n",
      "Epoch 262/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0436 - mse: 0.0371 - mae: 0.1527 - val_loss: 0.0431 - val_mse: 0.0362 - val_mae: 0.1507 - lr: 0.0010\n",
      "Epoch 263/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0436 - mse: 0.0371 - mae: 0.1527 - val_loss: 0.0430 - val_mse: 0.0360 - val_mae: 0.1505 - lr: 0.0010\n",
      "Epoch 264/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0436 - mse: 0.0371 - mae: 0.1527 - val_loss: 0.0429 - val_mse: 0.0360 - val_mae: 0.1504 - lr: 0.0010\n",
      "Epoch 265/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0434 - mse: 0.0370 - mae: 0.1524 - val_loss: 0.0429 - val_mse: 0.0360 - val_mae: 0.1503 - lr: 0.0010\n",
      "Epoch 266/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0435 - mse: 0.0370 - mae: 0.1525 - val_loss: 0.0429 - val_mse: 0.0360 - val_mae: 0.1503 - lr: 0.0010\n",
      "Epoch 267/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0434 - mse: 0.0370 - mae: 0.1524 - val_loss: 0.0428 - val_mse: 0.0359 - val_mae: 0.1502 - lr: 0.0010\n",
      "Epoch 268/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0433 - mse: 0.0369 - mae: 0.1522 - val_loss: 0.0428 - val_mse: 0.0359 - val_mae: 0.1500 - lr: 0.0010\n",
      "Epoch 269/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0433 - mse: 0.0369 - mae: 0.1520 - val_loss: 0.0428 - val_mse: 0.0359 - val_mae: 0.1499 - lr: 0.0010\n",
      "Epoch 270/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0432 - mse: 0.0368 - mae: 0.1520 - val_loss: 0.0426 - val_mse: 0.0358 - val_mae: 0.1497 - lr: 0.0010\n",
      "Epoch 271/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0432 - mse: 0.0368 - mae: 0.1519 - val_loss: 0.0427 - val_mse: 0.0358 - val_mae: 0.1498 - lr: 0.0010\n",
      "Epoch 272/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0431 - mse: 0.0367 - mae: 0.1517 - val_loss: 0.0426 - val_mse: 0.0357 - val_mae: 0.1495 - lr: 0.0010\n",
      "Epoch 273/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0431 - mse: 0.0367 - mae: 0.1517 - val_loss: 0.0425 - val_mse: 0.0357 - val_mae: 0.1497 - lr: 0.0010\n",
      "Epoch 274/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0431 - mse: 0.0367 - mae: 0.1516 - val_loss: 0.0425 - val_mse: 0.0357 - val_mae: 0.1496 - lr: 0.0010\n",
      "Epoch 275/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0430 - mse: 0.0366 - mae: 0.1515 - val_loss: 0.0424 - val_mse: 0.0356 - val_mae: 0.1493 - lr: 0.0010\n",
      "Epoch 276/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.0430 - mse: 0.0366 - mae: 0.1515 - val_loss: 0.0424 - val_mse: 0.0356 - val_mae: 0.1494 - lr: 0.0010\n",
      "Epoch 277/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0429 - mse: 0.0366 - mae: 0.1514 - val_loss: 0.0424 - val_mse: 0.0356 - val_mae: 0.1492 - lr: 0.0010\n",
      "Epoch 278/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0428 - mse: 0.0365 - mae: 0.1511 - val_loss: 0.0424 - val_mse: 0.0356 - val_mae: 0.1492 - lr: 0.0010\n",
      "Epoch 279/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0428 - mse: 0.0364 - mae: 0.1510 - val_loss: 0.0423 - val_mse: 0.0355 - val_mae: 0.1491 - lr: 0.0010\n",
      "Epoch 280/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0427 - mse: 0.0364 - mae: 0.1510 - val_loss: 0.0422 - val_mse: 0.0355 - val_mae: 0.1490 - lr: 0.0010\n",
      "Epoch 281/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0427 - mse: 0.0364 - mae: 0.1510 - val_loss: 0.0422 - val_mse: 0.0355 - val_mae: 0.1490 - lr: 0.0010\n",
      "Epoch 282/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0427 - mse: 0.0364 - mae: 0.1510 - val_loss: 0.0421 - val_mse: 0.0353 - val_mae: 0.1488 - lr: 0.0010\n",
      "Epoch 283/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0426 - mse: 0.0363 - mae: 0.1507 - val_loss: 0.0421 - val_mse: 0.0354 - val_mae: 0.1487 - lr: 0.0010\n",
      "Epoch 284/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0425 - mse: 0.0362 - mae: 0.1505 - val_loss: 0.0421 - val_mse: 0.0353 - val_mae: 0.1487 - lr: 0.0010\n",
      "Epoch 285/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0425 - mse: 0.0362 - mae: 0.1506 - val_loss: 0.0420 - val_mse: 0.0352 - val_mae: 0.1485 - lr: 0.0010\n",
      "Epoch 286/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0424 - mse: 0.0362 - mae: 0.1505 - val_loss: 0.0420 - val_mse: 0.0352 - val_mae: 0.1484 - lr: 0.0010\n",
      "Epoch 287/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0424 - mse: 0.0362 - mae: 0.1505 - val_loss: 0.0420 - val_mse: 0.0352 - val_mae: 0.1483 - lr: 0.0010\n",
      "Epoch 288/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0424 - mse: 0.0361 - mae: 0.1503 - val_loss: 0.0419 - val_mse: 0.0352 - val_mae: 0.1484 - lr: 0.0010\n",
      "Epoch 289/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0424 - mse: 0.0361 - mae: 0.1503 - val_loss: 0.0418 - val_mse: 0.0351 - val_mae: 0.1483 - lr: 0.0010\n",
      "Epoch 290/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0423 - mse: 0.0361 - mae: 0.1503 - val_loss: 0.0418 - val_mse: 0.0351 - val_mae: 0.1482 - lr: 0.0010\n",
      "Epoch 291/500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0423 - mse: 0.0360 - mae: 0.1501 - val_loss: 0.0418 - val_mse: 0.0351 - val_mae: 0.1481 - lr: 0.0010\n",
      "Epoch 292/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.0422 - mse: 0.0360 - mae: 0.1500 - val_loss: 0.0417 - val_mse: 0.0350 - val_mae: 0.1480 - lr: 0.0010\n",
      "Epoch 293/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0422 - mse: 0.0359 - mae: 0.1499 - val_loss: 0.0417 - val_mse: 0.0350 - val_mae: 0.1479 - lr: 0.0010\n",
      "Epoch 294/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0421 - mse: 0.0359 - mae: 0.1499 - val_loss: 0.0417 - val_mse: 0.0350 - val_mae: 0.1479 - lr: 0.0010\n",
      "Epoch 295/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0421 - mse: 0.0359 - mae: 0.1497 - val_loss: 0.0416 - val_mse: 0.0349 - val_mae: 0.1478 - lr: 0.0010\n",
      "Epoch 296/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0420 - mse: 0.0358 - mae: 0.1496 - val_loss: 0.0416 - val_mse: 0.0349 - val_mae: 0.1476 - lr: 0.0010\n",
      "Epoch 297/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0420 - mse: 0.0358 - mae: 0.1496 - val_loss: 0.0415 - val_mse: 0.0349 - val_mae: 0.1477 - lr: 0.0010\n",
      "Epoch 298/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0420 - mse: 0.0358 - mae: 0.1495 - val_loss: 0.0416 - val_mse: 0.0349 - val_mae: 0.1478 - lr: 0.0010\n",
      "Epoch 299/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0419 - mse: 0.0358 - mae: 0.1495 - val_loss: 0.0415 - val_mse: 0.0348 - val_mae: 0.1474 - lr: 0.0010\n",
      "Epoch 300/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0419 - mse: 0.0357 - mae: 0.1494 - val_loss: 0.0414 - val_mse: 0.0348 - val_mae: 0.1473 - lr: 0.0010\n",
      "Epoch 301/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0418 - mse: 0.0356 - mae: 0.1491 - val_loss: 0.0415 - val_mse: 0.0348 - val_mae: 0.1474 - lr: 0.0010\n",
      "Epoch 302/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0418 - mse: 0.0357 - mae: 0.1493 - val_loss: 0.0414 - val_mse: 0.0348 - val_mae: 0.1473 - lr: 0.0010\n",
      "Epoch 303/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0418 - mse: 0.0356 - mae: 0.1490 - val_loss: 0.0415 - val_mse: 0.0348 - val_mae: 0.1474 - lr: 0.0010\n",
      "Epoch 304/500\n",
      "16/25 [==================>...........] - ETA: 0s - loss: 0.0420 - mse: 0.0358 - mae: 0.1493\n",
      "Epoch 304: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0418 - mse: 0.0356 - mae: 0.1492 - val_loss: 0.0414 - val_mse: 0.0347 - val_mae: 0.1472 - lr: 0.0010\n",
      "Epoch 305/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.0417 - mse: 0.0355 - mae: 0.1488 - val_loss: 0.0414 - val_mse: 0.0347 - val_mae: 0.1470 - lr: 5.0000e-04\n",
      "Epoch 306/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0416 - mse: 0.0355 - mae: 0.1488 - val_loss: 0.0413 - val_mse: 0.0347 - val_mae: 0.1470 - lr: 5.0000e-04\n",
      "Epoch 307/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0416 - mse: 0.0355 - mae: 0.1488 - val_loss: 0.0412 - val_mse: 0.0346 - val_mae: 0.1469 - lr: 5.0000e-04\n",
      "Epoch 308/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0416 - mse: 0.0354 - mae: 0.1487 - val_loss: 0.0413 - val_mse: 0.0346 - val_mae: 0.1469 - lr: 5.0000e-04\n",
      "Epoch 309/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0416 - mse: 0.0354 - mae: 0.1487 - val_loss: 0.0413 - val_mse: 0.0346 - val_mae: 0.1469 - lr: 5.0000e-04\n",
      "Epoch 310/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0415 - mse: 0.0354 - mae: 0.1486 - val_loss: 0.0412 - val_mse: 0.0346 - val_mae: 0.1469 - lr: 5.0000e-04\n",
      "Epoch 311/500\n",
      "18/25 [====================>.........] - ETA: 0s - loss: 0.0415 - mse: 0.0354 - mae: 0.1484\n",
      "Epoch 311: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0415 - mse: 0.0354 - mae: 0.1486 - val_loss: 0.0412 - val_mse: 0.0346 - val_mae: 0.1469 - lr: 5.0000e-04\n",
      "Epoch 312/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0415 - mse: 0.0354 - mae: 0.1485 - val_loss: 0.0412 - val_mse: 0.0346 - val_mae: 0.1468 - lr: 2.5000e-04\n",
      "Epoch 313/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.0415 - mse: 0.0354 - mae: 0.1485 - val_loss: 0.0412 - val_mse: 0.0346 - val_mae: 0.1468 - lr: 2.5000e-04\n",
      "Epoch 314/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0415 - mse: 0.0353 - mae: 0.1485 - val_loss: 0.0412 - val_mse: 0.0346 - val_mae: 0.1468 - lr: 2.5000e-04\n",
      "Epoch 315/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0415 - mse: 0.0353 - mae: 0.1485 - val_loss: 0.0412 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 2.5000e-04\n",
      "Epoch 316/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0415 - mse: 0.0353 - mae: 0.1485 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1468 - lr: 2.5000e-04\n",
      "Epoch 317/500\n",
      "18/25 [====================>.........] - ETA: 0s - loss: 0.0419 - mse: 0.0358 - mae: 0.1493\n",
      "Epoch 317: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1484 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 2.5000e-04\n",
      "Epoch 318/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1484 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.2500e-04\n",
      "Epoch 319/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1484 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.2500e-04\n",
      "Epoch 320/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1484 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.2500e-04\n",
      "Epoch 321/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1484 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.2500e-04\n",
      "Epoch 322/500\n",
      "18/25 [====================>.........] - ETA: 0s - loss: 0.0423 - mse: 0.0360 - mae: 0.1499\n",
      "Epoch 322: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.2500e-04\n",
      "Epoch 323/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 6.2500e-05\n",
      "Epoch 324/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 6.2500e-05\n",
      "Epoch 325/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 6.2500e-05\n",
      "Epoch 326/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 6.2500e-05\n",
      "Epoch 327/500\n",
      "17/25 [===================>..........] - ETA: 0s - loss: 0.0418 - mse: 0.0357 - mae: 0.1487\n",
      "Epoch 327: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 6.2500e-05\n",
      "Epoch 328/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 3.1250e-05\n",
      "Epoch 329/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 3.1250e-05\n",
      "Epoch 330/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 3.1250e-05\n",
      "Epoch 331/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 3.1250e-05\n",
      "Epoch 332/500\n",
      "19/25 [=====================>........] - ETA: 0s - loss: 0.0417 - mse: 0.0355 - mae: 0.1491\n",
      "Epoch 332: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 3.1250e-05\n",
      "Epoch 333/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.5625e-05\n",
      "Epoch 334/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.5625e-05\n",
      "Epoch 335/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.5625e-05\n",
      "Epoch 336/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.5625e-05\n",
      "Epoch 337/500\n",
      "19/25 [=====================>........] - ETA: 0s - loss: 0.0414 - mse: 0.0353 - mae: 0.1482\n",
      "Epoch 337: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.5625e-05\n",
      "Epoch 338/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 7.8125e-06\n",
      "Epoch 339/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 7.8125e-06\n",
      "Epoch 340/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 7.8125e-06\n",
      "Epoch 341/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 7.8125e-06\n",
      "Epoch 342/500\n",
      "19/25 [=====================>........] - ETA: 0s - loss: 0.0411 - mse: 0.0351 - mae: 0.1478\n",
      "Epoch 342: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 7.8125e-06\n",
      "Epoch 343/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 3.9063e-06\n",
      "Epoch 344/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 3.9063e-06\n",
      "Epoch 345/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 3.9063e-06\n",
      "Epoch 346/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 3.9063e-06\n",
      "Epoch 347/500\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.0413 - mse: 0.0351 - mae: 0.1482\n",
      "Epoch 347: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 3.9063e-06\n",
      "Epoch 348/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.9531e-06\n",
      "Epoch 349/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.9531e-06\n",
      "Epoch 350/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.9531e-06\n",
      "Epoch 351/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.9531e-06\n",
      "Epoch 352/500\n",
      "19/25 [=====================>........] - ETA: 0s - loss: 0.0405 - mse: 0.0345 - mae: 0.1466\n",
      "Epoch 352: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.9531e-06\n",
      "Epoch 353/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 9.7656e-07\n",
      "Epoch 354/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 9.7656e-07\n",
      "Epoch 355/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 9.7656e-07\n",
      "Epoch 356/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 9.7656e-07\n",
      "Epoch 357/500\n",
      "19/25 [=====================>........] - ETA: 0s - loss: 0.0415 - mse: 0.0353 - mae: 0.1487\n",
      "Epoch 357: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 9.7656e-07\n",
      "Epoch 358/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 4.8828e-07\n",
      "Epoch 359/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 4.8828e-07\n",
      "Epoch 360/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 4.8828e-07\n",
      "Epoch 361/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 4.8828e-07\n",
      "Epoch 362/500\n",
      "18/25 [====================>.........] - ETA: 0s - loss: 0.0406 - mse: 0.0346 - mae: 0.1468\n",
      "Epoch 362: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 4.8828e-07\n",
      "Epoch 363/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 2.4414e-07\n",
      "Epoch 364/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 2.4414e-07\n",
      "Epoch 365/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 2.4414e-07\n",
      "Epoch 366/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 2.4414e-07\n",
      "Epoch 367/500\n",
      "13/25 [==============>...............] - ETA: 0s - loss: 0.0407 - mse: 0.0347 - mae: 0.1473\n",
      "Epoch 367: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 2.4414e-07\n",
      "Epoch 368/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.2207e-07\n",
      "Epoch 369/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.2207e-07\n",
      "Epoch 370/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.2207e-07\n",
      "Epoch 371/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.2207e-07\n",
      "Epoch 372/500\n",
      "20/25 [=======================>......] - ETA: 0s - loss: 0.0414 - mse: 0.0351 - mae: 0.1482\n",
      "Epoch 372: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.2207e-07\n",
      "Epoch 373/500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.0000e-07\n",
      "Epoch 374/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.0000e-07\n",
      "Epoch 375/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.0000e-07\n",
      "Epoch 376/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.0000e-07\n",
      "Epoch 377/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.0000e-07\n",
      "Epoch 378/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.0000e-07\n",
      "Epoch 379/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.0000e-07\n",
      "Epoch 380/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.0000e-07\n",
      "Epoch 381/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.0000e-07\n",
      "Epoch 382/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.0000e-07\n",
      "Epoch 383/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.0000e-07\n",
      "Epoch 384/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.0000e-07\n",
      "Epoch 385/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.0000e-07\n",
      "Epoch 386/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.0000e-07\n",
      "Epoch 387/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.0000e-07\n",
      "Epoch 388/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.0000e-07\n",
      "Epoch 389/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.0000e-07\n",
      "Epoch 390/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.0000e-07\n",
      "Epoch 391/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.0000e-07\n",
      "Epoch 392/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.0000e-07\n",
      "Epoch 393/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.0000e-07\n",
      "Epoch 394/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.0000e-07\n",
      "Epoch 395/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.0000e-07\n",
      "Epoch 396/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.0000e-07\n",
      "Epoch 397/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.0000e-07\n",
      "Epoch 398/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.0000e-07\n",
      "Epoch 399/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0414 - mse: 0.0353 - mae: 0.1483 - val_loss: 0.0411 - val_mse: 0.0345 - val_mae: 0.1467 - lr: 1.0000e-07\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=500,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[adjust_lr, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0411 - mse: 0.0345 - mae: 0.1467\n",
      "Test MSE: 0.0345, Test MAE: 0.1467\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_mse, test_mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test MSE: {test_mse:.4f}, Test MAE: {test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAGyCAYAAAAMKHu5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABnTUlEQVR4nO3deVzU1f7H8dfMsCMgsqOAmvuuuORWVuZWplk3M9Mss5+lltpqZov3Fi238nZNTVOr26KlLZaWYrnlkoriivsCKYS4gIqyzHx/f4xOIWCKyjDwfv4e83A43/P9zuc43h/vvss5JsMwDEREREQqELOzCxAREREpbQpAIiIiUuEoAImIiEiFowAkIiIiFY4CkIiIiFQ4CkAiIiJS4SgAiYiISIWjACQiIiIVjgKQiIiIVDhuzi6gLLLZbBw+fBg/Pz9MJpOzyxEREZFLYBgGJ0+eJDIyErP5b87xGE72/vvvG9WrVzc8PT2NFi1aGMuXLy+27+HDh41+/foZderUMUwmk/HEE08U2e/48ePGY489ZoSHhxuenp5GvXr1jPnz519yTSkpKQagl1566aWXXnq54CslJeVvf9c79QzQ7NmzGTlyJJMmTaJ9+/Z88MEHdO/ene3btxMdHV2of05ODiEhIYwdO5Z33323yGPm5uZy6623Ehoaypw5c6hWrRopKSn4+fldcl3n+6akpODv71+ywYmIiEipysrKIioq6pJ+55sMw3mLobZp04YWLVowefJkR1v9+vXp3bs3cXFxF923U6dONGvWjAkTJhRonzJlCm+99RY7duzA3d29RHVlZWUREBBAZmamApCIiIiLuJzf3067CTo3N5eEhAS6dOlSoL1Lly6sWrWqxMedN28ebdu2ZdiwYYSFhdGoUSNee+01rFZrsfvk5OSQlZVV4CUiIiLll9MCUEZGBlarlbCwsALtYWFhpKWllfi4+/btY86cOVitVhYsWMALL7zA22+/zauvvlrsPnFxcQQEBDheUVFRJf58ERERKfuc/hj8hU9ZGYZxRU9e2Ww2QkNDmTp1KrGxsdx7772MHTu2wGW2C40ZM4bMzEzHKyUlpcSfLyIiImWf026CDg4OxmKxFDrbk56eXuis0OWIiIjA3d0di8XiaKtfvz5paWnk5ubi4eFRaB9PT088PT1L/JkiIlKY1WolLy/P2WVIOePh4fH3j7hfAqcFIA8PD2JjY4mPj+fOO+90tMfHx9OrV68SH7d9+/Z8/vnn2Gw2x1/Qrl27iIiIKDL8iIjI1WUYBmlpaZw4ccLZpUg5ZDabqVGjxhX/TnfqY/CjR49mwIABtGzZkrZt2zJ16lSSk5MZOnQoYL80dejQIT755BPHPomJiQCcOnWKI0eOkJiYiIeHBw0aNADg0Ucf5b///S9PPPEEI0aMYPfu3bz22ms8/vjjpT4+EZGK6Hz4CQ0NxcfHRxPKylVzfqLi1NRUoqOjr+jfllMDUN++fTl69Cjjx48nNTWVRo0asWDBAmJiYgBITU0lOTm5wD7Nmzd3vE9ISODzzz8nJiaGAwcOABAVFcWiRYsYNWoUTZo0oWrVqjzxxBM8++yzpTYuEZGKymq1OsJPUFCQs8uRcigkJITDhw+Tn59f4uluwMnzAJVVmgdIRKRkzp49y/79+6levTre3t7OLkfKoTNnznDgwAFq1KiBl5dXgW0uMQ+QiIiUX7rsJdfK1fq3pQAkIiIiFY4CkIiIyDXQqVMnRo4cecn9Dxw4gMlkcjzsI9eWApCIiFRoJpPpoq9BgwaV6Lhff/01//znPy+5f1RUlOOBoGtJQcvOqU+BVTg2G6RuhNCG4O719/1FROSaS01NdbyfPXs2L774Ijt37nS0XXgzd15e3iU9fVSlSpXLqsNisRAeHn5Z+0jJ6QxQaTq6B6bdDHFVYUoHmDcCNn4GeWecXZmISIUVHh7ueAUEBGAymRw/nz17lsqVK/Pll1/SqVMnvLy8+PTTTzl69Cj9+vWjWrVq+Pj40LhxY7744osCx73wElj16tV57bXXeOihh/Dz8yM6OpqpU6c6tl94Zmbp0qWYTCZ+/vlnWrZsiY+PD+3atSsQzgD+9a9/ERoaip+fHw8//DDPPfcczZo1K/HfR05ODo8//jihoaF4eXnRoUMH1q1b59h+/Phx+vfvT0hICN7e3tSuXZuZM2cC9oXOhw8fTkREBF5eXlSvXp24uLgS13ItKQCVppOHwScIbPmQtgU2fALfPQYTmsCuhc6uTkTkqjMMg+zcfKe8ruYsL88++yyPP/44SUlJdO3albNnzxIbG8sPP/zA1q1beeSRRxgwYAC//fbbRY/z9ttv07JlSzZu3Mhjjz3Go48+yo4dOy66z9ixY3n77bdZv349bm5uPPTQQ45tn332Ga+++ipvvPEGCQkJREdHX3Tty0vxzDPPMHfuXD7++GM2bNhArVq16Nq1K8eOHQNg3LhxbN++nR9//JGkpCQmT55McHAwAO+99x7z5s3jyy+/ZOfOnXz66adUr179iuq5VnQJrDTV7ARP74XMFDicCIc3wtY5cCIZPu8LPf8DsQ84u0oRkavmTJ6VBi865z/wto/vio/H1fk1N3LkSPr06VOg7amnnnK8HzFiBD/99BNfffUVbdq0KfY4PXr04LHHHgPsoerdd99l6dKl1KtXr9h9Xn31VW688UYAnnvuOW677TbOnj2Ll5cX//3vfxk8eDAPPvggAC+++CKLFi3i1KlTJRrn6dOnmTx5Mh999BHdu3cHYNq0acTHxzN9+nSefvppkpOTad68OS1btgQoEHCSk5OpXbs2HTp0wGQyOSY2Lot0BqgU2WwG7y/dy9bTARj1e0Lnl2B4AsQ+CBjwwyjYt9TZZYqIyAXO/7I/z2q18uqrr9KkSROCgoKoVKkSixYtKrR6wYWaNGnieH/+Ult6evol7xMREQHg2Gfnzp20bt26QP8Lf74ce/fuJS8vj/bt2zva3N3dad26NUlJSYB9yalZs2bRrFkznnnmGVatWuXoO2jQIBITE6lbty6PP/44ixYtKnEt15rOAJWizYcyeWvhTt5auJOqlb25tUEY3RuF0/q2dzDlZcPm2TB3CDy2Bnw1hbyIuD5vdwvbx3d12mdfLb6+vgV+fvvtt3n33XeZMGECjRs3xtfXl5EjR5Kbm3vR41x487TJZMJms13yPucnAfzrPhdODHgll/7O71vUMc+3de/enYMHDzJ//nwWL17MLbfcwrBhw/j3v/9NixYt2L9/Pz/++COLFy/mnnvuoXPnzsyZM6fENV0rOgNUitzMJro1DMfb3cKhE2f4aNUB+k5dwz8+WMOu1v+CkHpwOh1+fNrZpYqIXBUmkwkfDzenvK7lbNQrVqygV69e3H///TRt2pSaNWuye/fua/Z5xalbty5r164t0LZ+/foSH69WrVp4eHjw66+/Otry8vJYv3499evXd7SFhIQwaNAgPv30UyZMmFDgZm5/f3/69u3LtGnTmD17NnPnznXcP1SW6AxQKWpUNYApA2I5m2fl190Z/LQtje83HWb9wePcPiWB6bf+i45L/gFb50KH0RB+beeCEBGRkqlVqxZz585l1apVBAYG8s4775CWllYgJJSGESNGMGTIEFq2bEm7du2YPXs2mzdvpmbNmn+774VPkwE0aNCARx99lKeffpoqVaoQHR3Nm2++SXZ2NoMHDwbs9xnFxsbSsGFDcnJy+OGHHxzjfvfdd4mIiKBZs2aYzWa++uorwsPDqVy58lUd99WgAOQEXu4WOjcIo3ODMJ7qUpfnvt7M0p1HGLQwj19rdiPi9x9h2RvQ93/OLlVERIowbtw49u/fT9euXfHx8eGRRx6hd+/eZGZmlmod/fv3Z9++fTz11FOcPXuWe+65h0GDBhU6K1SUe++9t1Db/v37ef3117HZbAwYMICTJ0/SsmVLFi5cSGBgIAAeHh6MGTOGAwcO4O3tTceOHZk1axYAlSpV4o033mD37t1YLBZatWrFggULMJvL3gUnrQZfhNJeDd5mM3hm7mbmJPxOC680vmY0mMwwegf4hV3zzxcRuVrOrwZf1ErdUjpuvfVWwsPD+d//yud/RF/s39jl/P7WGaAywGw2EdenMTvSsthwCPYFNKBmznb7pbC2jzm7PBERKaOys7OZMmUKXbt2xWKx8MUXX7B48WLi4+OdXVqZV/bOSVVQ7hYzb93dFJMJPjrVyt645UvnFiUiImWayWRiwYIFdOzYkdjYWL7//nvmzp1L586dnV1amaczQGVI/Qh/ejWN5IfEtrzk/j8shzfaJ0msHO3s0kREpAzy9vZm8eLFzi7DJekMUBkz4pbaHMOfDbZa9oY9+octIiJytSkAlTHXhVSifa0gllqb2Rt2KwCJiIhcbQpAZdA9LaNYamsKgLF/GeRffGZRERERuTwKQGVQ14bhHHSvyREjAFPuKUi5+OrCIiIicnkUgMogL3cL7WuHstLW0N5wYIVzCxIRESlnFIDKqJvrhbLqfADav9y5xYiIiJQzCkBlVKe6fwYg4/d1kHvayRWJiMjFdOrUiZEjRzp+rl69OhMmTLjoPiaTiW+//faKP/tqHaciUQAqo8L8vagUdh0pthBMtnxIXu3skkREyqWePXsWO3Hg6tWrMZlMbNiw4bKPu27dOh555JErLa+Al19+mWbNmhVqT01NpXv37lf1sy700UcflclFTUtKAagMa3tdkC6DiYhcY4MHD+aXX37h4MGDhbbNmDGDZs2a0aJFi8s+bkhICD4+PlejxL8VHh6Op6dnqXxWeaEAVIa1rRnEKlsD+w/7ljm3GBGRcur2228nNDSUjz76qEB7dnY2s2fPZvDgwRw9epR+/fpRrVo1fHx8aNy4MV988cVFj3vhJbDdu3dzww034OXlRYMGDYpcr+vZZ5+lTp06+Pj4ULNmTcaNG0deXh5gPwPzyiuvsGnTJkwmEyaTyVHzhZfAtmzZws0334y3tzdBQUE88sgjnDp1yrF90KBB9O7dm3//+99EREQQFBTEsGHDHJ9VEsnJyfTq1YtKlSrh7+/PPffcwx9//OHYvmnTJm666Sb8/Pzw9/cnNjaW9evXA3Dw4EF69uxJYGAgvr6+NGzYkAULFpS4lkuhpTDKsNY1qjDOOHcfUOomTGeOg3egk6sSEbkMhgF52c75bHcfMJn+tpubmxsDBw7ko48+4sUXX8R0bp+vvvqK3Nxc+vfvT3Z2NrGxsTz77LP4+/szf/58BgwYQM2aNWnTps3ffobNZqNPnz4EBwezZs0asrKyCtwvdJ6fnx8fffQRkZGRbNmyhSFDhuDn58czzzxD37592bp1Kz/99JNj+YuAgIBCx8jOzqZbt25cf/31rFu3jvT0dB5++GGGDx9eIOQtWbKEiIgIlixZwp49e+jbty/NmjVjyJAhfzueCxmGQe/evfH19WXZsmXk5+fz2GOP0bdvX5YuXQpA//79ad68OZMnT8ZisZCYmIi7uzsAw4YNIzc3l+XLl+Pr68v27dupVKnSZddxORSAyrDKPh4Ehcew52gktcyH4cBKqH+7s8sSEbl0ednwWqRzPvv5w+Dhe0ldH3roId566y2WLl3KTTfdBNgvf/Xp04fAwEACAwN56qmnHP1HjBjBTz/9xFdffXVJAWjx4sUkJSVx4MABqlWrBsBrr71W6L6dF154wfG+evXqPPnkk8yePZtnnnkGb29vKlWqhJubG+Hh4cV+1meffcaZM2f45JNP8PW1j3/ixIn07NmTN954g7CwMAACAwOZOHEiFouFevXqcdttt/Hzzz+XKAAtXryYzZs3s3//fqKiogD43//+R8OGDVm3bh2tWrUiOTmZp59+mnr16gFQu3Ztx/7JycncddddNG7cGICaNWtedg2XS5fAyrjrawb9OR+Q7gMSEbkm6tWrR7t27ZgxYwYAe/fuZcWKFTz00EMAWK1WXn31VZo0aUJQUBCVKlVi0aJFJCcnX9Lxk5KSiI6OdoQfgLZt2xbqN2fOHDp06EB4eDiVKlVi3Lhxl/wZf/2spk2bOsIPQPv27bHZbOzcudPR1rBhQywWi+PniIgI0tPTL+uz/vqZUVFRjvAD0KBBAypXrkxSUhIAo0eP5uGHH6Zz5868/vrr7N2719H38ccf51//+hft27fnpZdeYvPmzSWq43LoDFAZ1/a6IOasbsgDxCsAiYjrcfexn4lx1mdfhsGDBzN8+HDef/99Zs6cSUxMDLfccgsAb7/9Nu+++y4TJkygcePG+Pr6MnLkSHJzL22pIsMwCrWZLrg8t2bNGu69915eeeUVunbtSkBAALNmzeLtt9++rHEYhlHo2EV95vnLT3/dZrPZLuuz/u4z/9r+8ssvc9999zF//nx+/PFHXnrpJWbNmsWdd97Jww8/TNeuXZk/fz6LFi0iLi6Ot99+mxEjRpSonkuhM0BlXOvqVVhr1MdmmOBIEpwqWToXEXEKk8l+GcoZr0u4/+ev7rnnHiwWC59//jkff/wxDz74oOOX94oVK+jVqxf3338/TZs2pWbNmuzevfuSj92gQQOSk5M5fPjPMLh6dcHpTVauXElMTAxjx46lZcuW1K5du9CTaR4eHlit1r/9rMTERE6f/nP+uJUrV2I2m6lTp84l13w5zo8vJSXF0bZ9+3YyMzOpX7++o61OnTqMGjWKRYsW0adPH2bOnOnYFhUVxdChQ/n666958sknmTZt2jWp9TwFoDIuwMedyIiqbDdi7A06CyQick1UqlSJvn378vzzz3P48GEGDRrk2FarVi3i4+NZtWoVSUlJ/N///R9paWmXfOzOnTtTt25dBg4cyKZNm1ixYgVjx44t0KdWrVokJycza9Ys9u7dy3vvvcc333xToE/16tXZv38/iYmJZGRkkJOTU+iz+vfvj5eXFw888ABbt25lyZIljBgxggEDBjju/ykpq9VKYmJigdf27dvp3LkzTZo0oX///mzYsIG1a9cycOBAbrzxRlq2bMmZM2cYPnw4S5cu5eDBg6xcuZJ169Y5wtHIkSNZuHAh+/fvZ8OGDfzyyy8FgtO1oADkAtrUCGL1+cfhD/zq3GJERMqxwYMHc/z4cTp37kx0dLSjfdy4cbRo0YKuXbvSqVMnwsPD6d279yUf12w2880335CTk0Pr1q15+OGHefXVVwv06dWrF6NGjWL48OE0a9aMVatWMW7cuAJ97rrrLrp168ZNN91ESEhIkY/i+/j4sHDhQo4dO0arVq24++67ueWWW5g4ceLl/WUU4dSpUzRv3rzAq0ePHo7H8AMDA7nhhhvo3LkzNWvWZPbs2QBYLBaOHj3KwIEDqVOnDvfccw/du3fnlVdeAezBatiwYdSvX59u3bpRt25dJk2adMX1XozJKOrCZAWXlZVFQEAAmZmZ+Pv7O7scFmxJ5YcvJjHJ4z2IbAGPLHF2SSIiRTp79iz79++nRo0aeHl5ObscKYcu9m/scn5/6wyQC4iNCWSbUR0A449tYC35RFUiIiKiAOQSwvy9sPrHkGV4Y7LmQMYuZ5ckIiLi0hSAXERsjSC2nzsLROomp9YiIiLi6hSAXERsTCDbbNXtPygAiYiIXBEFIBfRIjqQrecCkHFYAUhEyjY9XyPXytX6t6UA5CLqhfux1+06AGxpm6GEs3WKiFxL52cXzs520gKoUu6dn337r8t4lISWwnARbhYzAVENOPu7O155p+HYPgiu5eyyREQKsFgsVK5c2bGmlI+PT7HLMohcLpvNxpEjR/Dx8cHN7coijAKQC2keE0xSSgzNTXsgNVEBSETKpPMrlZd0YU2RizGbzURHR19xsFYAciHNoiuz1Vad5uY99huhG9/t7JJERAoxmUxEREQQGhpKXp7mLZOry8PDA7P5yu/gUQByIfXC/Vlo1ADAlrpZN3CJSJlmsViu+D4NkWtFv0NdSESAFwfc7TdCG4c3gp6yEBERKREFIBdiMpmwhDUgz7BgycmEzBRnlyQiIuKSFIBczHURQewyqtl/0ISIIiIiJeL0ADRp0iTHiq6xsbGsWLGi2L6pqancd9991K1bF7PZzMiRIy967FmzZmEymejdu/fVLdqJ6ob7sdVmvw9IAUhERKRknBqAZs+ezciRIxk7diwbN26kY8eOdO/eneTk5CL75+TkEBISwtixY2natOlFj33w4EGeeuopOnbseC1Kd5r6EX5sdawJttmptYiIiLgqpwagd955h8GDB/Pwww9Tv359JkyYQFRUFJMnTy6yf/Xq1fnPf/7DwIEDCQgIKPa4VquV/v3788orr1CzZs1rVb5T1Anzc6wJZjuc6NRaREREXJXTAlBubi4JCQl06dKlQHuXLl1YtWrVFR17/PjxhISEMHjw4Evqn5OTQ1ZWVoFXWeXn5U5WQF1shgnz6T/gZJqzSxIREXE5TgtAGRkZWK1WwsLCCrSHhYWRllbyX+orV65k+vTpTJs27ZL3iYuLIyAgwPGKiooq8eeXhuoRIew1Iu0/6DKYiIjIZXP6TdAXTmVtGEaJp7c+efIk999/P9OmTSM4OPiS9xszZgyZmZmOV0pK2X68vF64/1/uA9KN0CIiIpfLaTNBBwcHY7FYCp3tSU9PL3RW6FLt3buXAwcO0LNnT0eb7dyq6W5ubuzcuZPrrruu0H6enp54enqW6DOdoW64H4m26txpWQlpCkAiIiKXy2lngDw8PIiNjSU+Pr5Ae3x8PO3atSvRMevVq8eWLVtITEx0vO644w5uuukmEhMTy/ylrUtVP8KPbeeWxDB0BkhEROSyOXUtsNGjRzNgwABatmxJ27ZtmTp1KsnJyQwdOhSwX5o6dOgQn3zyiWOfxMREAE6dOsWRI0dITEzEw8ODBg0a4OXlRaNGjQp8RuXKlQEKtbuy6kG+7DbbA5DpRDJkHwOfKk6uSkRExHU4NQD17duXo0ePMn78eFJTU2nUqBELFiwgJiYGsE98eOGcQM2bN3e8T0hI4PPPPycmJoYDBw6UZulO5WYxEx4axr6McGqa0+BQAtS+1dlliYiIuAyTYWhFzQtlZWUREBBAZmYm/v7+zi6nSKO/TKTdlnHcbVkONzwNN7/g7JJERESc6nJ+fzv9KTApmfrh/myw1bb/kLLWucWIiIi4GAUgF1U33O/PAHQoAWxW5xYkIiLiQhSAXFS9cD92GdU4aXhD7in4Y5uzSxIREXEZCkAuKsTPk8q+Xn+eBUpe7dyCREREXIgCkIsymUzUDfNjja2BvWH/cucWJCIi4kIUgFxYvQg/1tjq2384uBLOzXotIiIiF6cA5MLqhfuxxajBWZMXnDkO6dudXZKIiIhLUAByYfXC/cnHjQ3UszfsW+rUekRERFyFApALqxPmh8kEP+eeW+Zjz2LnFiQiIuIiFIBcmLeHhepBviy1NbU3HFwJOaecW5SIiIgLUABycY2rBrDXiCTTMxKsuXBghbNLEhERKfMUgFxc8+jKgIkEj1h7w+54Z5YjIiLiEhSAXFyzqMoAfHe6ob1hTzxofVsREZGLUgBycQ0i/fGwmFmUXQfD7AEnkiFjt7PLEhERKdMUgFycp5uFBpH+nMGLI0Et7Y17dBlMRETkYhSAyoHzl8E2eJwLQLoPSERE5KIUgMoB+43QMC/73H1AehxeRETkohSAyoHzZ4AWp/tjBETrcXgREZG/oQBUDkRX8aGKrwe5VoOMiBvsjboMJiIiUiwFoHLAZDI5zgJt9Gxtb9Tj8CIiIsVSAConYmMCAZiXWRMsehxeRETkYhSAyokOtYIBWLY/G1tMe3ujHocXEREpkgJQOdGoagAB3u6czMnncPC5ALR7kXOLEhERKaMUgMoJi9lEu+uCAFhibWZvPLhKj8OLiIgUQQGoHOlQ234Z7PvffaGyHocXEREpjgJQOXL+PqANKSfIq3GLvVGPw4uIiBSiAFSOxAT5ElXFm3ybwfZK19sb9Ti8iIhIIQpA5cz5s0ALTtb+83H4o3udXJWIiEjZogBUznSoFQLAkv2nIKqNvXHfEidWJCIiUvYoAJUz7a4LwmSCXX+c4mS1jvbGfUudWpOIiEhZowBUzgT6etAoMgCA9aYm9sb9y8Ga78SqREREyhYFoHLo/OPwPxwJBa/KkJMFhzc4tygREZEyRAGoHDp/I/SKvccxat5ob9RlMBEREQcFoHIoNiYQTzcz6SdzSA9ua2/cqxuhRUREzlMAKoe83C20rlEFgGX5jeyNv6+FnJNOrEpERKTsUAAqp85fBvvpsBcEVgdbPhz41blFiYiIlBEKQOXU+Ruh1+w7irXm+WUxtDq8iIgIKACVW/XD/Qny9SA718qeyu3sjbu1LIaIiAgoAJVbZrOJducugy08VRvcvCAzBdKTnFyZiIiI8ykAlWMdagUBsHT/Kahxg71x5wInViQiIlI2KACVYx1q29cF2/R7Jmdqdbc3bv/WeQWJiIiUEQpA5VjVyt7UCPbFajNY7d4WTBZI26LV4UVEpMJTACrnzj8OvzTFCjU72Ru3feO8gkRERMoABaBy7vzj8L/uzoCGve2N2751Wj0iIiJlgQJQOXd9zSDMJtiXcZrUiFvA7AZ/bIGM3c4uTURExGkUgMq5AG93mkZVBmDF77a/XAb71lkliYiIOJ0CUAXgWB1+TwY0vNPeqKfBRESkAnN6AJo0aRI1atTAy8uL2NhYVqxYUWzf1NRU7rvvPurWrYvZbGbkyJGF+kybNo2OHTsSGBhIYGAgnTt3Zu3atddwBGXf+QC0ak8Gtjo9zl0G2wpHdjm5MhEREedwagCaPXs2I0eOZOzYsWzcuJGOHTvSvXt3kpOTi+yfk5NDSEgIY8eOpWnTpkX2Wbp0Kf369WPJkiWsXr2a6OhounTpwqFDh67lUMq05tGB+HhYOHo6l6RMC9S8yb5BZ4FERKSCMhmG8xaHatOmDS1atGDy5MmOtvr169O7d2/i4uIuum+nTp1o1qwZEyZMuGg/q9VKYGAgEydOZODAgZdUV1ZWFgEBAWRmZuLv739J+5R1D85cy5KdR3i+Rz0e8VsD3z0GoQ3gsdXOLk1EROSquJzf3047A5Sbm0tCQgJdunQp0N6lSxdWrVp11T4nOzubvLw8qlSpUmyfnJwcsrKyCrzKm/OzQq/YnQH1eoDZHdK3w5GdTq5MRESk9DktAGVkZGC1WgkLCyvQHhYWRlpa2lX7nOeee46qVavSuXPnYvvExcUREBDgeEVFRV21zy8rzt8HtHb/Mc66+cN1N9s36GkwERGpgJx+E7TJZCrws2EYhdpK6s033+SLL77g66+/xsvLq9h+Y8aMITMz0/FKSUm5Kp9fltQJq0SInyc5+TY2HDz+l0kRvwbnXQUVERFxCqcFoODgYCwWS6GzPenp6YXOCpXEv//9b1577TUWLVpEkyZNLtrX09MTf3//Aq/yxmQyOc4CLd+dAXV7gMUTjuyAwxudXJ2IiEjpcloA8vDwIDY2lvj4+ALt8fHxtGvX7oqO/dZbb/HPf/6Tn376iZYtW17RscqTTnXt9wH9nPQHeFeG+j3tGzZ+6ryiREREnMCpl8BGjx7Nhx9+yIwZM0hKSmLUqFEkJyczdOhQwH5p6sIntxITE0lMTOTUqVMcOXKExMREtm/f7tj+5ptv8sILLzBjxgyqV69OWloaaWlpnDp1qlTHVhZ1qhuKm9nE7vRT7M84Dc3vt2/YMgfyzji3OBERkVLk5swP79u3L0ePHmX8+PGkpqbSqFEjFixYQExMDGCf+PDCOYGaN2/ueJ+QkMDnn39OTEwMBw4cAOwTK+bm5nL33XcX2O+ll17i5ZdfvqbjKesCvN1pU7MKK/ccJX57Go90uBEqR8OJZNg+D5r2dXaJIiIipcKp8wCVVeVxHqDzPlq5n5e/306r6oF8NbQdLH0Dlr4G1TvCoB+cXZ6IiEiJucQ8QOIcnRvYbzBPOHico6dyoNl9gAkOrIBj+5xbnIiISClRAKpgqgX60CDCH5sBP+9Ih8pRf84JpJuhRUSkglAAqoC6NLSfBVq07Q97w/mboRM/B5vVSVWJiIiUHgWgCqhLg3AAlu8+QtbZPKh3G3hXgZOpsPcXJ1cnIiJy7SkAVUD1I/yoFVqJ3HwbC7emgZsnNLrLvlErxIuISAWgAFQBmUwmejWNBGDepsP2xvq32//c+SNY851UmYiISOlQAKqgep4LQCv3ZHDkZA7EtAevypB9FFLWOLc4ERGRa0wBqIKqHuxL06jK2AyYv/kwWNyhbnf7xiTNByQiIuWbAlAFVugyWL1zl8F2/KAV4kVEpFxTAKrAbm8SgdkEG5JPkHw02z4fkJs3ZKZA6iZnlyciInLNKABVYKH+XrSvFQzANxsPgYcP1LrFvnGHLoOJiEj5pQBUwd3ZvCoA32z8HcMwoH5P+wbdByQiIuWYAlAF17VhON7uFg4czWZjygmo0xXMbnAkCY7udXZ5IiIi14QCUAXn6+lGt0b2maG/2XAIvAPtK8MDJH3vxMpERESuHQUgcVwG+37zYXLzbX9OiqhZoUVEpJxSABLa1wom1M+TE9l5LNmZDvV72S+DHd4I6UnOLk9EROSqUwASLGYTvc/fDL3hEFQKgdpd7Rs3furEykRERK4NBSAB/rwM9suOdDKz86B5f/uGTbMg76wTKxMREbn6FIAEgPoR/tQL9yPXamP+llSo3QX8q0F2Bmz50tnliYiIXFUKQOJwfoHUX3ak29cGu36ofcPq97U0hoiIlCsKQOJwY50QAFbvzbA/DdZiIHj4wZEdsGexk6sTERG5ehSAxKFBhD9Bvh6czrWyMfk4eAXYQxDA6onOLU5EROQqUgASB7PZRIfa9rXBlu8+Ym+8fiiYzLBvqf2xeBERkXJAAUgK6FTXfhls0bY/7A2Vo6HR3fb3S99wUlUiIiJXlwKQFHBL/TA8LGZ2p59i1x8n7Y03Pms/C7TrRzi0wbkFioiIXAUKQFKAv5c7Hc9dBpu/OdXeGFwLGt9jf780zkmViYiIXD0KQFJIj8YRAHyXeAjj/OPvNz4DJgvsXgS/r3didSIiIldOAUgK6dYoHF8PCweOZrNm3zF7Y9B10PRe+3udBRIRERenACSF+Hq6cUcz+9IYs9Yl/7nhhqfsZ4H2LIaUtU6qTkRE5MopAEmR+rWOAuDHLWkcP51rb6xSE5r1s7/XWSAREXFhCkBSpMZVA2gY6U+u1cbXGw/9ueGGp8HsBnt/geTfnFegiIjIFVAAkiKZTCbubR0NwKy1yX/eDB1YHZrdZ3//83itESYiIi5JAUiK1atZJD4eFnann+LXPRl/brjhGXDzgoO/wo4fnFegiIhICSkASbH8vdy5p6X9XqCpy/f9uaFyFLQbYX+/6AXIz3FCdSIiIiWnACQXNbhDDSxmEyt2Z7DtcOafG9qPhErhcPwA/DbFWeWJiIiUiAKQXFRUFR/HxIjT/noWyLMSdH7J/n7ZW3DqiBOqExERKRkFIPlb/3dDTQC+35zK78ez/9zQ5F6IaAa5J2HJq84pTkREpAQUgORvNaoaQLvrgrDaDGb8euDPDWYzdDs3H9CGjyFtq1PqExERuVwKQHJJHjl3FmjWumQys/P+3BDTDhr0BsMGC5/XY/EiIuISFIDkktxYJ4R64X5k51r5ePWBghtvfQUsnrB/Gez6ySn1iYiIXA4FILkkJpOJRztdB8CHK/aRdfYvZ4ECq8P1j9rfx78I1vzSL1BEROQyKADJJbu9SSS1QiuRdTafmX+9Fwig42jwrgIZu2DjJ06pT0RE5FIpAMkls5hNPHFLbQA+/HUfmWf+chbIKwBufNb+fkkc5Jx0QoUiIiKXRgFILsttjSOoE1aJk2fzmf7r/oIbWz5kXzH+dDp8/QjkZhd9EBERESdTAJLLYjabGNm5DgBTl+/l0Ikzf25084Db37XfEL1zAcwf7aQqRURELk4BSC5b90bhtK5RhbN5Nl6dv73gxpqdoP9X9vebvoBDG0q9PhERkb9TogCUkpLC77//7vh57dq1jBw5kqlTp161wqTsMplMvHJHQyxmEwu2pLHyryvFA9S80T5LNMDCsZobSEREypwSBaD77ruPJUuWAJCWlsatt97K2rVref755xk/fvxVLVDKpvoR/gy4PgaAl+ZtI89qK9jhlnHg5g3JqyDpeydUKCIiUrwSBaCtW7fSunVrAL788ksaNWrEqlWr+Pzzz/noo48u61iTJk2iRo0aeHl5ERsby4oVK4rtm5qayn333UfdunUxm82MHDmyyH5z586lQYMGeHp60qBBA7755pvLqkkuzahb6xDk68Ge9FN8vOpAwY0B1aDdCPv7+BchP6fU6xMRESlOiQJQXl4enp6eACxevJg77rgDgHr16pGamnrJx5k9ezYjR45k7NixbNy4kY4dO9K9e3eSk5OL7J+Tk0NISAhjx46ladOmRfZZvXo1ffv2ZcCAAWzatIkBAwZwzz338Ntvv13mKOXvBHi780y3ugBMWLyb9KyzBTu0fwIqhcHx/bB2mhMqFBERKZrJMC7/Bo02bdpw0003cdttt9GlSxfWrFlD06ZNWbNmDXfffXeB+4P+7jgtWrRg8uTJjrb69evTu3dv4uLiLrpvp06daNasGRMmTCjQ3rdvX7Kysvjxxx8dbd26dSMwMJAvvvjikurKysoiICCAzMxM/P39L2mfispmM7hz0ko2/Z5Jn+ZVeadvs4IdNvwP5g0HzwB4bDUEVHVKnSIiUv5dzu/vEp0BeuONN/jggw/o1KkT/fr1c5yNmTdvnuPS2N/Jzc0lISGBLl26FGjv0qULq1atKklZgP0M0IXH7Nq160WPmZOTQ1ZWVoGXXBqz2cT4Xo0wmeDrjYdYf+BYwQ7N7oPIFpCTCd8NA5ut6AOJiIiUohIFoE6dOpGRkUFGRgYzZsxwtD/yyCNMmTLlko6RkZGB1WolLCysQHtYWBhpaWklKQuw35R9uceMi4sjICDA8YqKiirx51dETaMqc0+s/e/sxe+2YbX95aSi2QJ9ptpviN63BNbpUpiIiDhfiQLQmTNnyMnJITAwEICDBw8yYcIEdu7cSWho6GUdy2QyFfjZMIxCbZfrco85ZswYMjMzHa+UlJQr+vyK6JludfH3cmN7ahYzV14wQ3RwbejyT/v7+BfhyM7SL1BEROQvShSAevXqxSef2Be8PHHiBG3atOHtt9+md+/eBe7nuZjg4GAsFkuhMzPp6emFzuBcjvDw8Ms+pqenJ/7+/gVecnmCKnnybPd6ALy1cCd7j5wq2KHVw3DdLZB/Fr4eAvm5TqhSRETErkQBaMOGDXTs2BGAOXPmEBYWxsGDB/nkk0947733LukYHh4exMbGEh8fX6A9Pj6edu3alaQsANq2bVvomIsWLbqiY8qlua91NB1rB5OTb2PM11uw/fVSmMkEvd4H70BI3QTL3nBeoSIiUuGVKABlZ2fj5+cH2MNFnz59MJvNXH/99Rw8ePCSjzN69Gg+/PBDZsyYQVJSEqNGjSI5OZmhQ4cC9ktTAwcOLLBPYmIiiYmJnDp1iiNHjpCYmMj27X8ux/DEE0+waNEi3njjDXbs2MEbb7zB4sWLi50zSK4ek8nEa3c2xtvdwtr9x/gq4YJLif4RcPsE+/tf34FkTU0gIiLOUaIAVKtWLb799ltSUlJYuHCh46mr9PT0y7p81LdvXyZMmMD48eNp1qwZy5cvZ8GCBcTE2GcYTk1NLTQnUPPmzWnevDkJCQl8/vnnNG/enB49eji2t2vXjlmzZjFz5kyaNGnCRx99xOzZs2nTpk1JhiqXKaqKD6NvtS+W+tqCHRw5ecEEiA17Q9N+YNjgm0cg51Thg4iIiFxjJZoHaM6cOdx3331YrVZuvvlmxyWnuLg4li9fXmAOHlekeYCuTL7VRq/3V7LtcBZ3NI3kvX7NC3Y4mwmT20Nmin3NsDun2C+RiYiIXIFrPg/Q3XffTXJyMuvXr2fhwoWO9ltuuYV33323JIeUcsTNYiauT2PMJpi36TC/7PijYAevALjzAzBZYPMsWP2+cwoVEZEKq0QBCOxPWzVv3pzDhw9z6NAhAFq3bk29evWuWnHiuppUq8xD7WsA8MI3WzmVk1+wQ/X20PU1+/v4cbB7cSlXKCIiFVmJApDNZmP8+PEEBAQQExNDdHQ0lStX5p///Cc2zfQr54zuUoeoKt4czjzLWz/tKNyhzf9Bi4H2+4HmPAQZu0u/SBERqZBKFIDGjh3LxIkTef3119m4cSMbNmzgtdde47///S/jxo272jWKi/LxcOO1OxsD8Mmag6y7cJkMkwl6vA3Rbe1LZXxxr/3+IBERkWusRDdBR0ZGMmXKFMcq8Od99913PPbYY45LYq5KN0FfXU9/tYmvEn6nRrAvCx7viLeHpWCHU0dgaifI+h3q3Q73/A/MJb46KyIiFdQ1vwn62LFjRd7rU69ePY4dO1bEHlKRvXB7A8L8PdmfcZq3FxWxDEalEOj7CVg8YMcP9tXjrfmF+4mIiFwlJQpATZs2ZeLEiYXaJ06cSJMmTa64KClfArzdietjvxQ2feV+Eg4WEZKrxkLvyfYnwxI/s98YLSIico2U6BLYsmXLuO2224iOjqZt27aYTCZWrVpFSkoKCxYscCyT4ap0CezaePLLTczd8Ds1Q+yXwrzcLYU7bZ1rvyEa4I6J0GJA6RYpIiIu65pfArvxxhvZtWsXd955JydOnODYsWP06dOHbdu2MXPmzBIVLeXfi7c3INTPk31HThO3IKnoTo3ugk5j7O9/GAUHV5degSIiUmGU6AxQcTZt2kSLFi2wWq1X65BOoTNA187SnekMmrkOgA8GxNK1YXjhTjYbzBkE278Dn2B4ZClUjirVOkVExPVc8zNAIiXVqW4oj9xQE4Bn5mzm0IkzhTuZzfb7gcIbQ3YGfNwTMvaUcqUiIlKeKQBJqXuqS12aRlUm80wej3+xkXxrEZNnevhCv1lQORqO74ePboOTfxTuJyIiUgIKQFLqPNzM/Pfe5vh5upFw8DjvLt5VdMeAavDwzxBcF06lwZwHIe9s6RYrIiLlktvldO7Tp89Ft584ceJKapEKJDrIh7i7GjP8841MWrqXtjWD6VA7uHDHSqHQ91OYdhMcXAmz+8PdM8FL92aJiEjJXdYZoICAgIu+YmJiGDhw4LWqVcqZ25tE0q91NIYBI2cncuRkTtEdQ+rAfbPBzRv2LIapN8KJ5NItVkREypWr+hRYeaGnwErPmVwrvd7/lV1/nKJj7WA+frA1ZrOp6M4p6+yXwTJTILQBPPQTeAWUbsEiIlJm6SkwcRneHhYm3tcCL3czK3Zn8MHyfcV3jmplDz2VwiF9O8zsAVmppVesiIiUGwpA4nR1wvx4uWdDAP69aCcJB48X3zmgGtw/B3xD4Y+t8EVfyM0upUpFRKS8UACSMqFvqyh6No3EajN4/IuNZGbnFd85vDEMXmSfJDF1E3z3GOhKroiIXAYFICkTTCYTr93ZiOgqPhw6cYZn527morenValhfzrM7A7bvoFlb5ResSIi4vIUgKTM8PNyZ+J9zXG3mPhpWxofrth/8R1i2sLt79jfL42DhI+ueY0iIlI+KABJmdKkWmXGdK8PwKsLkvh+0+GL79BiIHR80v7+h1GQ9P01rlBERMoDBSApcx5sX51B7aoD8OSXm1i99+jFd7h5nD0IGTaYMxh+X3/tixQREZemACRljslkYtztDejeKJxcq41H/reeHWlZF9sBbnsX6nQDaw7M6g9Zf3PmSEREKjQFICmTLGYT7/ZtRuvqVTh5Np9BM9ZxuKiV4x07uMFdH9onSDyVBrPug7yL9BcRkQpNAUjKLC93C9MGtqR2aCXSss7ywIy1HD1VzHIZAJ5+0O8L8K4ChzfCd8P0eLyIiBRJAUjKtAAfdz56qDXh/l7sTj/FgOlrOZGdW/wOgdWh7//A7AZb58KKt0utVhERcR0KQFLmVa3szWdD2hBcyZPtqVkMnLGWrLMXmSixegfo8Zb9/S//hB3zS6dQERFxGQpA4hKuC6nE50PaUMXXg82/ZzJoxlpO5eQXv0PLh6DVEPv7uUPgj22lU6iIiLgEBSBxGXXC/Pjf4NYEeLuzIfkED320juzci4SgbnFQ4wbIOw1f3AunM0qvWBERKdMUgMSlNIwM4H+DW+Pn6cba/ccY8sl6zuZZi+5scYd/fAyBNeBEMnw1CGzF9BURkQpFAUhcTpNqlfnoodb4eFhYuecoQz9NICe/mGDjUwXumw0eleDAClj9fukWKyIiZZICkLik2JhAZg5qhZe7maU7jzD8843kWW1Fdw6pa78cBvabotO2ll6hIiJSJikAictqUzOIDwe2wsPNTPz2Pxg5K5H84kJQ8wFQtwdYc+HrRyD/Io/Si4hIuacAJC6tQ+1gPhgQi4fFzPwtqTz11SZstiImPzSZoOd74BMM6dtg/YzSL1ZERMoMBSBxeTfVDWXifc1xM5v4NvEwL3+/DaOoGaArhcAt4+zvl70OZ46XbqEiIlJmKABJudClYTjv9G2GyQSfrD7If37eXXTHZvdDSH17+Fn4QukWKSIiZYYCkJQbdzSNZPwdDQGYsHg3s9YmF+5kcYOeEwATJH4KuxaVao0iIlI2KABJuTKgbXUev6U2AC98u5U1+44W7hR9PbQdZn///eNw5kTpFSgiImWCApCUO6M61+b2JhHk2wwe/TSB5KPZhTvdNBaqXAcnU2HJq6VfpIiIOJUCkJQ7JpOJf/+jKU2qBXA8O4/BH6/j5IWLp3r4wO3v2N+vn2mfKVpERCoMBSApl7zcLUwb2JIwf092p59i1OzEwo/H1+xkXyvMlge/6CyQiEhFogAk5VaYvxfTBrbEw83M4qR0Pli+r3CnW14GTLB5FuxZXNolioiIkygASbnWpFplx5Nhby3cweq9F9wUXS0W2gy1v1/wNFgvsrq8iIiUGwpAUu71bRXFXS2qYTPg8VkbOZF9wTIYN78APkFwbB9s+co5RYqISKlSAJJyz2Qy8a/ejagVWokjJ3MY/8P2gh08K0G7Efb3y96AvLOlX6SIiJQqBSCpELw9LLx5dxNMJvh6wyF+2fFHwQ6thkClcDi+3x6CRESkXFMAkgqjRXQgg9vXAOD5r7eS9ddH4z0rwW1v29+v/A+kbnZChSIiUlqcHoAmTZpEjRo18PLyIjY2lhUrVly0/7Jly4iNjcXLy4uaNWsyZcqUQn0mTJhA3bp18fb2JioqilGjRnH2rC5rCDzZpS4xQT6kZZ0lbkFSwY31b4cGvcCwwnfDdEO0iEg55tQANHv2bEaOHMnYsWPZuHEjHTt2pHv37iQnFz0p3f79++nRowcdO3Zk48aNPP/88zz++OPMnTvX0eezzz7jueee46WXXiIpKYnp06cze/ZsxowZU1rDkjLM28PCG3c1AeCLtSms2pNRsEP3t8CrMqRthtX/Lf0CRUSkVJgMwzD+vtu10aZNG1q0aMHkyZMdbfXr16d3797ExcUV6v/ss88yb948kpL+/C/3oUOHsmnTJlavXg3A8OHDSUpK4ueff3b0efLJJ1m7du3fnl06Lysri4CAADIzM/H39y/p8KQMe+HbLXy6Jpmawb78OLIjnm6WPzcmfg7fPgoWT3h0FQTXcl6hIiJyyS7n97fTzgDl5uaSkJBAly5dCrR36dKFVatWFbnP6tWrC/Xv2rUr69evJy/Pfj9Hhw4dSEhIYO3atQDs27ePBQsWcNtttxVbS05ODllZWQVeUr493bUewZU82ZdxmqnLLpggsWk/uO5msObAvBFgszmnSBERuWacFoAyMjKwWq2EhYUVaA8LCyMtLa3IfdLS0orsn5+fT0aG/VLGvffeyz//+U86dOiAu7s71113HTfddBPPPfdcsbXExcUREBDgeEVFRV3h6KSsC/B2Z9zt9QGYuGRPwQVTTSa4fQK4+0LyKkiY6ZwiRUTkmnH6TdAmk6nAz4ZhFGr7u/5/bV+6dCmvvvoqkyZNYsOGDXz99df88MMP/POf/yz2mGPGjCEzM9PxSklJKelwxIXc0TSS9rWCyMm38eK8rRS4GhwYA7e8aH8f/xJk/u6cIkVE5JpwWgAKDg7GYrEUOtuTnp5e6CzPeeHh4UX2d3NzIygoCIBx48YxYMAAHn74YRo3bsydd97Ja6+9RlxcHLZiLmV4enri7+9f4CXln8lkYnyvRnhYzCzdeYSftl5w5rH1EKjWGnJPwtf/p6fCRETKEacFIA8PD2JjY4mPjy/QHh8fT7t27Yrcp23btoX6L1q0iJYtW+Lu7g5AdnY2ZnPBYVksFgzDwIn3e0sZdV1IJf7vxpoAvPL9dk7l/CXkmC3QexJ4+MHBX+GX4s8iioiIa3HqJbDRo0fz4YcfMmPGDJKSkhg1ahTJyckMHWpfnHLMmDEMHDjQ0X/o0KEcPHiQ0aNHk5SUxIwZM5g+fTpPPfWUo0/Pnj2ZPHkys2bNYv/+/cTHxzNu3DjuuOMOLBZLoRpEht1Ui+gq9rmBJsTvKrgxuDb0ft/+fvVEyNhT+gWKiMhV5+bMD+/bty9Hjx5l/PjxpKam0qhRIxYsWEBMTAwAqampBeYEqlGjBgsWLGDUqFG8//77REZG8t5773HXXXc5+rzwwguYTCZeeOEFDh06REhICD179uTVV18t9fGJa/Byt/BKr4Y8OHMdM1cd4K7YatSP+Mtl0Aa9oHYX2L0IFo6BfrPB7PTb50RE5Ao4dR6gskrzAFVMj36awI9b04iNCeSr/2uL2fyXG+7Td8CUDmDLgxuegZvHOq9QEREpkkvMAyRS1rzYswG+HhYSDh7nq4QLngQMrQc9/2N/v/xN2Pxl6RcoIiJXjQKQyDkRAd6MurUOAHE/7uDY6dyCHZr3hw6j7O+/GwbJa0q5QhERuVoUgET+4oF21akX7seJ7DxenZ9UuMPNL0K928GaC7P6w/EDpV6jiIhcOQUgkb9wt5iJ69MYkwnmbvidlRculmo2Q5+pEN4EsjPg83vhrJZOERFxNQpAIhdoHh3IwOvtTyI+/80WzuZZC3bw8IX7ZoNfBBxJgjkPaZJEEREXowAkUoSnutYl3N+Lg0ezeWvhzsId/COh3xfg5g174mGRngoTEXElCkAiRfDzcue1Po0AmP7rflbsPlK4U2Rz6POB/f1vU2Dle6VYoYiIXAkFIJFi3FwvjAHnLoU9+eWmwk+FgX2SRMeiqeNg4VgoZs05EREpOxSARC7i+R71qRVaifSTOTw7d3PR68l1GA23vGR/v3oifD1EIUhEpIxTABK5CG8PC/+5txnuFhPx2//gi7UphTuZTNBxNPT5EMzusHUOrJxQ6rWKiMilUwAS+RsNIwN4pms9AF7+fhubUk4U3bHJP+C2f9vf//yK7gkSESnDFIBELsHgDjXoXD+U3Hwbj/xvPX9knS26Y4sHoN0I+/v4cbBmSukVKSIil0wBSOQSmM0m3u3bjNqhlfgjK4dH/pdAdm4Rc/+YTNDlX3DTucfif3oO1k4DrTksIlKmKACJXCI/L3c+fKAllX3c2ZRygv/7XwI5+daiO9/wNLQcDBiw4Cn4/gnIL+IpMhERcQoFIJHLEBPky/QHWuHjYWHF7gxGfL6RPGsRT3yZTHDb29D5FcAEGz6GuYP1dJiISBmhACRymWJjAvlwYEs83Mws2v4HT3+1CZutiEtcJhN0GGlfNsPiAUnz7JfEFIJERJxOAUikBNrVCmZy/xa4mU18m3iYp+dsJr+oM0EAdbpCz3NPhK39AOYMgrwzpVariIgUpgAkUkK31A9jwr3NsJhNzN3wO0/MSiQ3v5gQ1Kwf3DnVPk/Q9u/go9vgRHLpFiwiIg4KQCJX4PYmkbx/X3PcLSbmb0nl0U8TCq8ef17TvjDwW/CqDIcSYEoH2D6vNMsVEZFzFIBErlC3RhFMG9gSTzczP+9IZ/DH6zidU8Qj8gDVO8D/LYOqLeFsJnw5QBMmiog4gQKQyFXQqW4oHz/UGl8PCyv3HOX+6b+RXtxkiYHV4aGf4PrH7D/Hj4MlcZorSESkFCkAiVwl19cM4tOH2+Dv5cbG5BP0eO9Xth7KLLqzxR26xcHN4+w/L3sdPu0DR3aVXsEiIhWYApDIVdQ8OpBvh7WnXrgfGady6DdtDesPHCt+hxuegu5vgdkN9v4CM7vDkZ2lV7CISAWlACRyldUMqcSXQ9vSunoVTp7N5/7pv7F0Z3rxO7R5BIavh/AmkJ0BH94KO+aXXsEiIhWQApDINeDv5c7HD7WmU90QzubZGPLJer7fdLj4HarUgAHfQrVWkJMJs/rDqv/qviARkWtEAUjkGvH2sDB1QEtubxJBntXg8Vkb+XjVAYziQo1vEDz4I7R6GDBg0Qv2dcTyc0q1bhGRikABSOQa8nAz8597m9O/TTSGAS/N28bTczYXP1eQxR16/Nu+ojzAug/hgxshY3fpFS0iUgEoAIlcYxaziX/1bsTzPephNsGchN/5x5TV/H48u+gdTCZoNwLu/QJ8Q+BIEkzvAoc2lG7hIiLlmAKQSCkwmUw8csN1fDq4DVV8PdhyKJOe//2VJRe7ObpeD3h0NUS2gDPH4NO7IG1r6RUtIlKOKQCJlKJ2tYL5fkQHmlQL4Hh2Hg/OXMcbP+0ofiHVSiHwwLw/Q9D0W2Hzl6VbtIhIOaQAJFLKqlb25quhbXmgbQwAk5fu5b5pv5GWWczM0Z5+cP9cqNkJ8rLh6yHw43NgKyY0iYjI31IAEnECTzcLr/RqxPv3taCSpxtrDxyjx3srWLbrSNE7+FSB+7+GG58FTPDbZJg/CqzFrDkmIiIXpQAk4kS3NYnghxEdaBDhz7HTuQyauZZ/L9xZ9CUxswVueh7unAKYIOEj+OJeOJtV2mWLiLg8BSARJ6se7MvXj7Xj/uvtj8pPXLKH+6b9RsqxYp4Sa3ov3PMxuHnDnnj7E2LHD5Zu0SIiLk4BSKQM8HK38K/ejXmvX3N8PSysPXCMbhOW8+W6lKInTmzQCx5cAJXC7Y/JT7sZdv5U+oWLiLgoBSCRMuSOppH8+MQNtKoeyOlcK8/M3cyQT9Zz5GQRs0FXbQFDfvlzDbEv+sIvr5Z+0SIiLkgBSKSMiQ7yYdYjbXm+Rz08LGYWJ6XTdcJyfk76o3DngKrw0EJoO9z+8/I3If4lsBUz07SIiAAKQCJlksVsnzjx+7/cID344/XELUgi78IbpD18oOur0PkV+88rJ8AnveBkEYFJREQABSCRMq1uuB/fDmvPg+2rA/DB8n3cO3UNh0+cKdy5w0i4azq4+8KBFTCpDWz8VCvKi4gUQQFIpIzzcDPzUs+GTLm/BX5ebiQcPM5t761gyY4iltFofDc8shTCGsGZ4/DdMPj+CV0SExG5gAKQiIvo1iiC+SM60rjquWU0PlpH3IKkwivLh9Sxh6BbXgSTGTZ8DAvHOqVmEZGySgFIxIVEB/kw59E/l9H4YPk+uk5Yzu4/ThbsaHGHjk/CXR/af/5tMsx9WJMmioicowAk4mLOL6PxwYBYIgK8OHg0m75T17Bqb0bhzo3ugu5vgskCW76CT/vA2czSL1pEpIxRABJxUV0bhrPg8Y40rRbAsdO59P/wN/6zeHfhiRPb/B889BN4VYbf18GktrB/uVNqFhEpKxSARFxYoK8HXzxyPf1aR2EY8O7iXfT/8Dd2pF1wqSuqNTwwDyrHQNYh+OweSP7NOUWLiJQBCkAiLs7Hw424Pk2I69MYD4uZVXuPcsfElXyy+kDBs0ERTeGxNVDrVsg/Ax/fDive0YryIlIhKQCJlBP9Wkfz85M3cnO9UHLzbbz43TaGfprAsdO5f3by8LEvpFqnG1hz4edXYEZXOH3UeYWLiDiB0wPQpEmTqFGjBl5eXsTGxrJixYqL9l+2bBmxsbF4eXlRs2ZNpkyZUqjPiRMnGDZsGBEREXh5eVG/fn0WLFhwrYYgUmZEVfFh+gMteeG2+rhbTCzc9gdd3l3O4u1/mRXawxf6zYLeU8ArAA6th497QvoO5xUuIlLKnBqAZs+ezciRIxk7diwbN26kY8eOdO/eneTk5CL779+/nx49etCxY0c2btzI888/z+OPP87cuXMdfXJzc7n11ls5cOAAc+bMYefOnUybNo2qVauW1rBEnMpkMvFwx5p881h7aodWIuNUDg9/sp5n52wmMzvvfCdo1g8e/hkqhUH6NpjS3r6OWF4Rs0yLiJQzJqPQIyOlp02bNrRo0YLJkyc72urXr0/v3r2Ji4sr1P/ZZ59l3rx5JCUlOdqGDh3Kpk2bWL16NQBTpkzhrbfeYseOHbi7u5eorqysLAICAsjMzMTf379ExxApC87mWXl70U4+/HU/hgGBPu6MvrUO/VpH42Y5998/J1Lgx2dg57mzpLW7wL2f2+cSEhFxIZfz+9tpZ4Byc3NJSEigS5cuBdq7dOnCqlWritxn9erVhfp37dqV9evXk5dn/y/befPm0bZtW4YNG0ZYWBiNGjXitddew2otfimAnJwcsrKyCrxEygMvdwtjb2vArCHXUzu0Esez8xj33Ta6/2cFy3YdsXeqHAX9voC+n4GbN+xeZF8+Q2uIiUg55rQAlJGRgdVqJSwsrEB7WFgYaWlpRe6TlpZWZP/8/HwyMuyTwO3bt485c+ZgtVpZsGABL7zwAm+//TavvvpqsbXExcUREBDgeEVFRV3h6ETKljY1g/jxiY78s3cjAn3c2Z1+igdmrGXQzLXsST83i3T92+EfH9knTUz8zB6CNHO0iJRTTr8J2mQyFfjZMIxCbX/X/6/tNpuN0NBQpk6dSmxsLPfeey9jx44tcJntQmPGjCEzM9PxSklJKelwRMosN4uZAdfHsPTpm3i4Qw3cLSaW7jxCj//8yjcbf7d3qtsNek6wv9/wMXzQEY7udVrNIiLXitMCUHBwMBaLpdDZnvT09EJnec4LDw8vsr+bmxtBQUEAREREUKdOHSwWi6NP/fr1SUtLIzc3l6J4enri7+9f4CVSXgV4u/PC7Q1YNOpGbqgTQq7VxqjZm7jng9UkpWZBi4Fw/1wIiIbjB2B6Fzi80dlli4hcVU4LQB4eHsTGxhIfH1+gPT4+nnbt2hW5T9u2bQv1X7RoES1btnTc8Ny+fXv27NmDzWZz9Nm1axcRERF4eHhc5VGIuK4awb58NKgVw2+qhYfFzNr9x7hj4q9M/GU3+TVuhocXQ3hjyM6Aj26HvUucXbKIyFXj1Etgo0eP5sMPP2TGjBkkJSUxatQokpOTGTp0KGC/NDVw4EBH/6FDh3Lw4EFGjx5NUlISM2bMYPr06Tz11FOOPo8++ihHjx7liSeeYNeuXcyfP5/XXnuNYcOGlfr4RMo6s9nEU13rsuyZTtzaIIw8q8G/F+3izkmr2HnaBwYtgBo3QO4p+OwfsPFTZ5csInJVOPUxeLBPhPjmm2+SmppKo0aNePfdd7nhhhsAGDRoEAcOHGDp0qWO/suWLWPUqFFs27aNyMhInn32WUdgOm/16tWMGjWKxMREqlatyuDBg3n22WcLXBa7GD0GLxWRYRh8m3iIl77bRtbZfNwtJh698Toe6xiF1w+PwbZv7B07vwwdRjm1VhGRolzO72+nB6CySAFIKrI/ss4y9pstLE5KB6BOWCWmD4wlKvFdWPFve6dWQ+Cm58GnihMrFREpyCXmARKRsinM34tpA1syqX8Lgit5suuPU9zx/io+9OhPXvtzl5vXTYOZPeDMCafWKiJSUgpAIlKIyWSiR+MIfhjRgQYR/hzPzuNf85No+9v1bLhxJlQKhyNJ8EkvyNjt7HJFRC6bApCIFCs8wIvvhrfnzbuaEF3Fh4xTudy1yJMZNd7C8AqA1ESY2gl2/uTsUkVELosCkIhclLvFzD2tovj5yRu5//poDAPGr7Nwr/ltssKvtz8h9sW9sOq/Wj5DRFyGApCIXBJ3i5l/9W7MBwNiCfXz5LdjPrQ48Bhrg3oBBix6AeYNh7yzzi5VRORvKQCJyGXp2jCc+NE30q91FPm4cc+he3jL9CA2zPZ5gqa0h4OrnV2miMhFKQCJyGUL8HYnrk8TPh/ShupBvrx/5lYG5T7NUVMgHN0DM7vBL//SJTERKbMUgESkxNpdF0z86BuZcn8se/2v56YzbzLbepN94/K34LvhYM1zbpEiIkVQABKRK+JuMdOtUTgLnuhIp6a1eTZvCM/kDcGKGRI/hc/7wpnjzi5TRKQABSARuSoCvN15r19zJvRtxgK3WxmSO5ozhgfs/RljSkfYHf/3BxERKSUKQCJyVfVuXpUFj3fkbI1buTv3ZVJsIZgyU+Czu2HROMjPdXaJIiIKQCJy9UUH+fDZw214rF8f7jK/zcz8rvYNq97DOrkDHFjp3AJFpMJTABKRa8JkMnFbkwh+eLIrKW1eZljeE2QY/liO7oSPepC/6EWw2ZxdpohUUApAInJNhfp58WLPBgwZOpqH/Sbxef7NALit+g/pk2/DSE9ycoUiUhEpAIlIqWgWVZlZT/Qgp/s7vGIZTo7hRuiRVdgmtePEd8+CzersEkWkAjEZhmYqu1BWVhYBAQFkZmbi7+/v7HJEyp2zeVZmL1xGxNpX6WJeD8BGn/ZEPDCd8LAIJ1cnIq7qcn5/6wyQiJQ6L3cLD9x+MzGPfcukkHHkGG40z16J+6Q2LJ/+HJlZWc4uUUTKOZ0BKoLOAImUruRNSzHPG0Y16+8AJFGT5Bv+za2dbsZsNjm5OhFxFToDJCIuJbppJyKf28DmVm9wwuRPffZx67K7WBB3L/Eb9zi7PBEph3QGqAg6AyTiPLlHk0mZ/STXpS8C4HcjmAWhj9C8cz9a1onCZNIZIREp2uX8/lYAKoICkIjznd75C3lfD6NyzmEAjhmVmOk7hOibHqJ3i2q4W3QCW0QKUgC6QgpAImVEzimOLXoTtnxJldxUAFZaG/KjT08iWt/JPa1rEOLn6eQiRaSsUAC6QgpAImWMNY8zyybg/utbuNlyANhsq8HH9MSzUU9ubhRDh9rBeLlbnFyoiDiTAtAVUgASKaOO7SP3t+mQ8DEe+ScB2GuL4PX8fmzzaMptrerQv00M1YN9nVyoiDiDAtAVUgASKeNOpWOsmUJewv/wOJMOQL5hZrmtCaPzHqVx7Rr0aBxBt4bhBPp6OLlYESktCkBXSAFIxEVkH4Nlb2DsWojp+H4ADhhhLLC2Yb71enaba9K5QSid6obStmYQUVV8nFywiFxLCkBXSAFIxAWlbYHP/gEnUx1Ni63NeTf/brYZ1QETbWsGcVdsNTrXD6Wyj84MiZQ3CkBXSAFIxEWdPgo758Oen2HHD2DLByDNEk5ibhTrbHWYae2OyWyhTY0q9G5WlZbVA6ke5KsZp0XKAQWgK6QAJFIOHNkFy16HpB/AmuNo3muuwaa8qnxp7cR6Wx3ycSPUz5NbG4RxU91QWtWoQoC3uxMLF5GSUgC6QgpAIuVIzkk4uBr+2ArL34K87D834cEvthYkWmuwytaQLUZNPNzMdG0YTsNIf7o3CicmSE+UibgKBaArpAAkUk4dPwjJq+2vrV9DTsFV51dZWjHxzK3stlXjCJUBiAnyoWVMFVpVD6Rl9SpcF+Kr5ThEyigFoCukACRSAdhskLYZtn8HGbtg5wIwbI7NqW5V+c+ZHqy21eegEQbYQ08VXw9iYwJpVT2QVtWr0KhqgJblECkjFICukAKQSAWUsRuW/9t+digzpUAYyvSM4LARxK6zlfklvwkrbE04hv3/N/h4WIiNCeT6mkFcX7MKjatWxsNNgUjEGRSArpACkEgFdzYLNnwMGz6xXzb7y03UAAYmDnjWIT6nEWtzq7PdFsNhggAT3u4WmkVVpkm1ABpVDaBx1QBignx02UykFCgAXSEFIBFxyM2G/cvsN0+nbYHdi+GPLYW6HfaozsK8ZvyWU53NtpqOQATg5+VGo8gAIip7UTPYl9Y1gmgaFYCnm9YuE7maFICukAKQiFxUVirs/RkO/GoPRUd2OOYcOu+UW2V2mGuz+mw0v+Y1IAN/fjdCyME+AaOHm5mm1QKoF+5PnbBK1A7zo26Yn5buELkCCkBXSAFIRC7LmROwYz78vhYObYD07YUCEYDV5MYO71h+PRvDsRwL31nbkUZQgT7BlTypG16J2qF+NIjwp1l0ZcIDvPD30txEIn9HAegKKQCJyBXJOwN/bLOHoZQ1kLzGPh/RBY/dA5x0DybFEsW2vEg2nA1nt60qKUYoGQRg5c9LZOeDUdXK3sQE+dIg0p8GEf6E+nnq/iKRcxSArpACkIhcdYZhf9Js0xdw+ggc3WN/4qwYOWYfdrvVIjPHxIL8WH61NeK4UYmT+GDw51NmlX3cia7iQ1SgD9UCvalWxYdAH3eqB/nSIMJfS3xIhaIAdIUUgESkVJzNss9BlJ5kv4/oyA5I32Ff0NWwFrlLntmLP9yrstcaxtazIeyzRbDHiGSLURMbBR+/9/GwEF3Fh+pBvsQE+RBz7s/wAC9C/Typ5Omms0dSrigAXSEFIBFxKpsNDm+EY3vtYSjpezicCLa8Ync54x5Iumc0vmf/INUczs859dmdF8JBIxQDM0lGdKGA5O1uIaqKNw0jA6js446/lzt+Xm74e7vTKNL++L7FbMLLXU+riWtQALpCCkAiUuYYBlhzIfN3++Wzo3vP/bkHUjfB2RMX3f20W2UOuNVgtzWc7bnhZOR5kk4gSbZoTuHteDrtQm5mE3XD/Yiucu4SW+Cff1YN9KaSp9s1GKxIySgAXSEFIBFxKdY8+w3XmSlQKdT+/sgO+6W1k2n2OYyKuAH7PBtmMj0jMdly2e9Zn2RTOHtOmDhh8yLFCGG9rS65uJOPpdBZpEAf97+EIm+iqvhQJ8yP+hH+BHjryTUpXQpAV0gBSETKFWue/SzRkZ32e46O7oHc0/azSJnJl34Yk4Vt3q04mu/FntwqbMsN5yj+HDKC2W+EF7g5G+z3IPl6uuHn6UaovydtagRRI9iXqCreRAX64Ofljpe7WfchyVWjAHSFFIBEpMKw2eDUH/ZQZDLZn0w7ffTcY/uZcGgjZP3+t4exmtwwMGMYBjZMrLPWZq2tPtl4csQIIMGoi9Uwk0ZggaDk62Ghso8Hvp4WqgX6EHXuLFLUuSfboqp446c5kOQSKQBdIQUgEZFzDMN+tsiw2e8/2r0QMNkf6c9MgdMZcGwf5J+5pMNlm3zYRzWOWT3JNywcw59dtqpsstUiA3/OGJ6kUqVASAr0cSfM3ws/LzcCvN2JCPCmaqA39SP8qeRpIcDbAzezCR9PC6F+XtfoL0JcweX8/tbdayIiUjyTCTwr2d97NYCwBoX7WPPsZ5HOyzllD0pH90DeWftyIUf3AAY+tmwasYsCV8sueMjsrMmbZEsUpvwznLB5czAvnANHwvAy5RJCJt/YOrDAVhVvUw45hjsn8eEsHoCJhpH+RFb2JriSB0G+nnh7WHAzm3CzmAn0cSc8wAsPixl3ixk3i8n+p9nk+NnNbMbj/HuLCXezWXMplVM6A1QEnQESEbkGrHn2IHRkB+TnnAtOaeeWD0mC7GP2G7Yv8rh/cfIMC8fxI9WoQppRhcNGEDl4YMOEDRO5hjvZeHIGT84YHuThRi5u5P3llW+YsWHGOLePDTM2TGAyYTZbMJktYDZjT2/2dji35G2B9+erMp1vxnSui4HJ/n9/bT/X2zjfXihvFR/AjGK2ucIv9uvCg3hj0K1X9ZgudQZo0qRJvPXWW6SmptKwYUMmTJhAx44di+2/bNkyRo8ezbZt24iMjOSZZ55h6NChRfadNWsW/fr1o1evXnz77bfXaAQiInJJLO4QWt/+Ko413x6SMnbZzzydOWG/xHZsH9isYHaDpHn2p9rcvOxTAxg23E1WQjlBqOkETdlXOuMxinkvl2TH7/WBqxuALodTA9Ds2bMZOXIkkyZNon379nzwwQd0796d7du3Ex0dXaj//v376dGjB0OGDOHTTz9l5cqVPPbYY4SEhHDXXXcV6Hvw4EGeeuqpi4YpEREpYyxuEFrP/ipO7/fPhSHLuXuUTsHZTPsSI1mp9nuVsg7ZF6Q1DPus2vlnITfbfoYp97R9mzXPHqDO/2lYMQwbGDYMmw1sf/7seNmsgGF/X5SLXlS5yLZi9yt+H1MJ9ilLokMCnPr5Tr0E1qZNG1q0aMHkyZMdbfXr16d3797ExcUV6v/ss88yb948kpKSHG1Dhw5l06ZNrF7955o6VquVG2+8kQcffJAVK1Zw4sSJyzoDpEtgIiIirudyfn+bL7r1GsrNzSUhIYEuXboUaO/SpQurVq0qcp/Vq1cX6t+1a1fWr19PXt6f14zHjx9PSEgIgwcPvqRacnJyyMrKKvASERGR8stpASgjIwOr1UpYWFiB9rCwMNLS0orcJy0trcj++fn5ZGRkALBy5UqmT5/OtGnTLrmWuLg4AgICHK+oqKjLHI2IiIi4EqcFoPMunAHUMIyLzgpaVP/z7SdPnuT+++9n2rRpBAcHX3INY8aMITMz0/FKSUm5jBGIiIiIq3HaTdDBwcFYLJZCZ3vS09MLneU5Lzw8vMj+bm5uBAUFsW3bNg4cOEDPnj0d2202+41qbm5u7Ny5k+uuu67QcT09PfH09LzSIYmIiIiLcNoZIA8PD2JjY4mPjy/QHh8fT7t27Yrcp23btoX6L1q0iJYtW+Lu7k69evXYsmULiYmJjtcdd9zBTTfdRGJioi5tiYiICODkx+BHjx7NgAEDaNmyJW3btmXq1KkkJyc75vUZM2YMhw4d4pNPPgHsT3xNnDiR0aNHM2TIEFavXs306dP54osvAPDy8qJRo0YFPqNy5coAhdpFRESk4nJqAOrbty9Hjx5l/PjxpKam0qhRIxYsWEBMTAwAqampJCf/uVJxjRo1WLBgAaNGjeL9998nMjKS9957r9AcQCIiIiIXo6UwiqB5gERERFyPS8wDJCIiIuIsCkAiIiJS4SgAiYiISIWjACQiIiIVjgKQiIiIVDgKQCIiIlLhOHUeoLLq/MwAWhVeRETEdZz/vX0pM/woABXh5MmTAFo6Q0RExAWdPHmSgICAi/bRRIhFsNlsHD58GD8/v4uuTF8SWVlZREVFkZKSUi4nWSzv4wONsTwo7+MDjbE8KO/jg6s/RsMwOHnyJJGRkZjNF7/LR2eAimA2m6lWrdo1/Qx/f/9y+w8ayv/4QGMsD8r7+EBjLA/K+/jg6o7x7878nKeboEVERKTCUQASERGRCkcBqJR5enry0ksv4enp6exSronyPj7QGMuD8j4+0BjLg/I+PnDuGHUTtIiIiFQ4OgMkIiIiFY4CkIiIiFQ4CkAiIiJS4SgAiYiISIWjAFSKJk2aRI0aNfDy8iI2NpYVK1Y4u6QSe/nllzGZTAVe4eHhju2GYfDyyy8TGRmJt7c3nTp1Ytu2bU6s+OKWL19Oz549iYyMxGQy8e233xbYfinjycnJYcSIEQQHB+Pr68sdd9zB77//XoqjuLi/G+OgQYMKfafXX399gT5leYxxcXG0atUKPz8/QkND6d27Nzt37izQx9W/x0sZoyt/j5MnT6ZJkyaOSfHatm3Ljz/+6Nju6t8f/P0YXfn7K0pcXBwmk4mRI0c62srK96gAVEpmz57NyJEjGTt2LBs3bqRjx450796d5ORkZ5dWYg0bNiQ1NdXx2rJli2Pbm2++yTvvvMPEiRNZt24d4eHh3HrrrY511sqa06dP07RpUyZOnFjk9ksZz8iRI/nmm2+YNWsWv/76K6dOneL222/HarWW1jAu6u/GCNCtW7cC3+mCBQsKbC/LY1y2bBnDhg1jzZo1xMfHk5+fT5cuXTh9+rSjj6t/j5cyRnDd77FatWq8/vrrrF+/nvXr13PzzTfTq1cvxy9HV//+4O/HCK77/V1o3bp1TJ06lSZNmhRoLzPfoyGlonXr1sbQoUMLtNWrV8947rnnnFTRlXnppZeMpk2bFrnNZrMZ4eHhxuuvv+5oO3v2rBEQEGBMmTKllCosOcD45ptvHD9fynhOnDhhuLu7G7NmzXL0OXTokGE2m42ffvqp1Gq/VBeO0TAM44EHHjB69epV7D6uNsb09HQDMJYtW2YYRvn8Hi8co2GUv+8xMDDQ+PDDD8vl93fe+TEaRvn5/k6ePGnUrl3biI+PN2688UbjiSeeMAyjbP3vUGeASkFubi4JCQl06dKlQHuXLl1YtWqVk6q6crt37yYyMpIaNWpw7733sm/fPgD2799PWlpagfF6enpy4403uuR4L2U8CQkJ5OXlFegTGRlJo0aNXGrMS5cuJTQ0lDp16jBkyBDS09Md21xtjJmZmQBUqVIFKJ/f44VjPK88fI9Wq5VZs2Zx+vRp2rZtWy6/vwvHeF55+P6GDRvGbbfdRufOnQu0l6XvUYuhloKMjAysVithYWEF2sPCwkhLS3NSVVemTZs2fPLJJ9SpU4c//viDf/3rX7Rr145t27Y5xlTUeA8ePOiMcq/IpYwnLS0NDw8PAgMDC/Vxle+4e/fu/OMf/yAmJob9+/czbtw4br75ZhISEvD09HSpMRqGwejRo+nQoQONGjUCyt/3WNQYwfW/xy1bttC2bVvOnj1LpUqV+Oabb2jQoIHjF195+P6KGyO4/vcHMGvWLDZs2MC6desKbStL/ztUACpFJpOpwM+GYRRqcxXdu3d3vG/cuDFt27bluuuu4+OPP3bcsFeexgslG48rjblv376O940aNaJly5bExMQwf/58+vTpU+x+ZXGMw4cPZ/Pmzfz666+FtpWX77G4Mbr691i3bl0SExM5ceIEc+fO5YEHHmDZsmWO7eXh+ytujA0aNHD57y8lJYUnnniCRYsW4eXlVWy/svA96hJYKQgODsZisRRKrunp6YVSsKvy9fWlcePG7N692/E0WHkZ76WMJzw8nNzcXI4fP15sH1cTERFBTEwMu3fvBlxnjCNGjGDevHksWbKEatWqOdrL0/dY3BiL4mrfo4eHB7Vq1aJly5bExcXRtGlT/vOf/5Sr76+4MRbF1b6/hIQE0tPTiY2Nxc3NDTc3N5YtW8Z7772Hm5ubo8ay8D0qAJUCDw8PYmNjiY+PL9AeHx9Pu3btnFTV1ZWTk0NSUhIRERHUqFGD8PDwAuPNzc1l2bJlLjneSxlPbGws7u7uBfqkpqaydetWlxwzwNGjR0lJSSEiIgIo+2M0DIPhw4fz9ddf88svv1CjRo0C28vD9/h3YyyKq32PFzIMg5ycnHLx/RXn/BiL4mrf3y233MKWLVtITEx0vFq2bEn//v1JTEykZs2aZed7vGq3U8tFzZo1y3B3dzemT59ubN++3Rg5cqTh6+trHDhwwNmllciTTz5pLF261Ni3b5+xZs0a4/bbbzf8/Pwc43n99deNgIAA4+uvvza2bNli9OvXz4iIiDCysrKcXHnRTp48aWzcuNHYuHGjARjvvPOOsXHjRuPgwYOGYVzaeIYOHWpUq1bNWLx4sbFhwwbj5ptvNpo2bWrk5+c7a1gFXGyMJ0+eNJ588klj1apVxv79+40lS5YYbdu2NapWreoyY3z00UeNgIAAY+nSpUZqaqrjlZ2d7ejj6t/j343R1b/HMWPGGMuXLzf2799vbN682Xj++ecNs9lsLFq0yDAM1//+DOPiY3T17684f30KzDDKzveoAFSK3n//fSMmJsbw8PAwWrRoUeDRVVfTt29fIyIiwnB3dzciIyONPn36GNu2bXNst9lsxksvvWSEh4cbnp6exg033GBs2bLFiRVf3JIlSwyg0OuBBx4wDOPSxnPmzBlj+PDhRpUqVQxvb2/j9ttvN5KTk50wmqJdbIzZ2dlGly5djJCQEMPd3d2Ijo42HnjggUL1l+UxFjU2wJg5c6ajj6t/j383Rlf/Hh966CHH/48MCQkxbrnlFkf4MQzX//4M4+JjdPXvrzgXBqCy8j2aDMMwrt75JBEREZGyT/cAiYiISIWjACQiIiIVjgKQiIiIVDgKQCIiIlLhKACJiIhIhaMAJCIiIhWOApCIiIhUOApAIiLFMJlMfPvtt84uQ0SuAQUgESmTBg0ahMlkKvTq1q2bs0sTkXLAzdkFiIgUp1u3bsycObNAm6enp5OqEZHyRGeARKTM8vT0JDw8vMArMDAQsF+emjx5Mt27d8fb25saNWrw1VdfFdh/y5Yt3HzzzXh7exMUFMQjjzzCqVOnCvSZMWMGDRs2xNPTk4iICIYPH15ge0ZGBnfeeSc+Pj7Url2befPmObYdP36c/v37ExISgre3N7Vr1y4U2ESkbFIAEhGXNW7cOO666y42bdrE/fffT79+/UhKSgIgOzubbt26ERgYyLp16/jqq69YvHhxgYAzefJkhg0bxiOPPMKWLVuYN28etWrVKvAZr7zyCvfccw+bN2+mR48e9O/fn2PHjjk+f/v27fz4448kJSUxefJkgoODS+8vQERK7qourSoicpU88MADhsViMXx9fQu8xo8fbxiGfWX0oUOHFtinTZs2xqOPPmoYhmFMnTrVCAwMNE6dOuXYPn/+fMNsNhtpaWmGYRhGZGSkMXbs2GJrAIwXXnjB8fOpU6cMk8lk/Pjjj4ZhGEbPnj2NBx988OoMWERKle4BEpEy66abbmLy5MkF2qpUqeJ437Zt2wLb2rZtS2JiIgBJSUk0bdoUX19fx/b27dtjs9nYuXMnJpOJw4cPc8stt1y0hiZNmjje+/r64ufnR3p6OgCPPvood911Fxs2bKBLly707t2bdu3alWisIlK6FIBEpMzy9fUtdEnq75hMJgAMw3C8L6qPt7f3JR3P3d290L42mw2A7t27c/DgQebPn8/ixYu55ZZbGDZsGP/+978vq2YRKX26B0hEXNaaNWsK/VyvXj0AGjRoQGJiIqdPn3ZsX7lyJWazmTp16uDn50f16tX5+eefr6iGkJAQBg0axKeffsqECROYOnXqFR1PREqHzgCJSJmVk5NDWlpagTY3NzfHjcZfffUVLVu2pEOHDnz22WesXbuW6dOnA9C/f39eeuklHnjgAV5++WWOHDnCiBEjGDBgAGFhYQC8/PLLDB06lNDQULp3787JkydZuXIlI0aMuKT6XnzxRWJjY2nYsCE5OTn88MMP1K9f/yr+DYjItaIAJCJl1k8//URERESBtrp167Jjxw7A/oTWrFmzeOyxxwgPD+ezzz6jQYMGAPj4+LBw4UKeeOIJWrVqhY+PD3fddRfvvPOO41gPPPAAZ8+e5d133+Wpp54iODiYu++++5Lr8/DwYMyYMRw4cABvb286duzIrFmzrsLIReRaMxmGYTi7CBGRy2Uymfjmm2/o3bu3s0sREReke4BERESkwlEAEhERkQpH9wCJiEvS1XsRuRI6AyQiIiIVjgKQiIiIVDgKQCIiIlLhKACJiIhIhaMAJCIiIhWOApCIiIhUOApAIiIiUuEoAImIiEiFowAkIiIiFc7/A/1IslE24YhiAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 2ms/step\n",
      "Comparison of predictions and ground truth:\n",
      "Sample 1:\n",
      "  Predicted:    [1. 1. 1. 1. 1. 2. 4. 1. 1. 3. 2. 2. 1. 2. 4. 2.]\n",
      "  Ground Truth: [1. 0. 1. 1. 1. 1. 3. 0. 1. 3. 1. 2. 1. 1. 4. 2.]\n",
      "----------------------------------------\n",
      "Sample 2:\n",
      "  Predicted:    [3. 0. 1. 1. 1. 2. 3. 2. 1. 2. 1. 2. 3. 1. 1. 1.]\n",
      "  Ground Truth: [4. 0. 2. 0. 2. 0. 3. 2. 1. 2. 2. 2. 3. 0. 1. 0.]\n",
      "----------------------------------------\n",
      "Sample 3:\n",
      "  Predicted:    [2. 1. 0. 0. 1. 1. 4. 3. 3. 3. 1. 2. 4. 2. 0. 1.]\n",
      "  Ground Truth: [3. 2. 0. 0. 2. 0. 4. 4. 3. 3. 1. 3. 4. 3. 0. 1.]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = model.predict(X_test)\n",
    "y_pred_test_rescaled = y_pred_test * (q - 1)\n",
    "y_test_rescaled = y_test * (q - 1)\n",
    "\n",
    "print(\"Comparison of predictions and ground truth:\")\n",
    "for i in range(3):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  Predicted:    {np.round(y_pred_test_rescaled[i])}\")\n",
    "    print(f\"  Ground Truth: {np.round(y_test_rescaled[i])}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n",
      "Sample 1:\n",
      "  Predicted:    [1 2 1 0 0 0 0 3 2 0 0 3 1 1 4 3]\n",
      "  Ground Truth: [2 3 2 1 0 0 0 4 4 1 2 4 2 2 4 4]\n",
      "----------------------------------------\n",
      "Sample 2:\n",
      "  Predicted:    [ 1  3  0  4  4  0  2  2  3  0  3  0 -1  2  2  0]\n",
      "  Ground Truth: [1 3 0 4 4 2 3 4 3 0 2 1 0 3 3 0]\n",
      "----------------------------------------\n",
      "Sample 3:\n",
      "  Predicted:    [3 0 0 4 2 0 2 3 2 3 2 3 2 1 1 0]\n",
      "  Ground Truth: [3 1 1 3 3 1 3 4 2 4 3 3 2 3 2 1]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def predict_on_unseen_data(model, q, num_samples=3):\n",
    "    unseen_dataset = np.random.randint(0, q, size=(num_samples, n))\n",
    "    \n",
    "    padded_unseen_data = np.hstack((unseen_dataset, np.zeros((num_samples, n_padded - n))))\n",
    "\n",
    "    encoded_unseen_dataset = np.array([cos2(message, n_padded) for message in padded_unseen_data])\n",
    "\n",
    "    predictions = model.predict(encoded_unseen_dataset)\n",
    "\n",
    "    predictions_rounded = np.round(predictions*q-1).astype(int)\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        print(f\"Sample {i+1}:\")\n",
    "        print(f\"  Predicted:    {predictions_rounded[i]}\")\n",
    "        print(f\"  Ground Truth: {unseen_dataset[i]}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    return predictions_rounded, unseen_dataset\n",
    "\n",
    "predictions, ground_truth = predict_on_unseen_data(model, q, num_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1 Gradient Mean: 0.6662954\n",
      "leaky_re_lu has no trainable variables.\n",
      "support_layer_1 Gradient Mean: 1.3332868\n",
      "leaky_re_lu_1 has no trainable variables.\n",
      "layer2 Gradient Mean: 1.3540019\n",
      "leaky_re_lu_2 has no trainable variables.\n",
      "support_layer_2 Gradient Mean: 1.3567832\n",
      "leaky_re_lu_3 has no trainable variables.\n",
      "layer3 Gradient Mean: 1.608211\n",
      "leaky_re_lu_4 has no trainable variables.\n",
      "output_layer Gradient Mean: 2.281895\n",
      "activation has no trainable variables.\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    if layer.trainable_variables:  # Check if the layer has trainable variables\n",
    "        grads = tf.reduce_mean(tf.abs(layer.trainable_variables[0]))\n",
    "        print(layer.name, \"Gradient Mean:\", grads.numpy())\n",
    "    else:\n",
    "        print(layer.name, \"has no trainable variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
