{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCT3SNN - DCT-2 Encoded Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is encoded using the padded generator matrix. the imaginary and real values are parellel processed until concatenation. The parallel execution mimics the DCT-III SNN. One additional layer is included to account for the scaling diagonal matrix $\\hat{D}_n$ in the classical algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (r + 1) | (q - 1)\n",
    "# (r + 1) | n\n",
    "# q is a prime number\n",
    "# n = 2^t\n",
    "# r < n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset -f\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original n: 16\n",
      "Padded n: 16\n",
      "Generated dataset shape: (1000, 16)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n = 16\n",
    "q = 5\n",
    "num_samples = 1000\n",
    "\n",
    "def next_power_of_two(x):\n",
    "    return 1 if x == 0 else 2**(x - 1).bit_length()\n",
    "\n",
    "n_padded = next_power_of_two(n)\n",
    "\n",
    "dataset = np.random.randint(0, q, size=(num_samples, n))\n",
    "\n",
    "if n_padded > n:\n",
    "    pad_width = n_padded - n\n",
    "    dataset = np.pad(dataset, ((0, 0), (0, pad_width)), mode='constant', constant_values=0)\n",
    "\n",
    "print(\"Original n:\", n)\n",
    "print(\"Padded n:\", n_padded)\n",
    "print(\"Generated dataset shape:\", dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 4 2 ... 1 3 1]\n",
      " [3 4 0 ... 3 2 3]\n",
      " [3 0 2 ... 0 1 4]\n",
      " ...\n",
      " [4 4 4 ... 0 2 2]\n",
      " [4 2 4 ... 4 0 0]\n",
      " [2 2 2 ... 0 4 2]]\n",
      "(1000, 16)\n",
      "(16,)\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(dataset.shape)\n",
    "print(dataset[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\tilde{M}_{kj} = \\left[ \\left( \\frac{w_0}{z_0} \\right)^j \\zeta^{kj} \\right]_{k,j=0}^{n-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padded_generator_matrix(n, w0, z0):\n",
    "    zeta = np.exp(-2j * np.pi / n)\n",
    "    M_tilde = np.array([[(w0 / z0) ** j * zeta**(k * j) for j in range(n)] for k in range(n)], dtype=complex)\n",
    "    return M_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 16)\n"
     ]
    }
   ],
   "source": [
    "# (x, y, z, w) --> (1, 2, 3, 4)\n",
    "w0 = 4\n",
    "z0 = 3\n",
    "\n",
    "M_tilde = padded_generator_matrix(n_padded, w0, z0)\n",
    "print(M_tilde.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 649.89449698  +0.j           69.97998169+371.13335899j\n",
      "   -83.92481787+137.40000024j ...  -88.37954911-103.22398682j\n",
      "   -83.92481787-137.40000024j   69.97998169-371.13335899j]\n",
      " [ 695.87003951  +0.j          268.59479107+394.57072111j\n",
      "   -36.50274968+309.16102475j ...  -47.36788425-179.88976988j\n",
      "   -36.50274968-309.16102475j  268.59479107-394.57072111j]\n",
      " [ 540.63797786  +0.j          235.29809241+209.31041098j\n",
      "   176.66145652+204.71795948j ...  108.87153914-320.22828928j\n",
      "   176.66145652-204.71795948j  235.29809241-209.31041098j]\n",
      " ...\n",
      " [ 460.45123158  +0.j          162.64910275+168.11969395j\n",
      "    54.45039471+219.81284778j ...  -26.91067254-124.72256855j\n",
      "    54.45039471-219.81284778j  162.64910275-168.11969395j]\n",
      " [ 558.17613119  +0.j          -92.41727316+362.85897707j\n",
      "  -228.64175513 +13.85662093j ...  -76.51498185+176.39278765j\n",
      "  -228.64175513 -13.85662093j  -92.41727316-362.85897707j]\n",
      " [ 671.99178181  +0.j          161.97417378+352.04621278j\n",
      "    43.09272997+244.01831359j ...  -56.20864468-237.97753496j\n",
      "    43.09272997-244.01831359j  161.97417378-352.04621278j]]\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset = np.array([np.dot(M_tilde, x) for x in dataset])\n",
    "encoded_dataset[np.abs(encoded_dataset) < 1e-10] = 0\n",
    "encoded_dataset = np.round(encoded_dataset, decimals=10)\n",
    "print(encoded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 2 4 4 1 2 2 2 4 3 2 4 1 3 1]\n",
      "[ 649.89449698  +0.j           69.97998169+371.13335899j\n",
      "  -83.92481787+137.40000024j  -88.37954911+103.22398682j\n",
      "  -74.53901813 +41.73652084j  -97.48254893 +37.96448017j\n",
      " -147.95858217-108.15626408j   47.97234649-148.66691513j\n",
      "  146.76987906  -0.j           47.97234649+148.66691513j\n",
      " -147.95858217+108.15626408j  -97.48254893 -37.96448017j\n",
      "  -74.53901813 -41.73652084j  -88.37954911-103.22398682j\n",
      "  -83.92481787-137.40000024j   69.97998169-371.13335899j]\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])\n",
    "print(encoded_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train_real : (1000, 16)\n",
      "[ 649.8945     69.97998   -83.92482   -88.37955   -74.53902   -97.48255\n",
      " -147.95859    47.972347  146.76988    47.972347 -147.95859   -97.48255\n",
      "  -74.53902   -88.37955   -83.92482    69.97998 ]\n",
      "\n",
      "X_train_imag : (1000, 16)\n",
      "[   0.        371.13336   137.4       103.22398    41.736523   37.96448\n",
      " -108.156265 -148.66692    -0.        148.66692   108.156265  -37.96448\n",
      "  -41.736523 -103.22398  -137.4      -371.13336 ]\n"
     ]
    }
   ],
   "source": [
    "X_real = np.real(encoded_dataset).astype(np.float32)\n",
    "X_imag = np.imag(encoded_dataset).astype(np.float32)\n",
    "\n",
    "print(\"\\nX_train_real :\", X_real.shape)\n",
    "print(X_real[0])\n",
    "print(\"\\nX_train_imag :\", X_imag.shape)\n",
    "print(X_imag[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: \n",
      "[3. 4. 2. 4. 4. 1. 2. 2. 2. 4. 3. 2. 4. 1. 3. 1.]\n",
      "\n",
      "y_normalized: \n",
      "[0.75 1.   0.5  1.   1.   0.25 0.5  0.5  0.5  1.   0.75 0.5  1.   0.25\n",
      " 0.75 0.25]\n"
     ]
    }
   ],
   "source": [
    "# normalize target data (integers 0-q to [0, 1])\n",
    "y_normalized = dataset.astype(np.float32) / (q - 1)  # Scale to [0, 1]\n",
    "print(\"y: \")\n",
    "print(dataset.astype(np.float32)[0])\n",
    "print(\"\\ny_normalized: \")\n",
    "print(y_normalized[0])\n",
    "\n",
    "labels = y_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shapes: X_real: (800, 16) X_imag: (800, 16) y: (800, 16)\n",
      "Testing data shapes: X_real: (200, 16) X_imag: (200, 16) y: (200, 16)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_real_train, X_real_test, y_train, y_test = train_test_split(\n",
    "    X_real, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_imag_train, X_imag_test, _, _ = train_test_split(\n",
    "    X_imag, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training data shapes: X_real:\", X_real_train.shape, \"X_imag:\", X_imag_train.shape, \"y:\", y_train.shape)\n",
    "print(\"Testing data shapes: X_real:\", X_real_test.shape, \"X_imag:\", X_imag_test.shape, \"y:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "print(X_real_train.shape[1])\n",
    "print(X_real_test.shape[1])\n",
    "print(X_imag_train.shape[1])\n",
    "print(X_imag_test.shape[1])\n",
    "print(y_train.shape[1])\n",
    "print(y_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_real_train.shape[1]\n",
    "output_dim = y_train.shape[1]\n",
    "print(input_dim)\n",
    "print(output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DCT-3 Structure Imposed Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, units, kernel_initializer='he_normal', bias_initializer='zeros', use_bias=True, **kwargs):\n",
    "    super(FirstLayer, self).__init__(**kwargs)\n",
    "    self.units = units # features/neurons\n",
    "    self.kernel_initializer = kernel_initializer\n",
    "    self.bias_initializer = bias_initializer\n",
    "    self.use_bias = use_bias\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    # input_shape --> (batch_size, input_dim)\n",
    "    n = self.units\n",
    "    n1 = n // 2\n",
    "    \n",
    "    self.d_1 = self.add_weight(name =\"kernel_d1\",\n",
    "                               shape=(n1,),\n",
    "                               initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                              #  regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                               trainable=True)\n",
    "    self.d_2 = self.add_weight(name =\"kernel_d2\",\n",
    "                               shape=(n1,),\n",
    "                               initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                              #  regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                               trainable=True)\n",
    "    if self.use_bias:\n",
    "      self.bias = self.add_weight(name =\"bias\",\n",
    "                                  shape=(self.units,),\n",
    "                                  initializer=tf.keras.initializers.get(self.bias_initializer),\n",
    "                                  # regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                                  trainable=True)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # print(\"Enters first layer\")\n",
    "    P_n = tf.concat([inputs[:, ::2], inputs[:, 1::2]], axis=1) # permutation [x0, x2, x4, ..., xn-2, x1, x3, x5, ..., xn-1]\n",
    "    out1 = P_n[:, :int(P_n.shape[1]/2)] # even indices [x0 x2 x4 ...]\n",
    "    out2 = P_n[:, int(P_n.shape[1]/2):] # odd indices [x1 x3 x5 ...]\n",
    "    \n",
    "    # expected output for rearrange out2 after bidiagonal matrix ---> out2_n = [sqrt(2).x1, x1+x3, x3+x5+ ..., xn-3+xn-1]   \n",
    "    \n",
    "    out3 = tf.add(\n",
    "      tf.multiply(out2, self.d_1), # diagonal\n",
    "      tf.multiply(tf.concat([tf.zeros_like(out2[:, :1]), out2[:, :-1]], axis=1), self.d_2) # super diagonal\n",
    "    )\n",
    "    \n",
    "    out = tf.concat([out1, out3], axis=1) # [x0 x2 x4 ... sqrt(2).x1, x1+x3, x3+x5+ ..., xn-3+xn-1]\n",
    "\n",
    "    if self.use_bias:\n",
    "      out += self.bias\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, kernel_initializer='he_normal', bias_initializer='zeros', use_bias=True, **kwargs):\n",
    "        super(SecondLayer, self).__init__(**kwargs)\n",
    "        self.units = units  # Number of neurons/features\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n = self.units\n",
    "        n1 = n // 2\n",
    "        num_blocks = n1 // 2  # Number of 2×2 blocks needed for c1 and c2 (for 16 we need 4 blocks)\n",
    "\n",
    "        self.d_1 = self.add_weight(name=\"kernel_d1\",\n",
    "                                   shape=(n1 - 2,),\n",
    "                                   initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                #    regularizer=tf.keras.regularizers.l2(1e-5),\n",
    "                                   trainable=True)\n",
    "        self.d_2 = self.add_weight(name=\"kernel_d2\",\n",
    "                                   shape=(n1 - 2,),\n",
    "                                   initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                #    regularizer=tf.keras.regularizers.l2(1e-5),\n",
    "                                   trainable=True)\n",
    "        self.w = self.add_weight(name=\"kernel_w\",\n",
    "                                 shape=(n1 - 2,),\n",
    "                                 initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                #  regularizer=tf.keras.regularizers.l2(1e-5),\n",
    "                                 trainable=True)\n",
    "    \n",
    "        self.C_1 = self.add_weight(name=\"kernel_C_1\",\n",
    "                                   shape=(num_blocks, 2, 2),\n",
    "                                   initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                #    regularizer=tf.keras.regularizers.l2(1e-5),\n",
    "                                   trainable=True)\n",
    "        self.C_2 = self.add_weight(name=\"kernel_C_2\",\n",
    "                                   shape=(num_blocks, 2, 2),\n",
    "                                   initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                #    regularizer=tf.keras.regularizers.l2(1e-5),\n",
    "                                   trainable=True)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name=\"bias\",\n",
    "                                        shape=(self.units,),\n",
    "                                        initializer=tf.keras.initializers.get(self.bias_initializer),\n",
    "                                        # regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                                        trainable=True)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # print(\"Enters second layer\")\n",
    "        \n",
    "        def build_final_matrix(C, n1):\n",
    "            \"\"\"Constructs an n1 x n1 block-diagonal matrix from 2x2 blocks in C.\"\"\"\n",
    "            final_C = tf.eye(n1)  # initialize as an identity matrix\n",
    "            num_blocks = n1 // 2\n",
    "\n",
    "            for i in range(num_blocks):\n",
    "                final_C = tf.tensor_scatter_nd_update(\n",
    "                    final_C,\n",
    "                    indices=[[2*i, 2*i], [2*i, 2*i+1], [2*i+1, 2*i], [2*i+1, 2*i+1]],\n",
    "                    updates=tf.reshape(C[i], (-1,))\n",
    "                )\n",
    "\n",
    "            return final_C\n",
    "\n",
    "        def recursiveDCTIII(inputVector, d_1, d_2, w, C, level):\n",
    "            n = inputVector.shape[1]\n",
    "\n",
    "            if n == 2:\n",
    "                out = tf.matmul(inputVector, C[level])  # select correct 2×2 matrix\n",
    "                return out\n",
    "            else:\n",
    "                P_n = tf.concat([inputVector[:, ::2], inputVector[:, 1::2]], axis=1)  # Permutation\n",
    "                out1 = P_n[:, :n // 2]\n",
    "                out2 = P_n[:, n // 2:]\n",
    "\n",
    "                # Apply diagonal and superdiagonal scaling\n",
    "                d1 = tf.multiply(out2, tf.reshape(d_1[:(n // 2)], (1, -1)))\n",
    "                d2 = tf.concat([tf.zeros_like(out2[:, :1]), tf.multiply(out2[:, :-1], tf.reshape(d_2[:(n // 2) - 1], (1, -1)))], axis=1)\n",
    "                out2_bn = tf.add(d1, d2)\n",
    "\n",
    "                # Recursively apply DCT-III\n",
    "                out1_n = recursiveDCTIII(out1, d_1[n // 2:], d_2[n // 2:], w[n // 2:], C, level + 1)\n",
    "                out2_n = recursiveDCTIII(out2_bn, d_1[n // 2:], d_2[n // 2:], w[n // 2:], C, level + 1)\n",
    "\n",
    "                out2_wn = tf.multiply(out2_n, tf.reshape(w[:n // 2], (1, -1)))\n",
    "\n",
    "                out = (1 / tf.sqrt(tf.constant(2.0, dtype=tf.float32))) * tf.concat(\n",
    "                    [(out1_n + out2_wn), tf.reverse((out1_n - out2_wn), axis=[1])], axis=1)\n",
    "\n",
    "                return out\n",
    "\n",
    "        input1 = inputs[:, :self.units // 2]\n",
    "        input2 = inputs[:, self.units // 2:]\n",
    "\n",
    "        z1 = recursiveDCTIII(input1, self.d_1, self.d_2, self.w, self.C_1, level=0)\n",
    "        z2 = recursiveDCTIII(input2, self.d_1, self.d_2, self.w, self.C_2, level=0)\n",
    "        \n",
    "        final_C1 = build_final_matrix(self.C_1, self.units // 2)\n",
    "        final_C2 = build_final_matrix(self.C_2, self.units // 2)\n",
    "\n",
    "        z1 = tf.matmul(z1, final_C1)\n",
    "        z2 = tf.matmul(z2, final_C2)\n",
    "\n",
    "        out = tf.concat([z1, z2], axis=1)\n",
    "\n",
    "        if self.use_bias:\n",
    "            out += self.bias\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThirdLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, kernel_initializer='he_normal', bias_initializer='zeros', use_bias=True, **kwargs):\n",
    "        super(ThirdLayer, self).__init__(**kwargs)\n",
    "        self.units = units # features/neurons\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n = self.units\n",
    "        n1 = n // 2\n",
    "        \n",
    "        self.w = self.add_weight(name =\"kernel_w\",\n",
    "                                      shape=(n1,),\n",
    "                                      initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                    #   regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                                      trainable=True)\n",
    "        # self.h = self.add_weight(name =\"kernel_h\",\n",
    "        #                               shape = (2,2),\n",
    "        #                               initializer='glorot_normal',\n",
    "        #                               trainable=True)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name =\"bias\",\n",
    "                                       shape=(self.units,),\n",
    "                                       initializer=tf.keras.initializers.get(self.bias_initializer),\n",
    "                                    #    regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                                       trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # print(\"Enters third layer\")\n",
    "        \n",
    "        out1 = inputs[:, :int(inputs.shape[1] / 2)]\n",
    "        out2 = inputs[:, int(inputs.shape[1] / 2):]\n",
    "\n",
    "        # out1 - stays as is\n",
    "        out3 = tf.multiply(out2, self.w)\n",
    "        \n",
    "        # TODO: make H trainable instead of hardcoded\n",
    "        \n",
    "        out = (1 / tf.sqrt(tf.constant(2.0, dtype=tf.float32))) * tf.concat([(out1 + out3), tf.reverse((out1 - out3), axis=[1])], axis=1)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            out += self.bias\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, kernel_initializer='he_normal', bias_initializer='zeros', use_bias=True, **kwargs):\n",
    "        super(CustomLayer, self).__init__(**kwargs)\n",
    "        self.units = units # features/neurons\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n = self.units\n",
    "        \n",
    "        self.m = self.add_weight(name =\"kernel_m\",\n",
    "                                      shape=(n,),\n",
    "                                      initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                    #   regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                                      trainable=True)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name =\"bias\",\n",
    "                                       shape=(self.units,),\n",
    "                                       initializer=tf.keras.initializers.get(self.bias_initializer),\n",
    "                                    #    regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                                       trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # print(\"Enters custom layer\")\n",
    "\n",
    "        out = tf.multiply(inputs, self.m)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            out += self.bias\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only difference between the gen matrix encoded and dct encoded is the linear mapping (m weight matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, kernel_initializer='ones', bias_initializer='zeros', use_bias=True, **kwargs):\n",
    "        super(LinearLayer, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n = self.units\n",
    "\n",
    "        self.m = self.add_weight(name=\"kernel_m_1\",\n",
    "                                 shape=(n,),\n",
    "                                 initializer=tf.keras.initializers.get(self.kernel_initializer),\n",
    "                                 trainable=True)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name=\"bias\",\n",
    "                                        shape=(self.units,),\n",
    "                                        initializer=tf.keras.initializers.get(self.bias_initializer),\n",
    "                                        trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # print(\"linear layer input:\", inputs.shape)\n",
    "\n",
    "        inputs1 = inputs[:, :int(inputs.shape[1] / 2)]\n",
    "        inputs2 = inputs[:, int(inputs.shape[1] / 2):]\n",
    "\n",
    "        out = tf.math.sqrt(inputs1**2 + inputs2**2)\n",
    "        out = tf.multiply(out, self.m)\n",
    "\n",
    "        if self.use_bias:\n",
    "            out += self.bias\n",
    "\n",
    "        # print(\"linear layer output:\", out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import Initializer\n",
    "\n",
    "class CustomScalingInitializer(Initializer):\n",
    "    def __init__(self, n, w0, z0):\n",
    "        self.n = n\n",
    "        self.w0 = w0\n",
    "        self.z0 = z0\n",
    "\n",
    "    def __call__(self, shape, dtype=None):\n",
    "        return tf.convert_to_tensor([(self.z0 / self.w0) ** k for k in range(self.n)], dtype=dtype)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"n\": self.n, \"w0\": self.w0, \"z0\": self.z0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " real_input (InputLayer)     [(None, 16)]                 0         []                            \n",
      "                                                                                                  \n",
      " imag_input (InputLayer)     [(None, 16)]                 0         []                            \n",
      "                                                                                                  \n",
      " real_layer_1 (FirstLayer)   (None, 16)                   32        ['real_input[0][0]']          \n",
      "                                                                                                  \n",
      " imag_layer_1 (FirstLayer)   (None, 16)                   32        ['imag_input[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)     (None, 16)                   0         ['real_layer_1[0][0]']        \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 16)                   0         ['imag_layer_1[0][0]']        \n",
      "                                                                                                  \n",
      " real_layer_2 (SecondLayer)  (None, 16)                   66        ['leaky_re_lu[0][0]']         \n",
      "                                                                                                  \n",
      " imag_layer_2 (SecondLayer)  (None, 16)                   66        ['leaky_re_lu_3[0][0]']       \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 16)                   0         ['real_layer_2[0][0]']        \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 16)                   0         ['imag_layer_2[0][0]']        \n",
      "                                                                                                  \n",
      " real_layer_3 (ThirdLayer)   (None, 16)                   24        ['leaky_re_lu_1[0][0]']       \n",
      "                                                                                                  \n",
      " imag_layer_3 (ThirdLayer)   (None, 16)                   24        ['leaky_re_lu_4[0][0]']       \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 16)                   0         ['real_layer_3[0][0]']        \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 16)                   0         ['imag_layer_3[0][0]']        \n",
      "                                                                                                  \n",
      " merge_real_imag (Concatena  (None, 32)                   0         ['leaky_re_lu_2[0][0]',       \n",
      " te)                                                                 'leaky_re_lu_5[0][0]']       \n",
      "                                                                                                  \n",
      " diagonal_scaling_layer (Cu  (None, 32)                   64        ['merge_real_imag[0][0]']     \n",
      " stomLayer)                                                                                       \n",
      "                                                                                                  \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 32)                   0         ['diagonal_scaling_layer[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " output_layer (LinearLayer)  (None, 16)                   32        ['leaky_re_lu_6[0][0]']       \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 16)                   0         ['output_layer[0][0]']        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 340 (1.33 KB)\n",
      "Trainable params: 340 (1.33 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Concatenate, LeakyReLU, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def cosine_loss(y_true, y_pred):\n",
    "    y_true_norm = tf.nn.l2_normalize(y_true, axis=-1)\n",
    "    y_pred_norm = tf.nn.l2_normalize(y_pred, axis=-1)\n",
    "    return 1 - tf.reduce_mean(tf.reduce_sum(y_true_norm * y_pred_norm, axis=-1))\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    return mse\n",
    "\n",
    "def hybrid_loss(y_true, y_pred):\n",
    "    mse_part = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    cos_part = cosine_loss(y_true, y_pred)\n",
    "    return 0.5 * mse_part + 0.5 * cos_part\n",
    "\n",
    "\n",
    "def structured_NN(input_dim, output_dim):\n",
    "    real_input = Input(shape=(input_dim,), name=\"real_input\")\n",
    "    imag_input = Input(shape=(input_dim,), name=\"imag_input\")\n",
    "    \n",
    "    # he_normal\n",
    "    # glorot_normal\n",
    "    # glorot_uniform\n",
    "    \n",
    "    real_x = FirstLayer(units=input_dim, kernel_initializer='glorot_normal', bias_initializer='glorot_uniform', name=\"real_layer_1\")(real_input)\n",
    "    real_x = LeakyReLU(alpha=0.1)(real_x)\n",
    "    # real_x = CustomLayer(units=input_dim, kernel_initializer='he_normal', bias_initializer='zeros', name=\"real_support_layer_1\")(real_x)\n",
    "    # real_x = LeakyReLU(alpha=0.1)(real_x)\n",
    "    real_x = SecondLayer(units=input_dim, kernel_initializer='glorot_normal', bias_initializer='glorot_uniform', name=\"real_layer_2\")(real_x)\n",
    "    real_x = LeakyReLU(alpha=0.1)(real_x)\n",
    "    # real_x = CustomLayer(units=input_dim, kernel_initializer='he_normal', bias_initializer='zeros', name=\"real_support_layer_2\")(real_x)\n",
    "    # real_x = LeakyReLU(alpha=0.1)(real_x)\n",
    "    real_x = ThirdLayer(units=input_dim, kernel_initializer='glorot_normal', bias_initializer='glorot_uniform', name=\"real_layer_3\")(real_x)\n",
    "    real_x = LeakyReLU(alpha=0.1)(real_x)\n",
    "\n",
    "    imag_x = FirstLayer(units=input_dim, kernel_initializer='glorot_normal', bias_initializer='glorot_uniform', name=\"imag_layer_1\")(imag_input)\n",
    "    imag_x = LeakyReLU(alpha=0.1)(imag_x)\n",
    "    # imag_x = CustomLayer(units=input_dim, kernel_initializer='he_normal', bias_initializer='zeros', name=\"imag_support_layer_1\")(imag_x)\n",
    "    # imag_x = LeakyReLU(alpha=0.1)(imag_x)\n",
    "    imag_x = SecondLayer(units=input_dim, kernel_initializer='glorot_normal', bias_initializer='glorot_uniform', name=\"imag_layer_2\")(imag_x)\n",
    "    imag_x = LeakyReLU(alpha=0.1)(imag_x)\n",
    "    # imag_x = CustomLayer(units=input_dim, kernel_initializer='he_normal', bias_initializer='zeros', name=\"imag_support_layer_2\")(imag_x)\n",
    "    # imag_x = LeakyReLU(alpha=0.1)(imag_x)\n",
    "    imag_x = ThirdLayer(units=input_dim, kernel_initializer='glorot_normal', bias_initializer='glorot_uniform', name=\"imag_layer_3\")(imag_x)\n",
    "    imag_x = LeakyReLU(alpha=0.1)(imag_x)\n",
    "\n",
    "    merged = Concatenate(name=\"merge_real_imag\")([real_x, imag_x])\n",
    "    \n",
    "    # output = CustomLayer(units=input_dim * 2, kernel_initializer=CustomScalingInitializer(n=input_dim * 2, w0=w0, z0=z0), \n",
    "    #                      bias_initializer='zeros', name=\"diagonal_scaling_layer\")(merged)\n",
    "    output = CustomLayer(units=input_dim * 2, kernel_initializer='glorot_normal', bias_initializer='glorot_uniform', name=\"diagonal_scaling_layer\")(merged)\n",
    "    output = LeakyReLU(alpha=0.1)(output)\n",
    "    output = LinearLayer(units=output_dim, kernel_initializer='glorot_normal', bias_initializer='glorot_uniform', name=\"output_layer\")(output)\n",
    "    output = Activation('sigmoid')(output) #sigmoid #tanh #linear\n",
    "\n",
    "    model = Model(inputs=[real_input, imag_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss=hybrid_loss, metrics=['mse', 'mae'])\n",
    "\n",
    "    return model\n",
    "\n",
    "input_dim = X_real_train.shape[1]\n",
    "output_dim = y_train.shape[1]\n",
    "model = structured_NN(input_dim, output_dim)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real_input [(None, 16)]\n",
      "imag_input [(None, 16)]\n",
      "real_layer_1 (None, 16)\n",
      "imag_layer_1 (None, 16)\n",
      "leaky_re_lu (None, 16)\n",
      "leaky_re_lu_3 (None, 16)\n",
      "real_layer_2 (None, 16)\n",
      "imag_layer_2 (None, 16)\n",
      "leaky_re_lu_1 (None, 16)\n",
      "leaky_re_lu_4 (None, 16)\n",
      "real_layer_3 (None, 16)\n",
      "imag_layer_3 (None, 16)\n",
      "leaky_re_lu_2 (None, 16)\n",
      "leaky_re_lu_5 (None, 16)\n",
      "merge_real_imag (None, 32)\n",
      "diagonal_scaling_layer (None, 32)\n",
      "leaky_re_lu_6 (None, 32)\n",
      "output_layer (None, 16)\n",
      "activation (None, 16)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name, layer.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "adjust_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "50/50 [==============================] - 13s 42ms/step - loss: 0.1707 - mse: 0.1395 - mae: 0.3203 - val_loss: 0.1628 - val_mse: 0.1310 - val_mae: 0.3125 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1556 - mse: 0.1274 - mae: 0.3089 - val_loss: 0.1558 - val_mse: 0.1257 - val_mae: 0.3058 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1520 - mse: 0.1247 - mae: 0.3049 - val_loss: 0.1535 - val_mse: 0.1239 - val_mae: 0.3030 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1505 - mse: 0.1235 - mae: 0.3030 - val_loss: 0.1520 - val_mse: 0.1228 - val_mae: 0.3012 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1493 - mse: 0.1227 - mae: 0.3014 - val_loss: 0.1508 - val_mse: 0.1219 - val_mae: 0.2997 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1484 - mse: 0.1219 - mae: 0.3001 - val_loss: 0.1498 - val_mse: 0.1211 - val_mae: 0.2984 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1476 - mse: 0.1213 - mae: 0.2989 - val_loss: 0.1490 - val_mse: 0.1205 - val_mae: 0.2973 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1469 - mse: 0.1208 - mae: 0.2979 - val_loss: 0.1482 - val_mse: 0.1199 - val_mae: 0.2964 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1461 - mse: 0.1202 - mae: 0.2970 - val_loss: 0.1471 - val_mse: 0.1191 - val_mae: 0.2950 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1448 - mse: 0.1193 - mae: 0.2957 - val_loss: 0.1458 - val_mse: 0.1181 - val_mae: 0.2937 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1440 - mse: 0.1186 - mae: 0.2947 - val_loss: 0.1451 - val_mse: 0.1176 - val_mae: 0.2928 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1433 - mse: 0.1181 - mae: 0.2938 - val_loss: 0.1445 - val_mse: 0.1171 - val_mae: 0.2921 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1427 - mse: 0.1177 - mae: 0.2932 - val_loss: 0.1439 - val_mse: 0.1167 - val_mae: 0.2913 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1421 - mse: 0.1172 - mae: 0.2924 - val_loss: 0.1433 - val_mse: 0.1163 - val_mae: 0.2907 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1415 - mse: 0.1168 - mae: 0.2917 - val_loss: 0.1427 - val_mse: 0.1158 - val_mae: 0.2901 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1410 - mse: 0.1163 - mae: 0.2911 - val_loss: 0.1421 - val_mse: 0.1153 - val_mae: 0.2894 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.1404 - mse: 0.1159 - mae: 0.2905 - val_loss: 0.1415 - val_mse: 0.1149 - val_mae: 0.2888 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1397 - mse: 0.1154 - mae: 0.2896 - val_loss: 0.1405 - val_mse: 0.1141 - val_mae: 0.2877 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "50/50 [==============================] - 1s 14ms/step - loss: 0.1385 - mse: 0.1144 - mae: 0.2880 - val_loss: 0.1389 - val_mse: 0.1129 - val_mae: 0.2854 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1372 - mse: 0.1133 - mae: 0.2861 - val_loss: 0.1377 - val_mse: 0.1120 - val_mae: 0.2838 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1362 - mse: 0.1125 - mae: 0.2846 - val_loss: 0.1367 - val_mse: 0.1113 - val_mae: 0.2825 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1353 - mse: 0.1118 - mae: 0.2834 - val_loss: 0.1358 - val_mse: 0.1105 - val_mae: 0.2812 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1343 - mse: 0.1110 - mae: 0.2821 - val_loss: 0.1348 - val_mse: 0.1098 - val_mae: 0.2798 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.1335 - mse: 0.1103 - mae: 0.2810 - val_loss: 0.1340 - val_mse: 0.1091 - val_mae: 0.2787 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "50/50 [==============================] - 1s 24ms/step - loss: 0.1326 - mse: 0.1096 - mae: 0.2797 - val_loss: 0.1331 - val_mse: 0.1084 - val_mae: 0.2775 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "50/50 [==============================] - 1s 14ms/step - loss: 0.1318 - mse: 0.1090 - mae: 0.2786 - val_loss: 0.1324 - val_mse: 0.1079 - val_mae: 0.2761 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.1310 - mse: 0.1083 - mae: 0.2773 - val_loss: 0.1316 - val_mse: 0.1072 - val_mae: 0.2749 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1302 - mse: 0.1077 - mae: 0.2762 - val_loss: 0.1309 - val_mse: 0.1067 - val_mae: 0.2739 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.1295 - mse: 0.1072 - mae: 0.2752 - val_loss: 0.1301 - val_mse: 0.1061 - val_mae: 0.2728 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "50/50 [==============================] - 1s 19ms/step - loss: 0.1288 - mse: 0.1066 - mae: 0.2741 - val_loss: 0.1294 - val_mse: 0.1055 - val_mae: 0.2719 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.1282 - mse: 0.1061 - mae: 0.2732 - val_loss: 0.1289 - val_mse: 0.1051 - val_mae: 0.2711 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.1276 - mse: 0.1056 - mae: 0.2723 - val_loss: 0.1281 - val_mse: 0.1045 - val_mae: 0.2699 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.1270 - mse: 0.1052 - mae: 0.2715 - val_loss: 0.1274 - val_mse: 0.1039 - val_mae: 0.2690 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1264 - mse: 0.1047 - mae: 0.2706 - val_loss: 0.1268 - val_mse: 0.1034 - val_mae: 0.2679 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1259 - mse: 0.1043 - mae: 0.2697 - val_loss: 0.1260 - val_mse: 0.1028 - val_mae: 0.2668 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1252 - mse: 0.1038 - mae: 0.2688 - val_loss: 0.1248 - val_mse: 0.1018 - val_mae: 0.2650 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.1242 - mse: 0.1029 - mae: 0.2673 - val_loss: 0.1239 - val_mse: 0.1010 - val_mae: 0.2636 - lr: 0.0010\n",
      "Epoch 38/500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.1236 - mse: 0.1025 - mae: 0.2663 - val_loss: 0.1233 - val_mse: 0.1005 - val_mae: 0.2627 - lr: 0.0010\n",
      "Epoch 39/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1231 - mse: 0.1021 - mae: 0.2654 - val_loss: 0.1225 - val_mse: 0.0999 - val_mae: 0.2614 - lr: 0.0010\n",
      "Epoch 40/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1226 - mse: 0.1017 - mae: 0.2646 - val_loss: 0.1221 - val_mse: 0.0996 - val_mae: 0.2608 - lr: 0.0010\n",
      "Epoch 41/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1222 - mse: 0.1013 - mae: 0.2640 - val_loss: 0.1217 - val_mse: 0.0992 - val_mae: 0.2602 - lr: 0.0010\n",
      "Epoch 42/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1218 - mse: 0.1010 - mae: 0.2634 - val_loss: 0.1212 - val_mse: 0.0988 - val_mae: 0.2592 - lr: 0.0010\n",
      "Epoch 43/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1213 - mse: 0.1007 - mae: 0.2627 - val_loss: 0.1206 - val_mse: 0.0984 - val_mae: 0.2585 - lr: 0.0010\n",
      "Epoch 44/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1209 - mse: 0.1003 - mae: 0.2620 - val_loss: 0.1202 - val_mse: 0.0981 - val_mae: 0.2579 - lr: 0.0010\n",
      "Epoch 45/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1206 - mse: 0.1001 - mae: 0.2614 - val_loss: 0.1197 - val_mse: 0.0977 - val_mae: 0.2571 - lr: 0.0010\n",
      "Epoch 46/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1202 - mse: 0.0998 - mae: 0.2608 - val_loss: 0.1196 - val_mse: 0.0976 - val_mae: 0.2568 - lr: 0.0010\n",
      "Epoch 47/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1198 - mse: 0.0995 - mae: 0.2603 - val_loss: 0.1190 - val_mse: 0.0971 - val_mae: 0.2559 - lr: 0.0010\n",
      "Epoch 48/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1195 - mse: 0.0992 - mae: 0.2597 - val_loss: 0.1188 - val_mse: 0.0969 - val_mae: 0.2555 - lr: 0.0010\n",
      "Epoch 49/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1192 - mse: 0.0990 - mae: 0.2593 - val_loss: 0.1184 - val_mse: 0.0966 - val_mae: 0.2548 - lr: 0.0010\n",
      "Epoch 50/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1188 - mse: 0.0987 - mae: 0.2585 - val_loss: 0.1181 - val_mse: 0.0964 - val_mae: 0.2543 - lr: 0.0010\n",
      "Epoch 51/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1186 - mse: 0.0985 - mae: 0.2582 - val_loss: 0.1179 - val_mse: 0.0963 - val_mae: 0.2539 - lr: 0.0010\n",
      "Epoch 52/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1183 - mse: 0.0983 - mae: 0.2576 - val_loss: 0.1174 - val_mse: 0.0959 - val_mae: 0.2531 - lr: 0.0010\n",
      "Epoch 53/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1180 - mse: 0.0980 - mae: 0.2572 - val_loss: 0.1171 - val_mse: 0.0957 - val_mae: 0.2528 - lr: 0.0010\n",
      "Epoch 54/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1178 - mse: 0.0978 - mae: 0.2568 - val_loss: 0.1168 - val_mse: 0.0954 - val_mae: 0.2520 - lr: 0.0010\n",
      "Epoch 55/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1175 - mse: 0.0976 - mae: 0.2562 - val_loss: 0.1166 - val_mse: 0.0953 - val_mae: 0.2516 - lr: 0.0010\n",
      "Epoch 56/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1172 - mse: 0.0974 - mae: 0.2557 - val_loss: 0.1165 - val_mse: 0.0952 - val_mae: 0.2513 - lr: 0.0010\n",
      "Epoch 57/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1169 - mse: 0.0971 - mae: 0.2551 - val_loss: 0.1158 - val_mse: 0.0946 - val_mae: 0.2503 - lr: 0.0010\n",
      "Epoch 58/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1165 - mse: 0.0968 - mae: 0.2543 - val_loss: 0.1156 - val_mse: 0.0945 - val_mae: 0.2498 - lr: 0.0010\n",
      "Epoch 59/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1162 - mse: 0.0966 - mae: 0.2538 - val_loss: 0.1154 - val_mse: 0.0943 - val_mae: 0.2494 - lr: 0.0010\n",
      "Epoch 60/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1160 - mse: 0.0964 - mae: 0.2534 - val_loss: 0.1152 - val_mse: 0.0942 - val_mae: 0.2491 - lr: 0.0010\n",
      "Epoch 61/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1158 - mse: 0.0963 - mae: 0.2532 - val_loss: 0.1150 - val_mse: 0.0940 - val_mae: 0.2487 - lr: 0.0010\n",
      "Epoch 62/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1157 - mse: 0.0962 - mae: 0.2528 - val_loss: 0.1149 - val_mse: 0.0939 - val_mae: 0.2484 - lr: 0.0010\n",
      "Epoch 63/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1155 - mse: 0.0960 - mae: 0.2525 - val_loss: 0.1147 - val_mse: 0.0938 - val_mae: 0.2481 - lr: 0.0010\n",
      "Epoch 64/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1154 - mse: 0.0959 - mae: 0.2523 - val_loss: 0.1147 - val_mse: 0.0937 - val_mae: 0.2479 - lr: 0.0010\n",
      "Epoch 65/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1152 - mse: 0.0958 - mae: 0.2520 - val_loss: 0.1146 - val_mse: 0.0936 - val_mae: 0.2477 - lr: 0.0010\n",
      "Epoch 66/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1151 - mse: 0.0957 - mae: 0.2518 - val_loss: 0.1144 - val_mse: 0.0935 - val_mae: 0.2474 - lr: 0.0010\n",
      "Epoch 67/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1150 - mse: 0.0956 - mae: 0.2517 - val_loss: 0.1142 - val_mse: 0.0933 - val_mae: 0.2471 - lr: 0.0010\n",
      "Epoch 68/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1149 - mse: 0.0956 - mae: 0.2513 - val_loss: 0.1142 - val_mse: 0.0933 - val_mae: 0.2470 - lr: 0.0010\n",
      "Epoch 69/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1148 - mse: 0.0955 - mae: 0.2512 - val_loss: 0.1141 - val_mse: 0.0932 - val_mae: 0.2466 - lr: 0.0010\n",
      "Epoch 70/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1147 - mse: 0.0953 - mae: 0.2508 - val_loss: 0.1141 - val_mse: 0.0932 - val_mae: 0.2465 - lr: 0.0010\n",
      "Epoch 71/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1147 - mse: 0.0953 - mae: 0.2508 - val_loss: 0.1140 - val_mse: 0.0931 - val_mae: 0.2463 - lr: 0.0010\n",
      "Epoch 72/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1145 - mse: 0.0952 - mae: 0.2504 - val_loss: 0.1141 - val_mse: 0.0932 - val_mae: 0.2461 - lr: 0.0010\n",
      "Epoch 73/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1144 - mse: 0.0951 - mae: 0.2503 - val_loss: 0.1137 - val_mse: 0.0929 - val_mae: 0.2458 - lr: 0.0010\n",
      "Epoch 74/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1144 - mse: 0.0951 - mae: 0.2503 - val_loss: 0.1138 - val_mse: 0.0930 - val_mae: 0.2457 - lr: 0.0010\n",
      "Epoch 75/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1142 - mse: 0.0950 - mae: 0.2498 - val_loss: 0.1136 - val_mse: 0.0928 - val_mae: 0.2455 - lr: 0.0010\n",
      "Epoch 76/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1142 - mse: 0.0949 - mae: 0.2498 - val_loss: 0.1135 - val_mse: 0.0927 - val_mae: 0.2452 - lr: 0.0010\n",
      "Epoch 77/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1141 - mse: 0.0949 - mae: 0.2497 - val_loss: 0.1136 - val_mse: 0.0928 - val_mae: 0.2453 - lr: 0.0010\n",
      "Epoch 78/500\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1140 - mse: 0.0948 - mae: 0.2495 - val_loss: 0.1135 - val_mse: 0.0927 - val_mae: 0.2451 - lr: 0.0010\n",
      "Epoch 79/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1140 - mse: 0.0948 - mae: 0.2494 - val_loss: 0.1133 - val_mse: 0.0926 - val_mae: 0.2449 - lr: 0.0010\n",
      "Epoch 80/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1138 - mse: 0.0946 - mae: 0.2492 - val_loss: 0.1137 - val_mse: 0.0928 - val_mae: 0.2451 - lr: 0.0010\n",
      "Epoch 81/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1138 - mse: 0.0947 - mae: 0.2491 - val_loss: 0.1135 - val_mse: 0.0927 - val_mae: 0.2449 - lr: 0.0010\n",
      "Epoch 82/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1137 - mse: 0.0946 - mae: 0.2491 - val_loss: 0.1132 - val_mse: 0.0925 - val_mae: 0.2445 - lr: 0.0010\n",
      "Epoch 83/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1137 - mse: 0.0945 - mae: 0.2488 - val_loss: 0.1134 - val_mse: 0.0926 - val_mae: 0.2446 - lr: 0.0010\n",
      "Epoch 84/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1136 - mse: 0.0945 - mae: 0.2487 - val_loss: 0.1130 - val_mse: 0.0923 - val_mae: 0.2443 - lr: 0.0010\n",
      "Epoch 85/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1135 - mse: 0.0944 - mae: 0.2486 - val_loss: 0.1133 - val_mse: 0.0925 - val_mae: 0.2445 - lr: 0.0010\n",
      "Epoch 86/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1135 - mse: 0.0944 - mae: 0.2485 - val_loss: 0.1130 - val_mse: 0.0923 - val_mae: 0.2442 - lr: 0.0010\n",
      "Epoch 87/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1134 - mse: 0.0943 - mae: 0.2483 - val_loss: 0.1130 - val_mse: 0.0923 - val_mae: 0.2440 - lr: 0.0010\n",
      "Epoch 88/500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1133 - mse: 0.0942 - mae: 0.2482 - val_loss: 0.1130 - val_mse: 0.0923 - val_mae: 0.2439 - lr: 0.0010\n",
      "Epoch 89/500\n",
      "42/50 [========================>.....] - ETA: 0s - loss: 0.1136 - mse: 0.0945 - mae: 0.2490\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1133 - mse: 0.0942 - mae: 0.2481 - val_loss: 0.1131 - val_mse: 0.0924 - val_mae: 0.2440 - lr: 0.0010\n",
      "Epoch 90/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1132 - mse: 0.0941 - mae: 0.2480 - val_loss: 0.1129 - val_mse: 0.0922 - val_mae: 0.2438 - lr: 5.0000e-04\n",
      "Epoch 91/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1131 - mse: 0.0941 - mae: 0.2478 - val_loss: 0.1129 - val_mse: 0.0922 - val_mae: 0.2437 - lr: 5.0000e-04\n",
      "Epoch 92/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1131 - mse: 0.0941 - mae: 0.2478 - val_loss: 0.1130 - val_mse: 0.0923 - val_mae: 0.2437 - lr: 5.0000e-04\n",
      "Epoch 93/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1131 - mse: 0.0940 - mae: 0.2477 - val_loss: 0.1129 - val_mse: 0.0922 - val_mae: 0.2436 - lr: 5.0000e-04\n",
      "Epoch 94/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1130 - mse: 0.0940 - mae: 0.2476 - val_loss: 0.1129 - val_mse: 0.0922 - val_mae: 0.2435 - lr: 5.0000e-04\n",
      "Epoch 95/500\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.1130 - mse: 0.0940 - mae: 0.2475\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1130 - mse: 0.0940 - mae: 0.2475 - val_loss: 0.1129 - val_mse: 0.0922 - val_mae: 0.2435 - lr: 5.0000e-04\n",
      "Epoch 96/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1130 - mse: 0.0940 - mae: 0.2475 - val_loss: 0.1128 - val_mse: 0.0922 - val_mae: 0.2434 - lr: 2.5000e-04\n",
      "Epoch 97/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1129 - mse: 0.0939 - mae: 0.2474 - val_loss: 0.1128 - val_mse: 0.0921 - val_mae: 0.2434 - lr: 2.5000e-04\n",
      "Epoch 98/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1129 - mse: 0.0939 - mae: 0.2474 - val_loss: 0.1128 - val_mse: 0.0922 - val_mae: 0.2435 - lr: 2.5000e-04\n",
      "Epoch 99/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1129 - mse: 0.0939 - mae: 0.2474 - val_loss: 0.1128 - val_mse: 0.0922 - val_mae: 0.2434 - lr: 2.5000e-04\n",
      "Epoch 100/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1129 - mse: 0.0939 - mae: 0.2473 - val_loss: 0.1128 - val_mse: 0.0922 - val_mae: 0.2434 - lr: 2.5000e-04\n",
      "Epoch 101/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1129 - mse: 0.0939 - mae: 0.2473 - val_loss: 0.1129 - val_mse: 0.0922 - val_mae: 0.2434 - lr: 2.5000e-04\n",
      "Epoch 102/500\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.1127 - mse: 0.0938 - mae: 0.2471\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1129 - mse: 0.0939 - mae: 0.2473 - val_loss: 0.1128 - val_mse: 0.0922 - val_mae: 0.2434 - lr: 2.5000e-04\n",
      "Epoch 103/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1129 - mse: 0.0939 - mae: 0.2473 - val_loss: 0.1128 - val_mse: 0.0921 - val_mae: 0.2433 - lr: 1.2500e-04\n",
      "Epoch 104/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1128 - mse: 0.0938 - mae: 0.2472 - val_loss: 0.1128 - val_mse: 0.0921 - val_mae: 0.2433 - lr: 1.2500e-04\n",
      "Epoch 105/500\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1128 - mse: 0.0938 - mae: 0.2472 - val_loss: 0.1128 - val_mse: 0.0921 - val_mae: 0.2433 - lr: 1.2500e-04\n",
      "Epoch 106/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1128 - mse: 0.0938 - mae: 0.2472 - val_loss: 0.1128 - val_mse: 0.0921 - val_mae: 0.2433 - lr: 1.2500e-04\n",
      "Epoch 107/500\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1124 - mse: 0.0939 - mae: 0.2472\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1128 - mse: 0.0938 - mae: 0.2472 - val_loss: 0.1128 - val_mse: 0.0921 - val_mae: 0.2433 - lr: 1.2500e-04\n",
      "Epoch 108/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1128 - mse: 0.0938 - mae: 0.2472 - val_loss: 0.1128 - val_mse: 0.0921 - val_mae: 0.2433 - lr: 6.2500e-05\n",
      "Epoch 109/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1128 - mse: 0.0938 - mae: 0.2472 - val_loss: 0.1128 - val_mse: 0.0921 - val_mae: 0.2433 - lr: 6.2500e-05\n",
      "Epoch 110/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1128 - mse: 0.0938 - mae: 0.2471 - val_loss: 0.1128 - val_mse: 0.0921 - val_mae: 0.2433 - lr: 6.2500e-05\n",
      "Epoch 111/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1128 - mse: 0.0938 - mae: 0.2471 - val_loss: 0.1128 - val_mse: 0.0921 - val_mae: 0.2433 - lr: 6.2500e-05\n",
      "Epoch 112/500\n",
      "43/50 [========================>.....] - ETA: 0s - loss: 0.1132 - mse: 0.0940 - mae: 0.2475\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1128 - mse: 0.0938 - mae: 0.2471 - val_loss: 0.1128 - val_mse: 0.0921 - val_mae: 0.2433 - lr: 6.2500e-05\n",
      "Epoch 113/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1128 - mse: 0.0938 - mae: 0.2471 - val_loss: 0.1128 - val_mse: 0.0921 - val_mae: 0.2433 - lr: 3.1250e-05\n",
      "Epoch 114/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1128 - mse: 0.0938 - mae: 0.2471 - val_loss: 0.1128 - val_mse: 0.0921 - val_mae: 0.2432 - lr: 3.1250e-05\n",
      "Epoch 115/500\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1128 - mse: 0.0938 - mae: 0.2471 - val_loss: 0.1128 - val_mse: 0.0921 - val_mae: 0.2432 - lr: 3.1250e-05\n",
      "Epoch 116/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1127 - mse: 0.0938 - mae: 0.2471 - val_loss: 0.1128 - val_mse: 0.0921 - val_mae: 0.2432 - lr: 3.1250e-05\n",
      "Epoch 117/500\n",
      "45/50 [==========================>...] - ETA: 0s - loss: 0.1128 - mse: 0.0940 - mae: 0.2472\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1127 - mse: 0.0938 - mae: 0.2471 - val_loss: 0.1128 - val_mse: 0.0921 - val_mae: 0.2432 - lr: 3.1250e-05\n",
      "Epoch 118/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1127 - mse: 0.0938 - mae: 0.2471 - val_loss: 0.1128 - val_mse: 0.0921 - val_mae: 0.2432 - lr: 1.5625e-05\n",
      "Epoch 119/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1127 - mse: 0.0938 - mae: 0.2471 - val_loss: 0.1128 - val_mse: 0.0921 - val_mae: 0.2432 - lr: 1.5625e-05\n",
      "Epoch 120/500\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1127 - mse: 0.0938 - mae: 0.2471 - val_loss: 0.1128 - val_mse: 0.0921 - val_mae: 0.2432 - lr: 1.5625e-05\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [X_real_train, X_imag_train],\n",
    "    y_train,\n",
    "    epochs=500,\n",
    "    batch_size=16,\n",
    "    validation_data=([X_real_test, X_imag_test], y_test),\n",
    "    callbacks=[adjust_lr, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1128 - mse: 0.0921 - mae: 0.2433\n",
      "Test MSE: 0.0921, Test MAE: 0.2433\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_mse, test_mae = model.evaluate([X_real_test, X_imag_test], y_test)\n",
    "print(f\"Test MSE: {test_mse:.4f}, Test MAE: {test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 2s 3ms/step\n",
      "Comparison of predictions and ground truth:\n",
      "Sample 1:\n",
      "predicted as is:  [0.48633242 0.4861646  0.48073894 0.5032941  0.4964755  0.4988162\n",
      " 0.5002389  0.4910947  0.4849074  0.48880762 0.53606254 0.34193242\n",
      " 0.25931448 0.3225183  0.9208442  0.43935198]\n",
      "  Predicted:    [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 4. 2.]\n",
      "  Ground Truth: [1. 0. 1. 1. 1. 1. 3. 0. 1. 3. 1. 2. 1. 1. 4. 2.]\n",
      "----------------------------------------\n",
      "Sample 2:\n",
      "predicted as is:  [0.49705675 0.48679802 0.49531263 0.50319725 0.49753606 0.49883005\n",
      " 0.49668062 0.48524767 0.49537262 0.51703924 0.50907946 0.7093346\n",
      " 0.65734375 0.07373916 0.23822236 0.10370464]\n",
      "  Predicted:    [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 3. 3. 0. 1. 0.]\n",
      "  Ground Truth: [4. 0. 2. 0. 2. 0. 3. 2. 1. 2. 2. 2. 3. 0. 1. 0.]\n",
      "----------------------------------------\n",
      "Sample 3:\n",
      "predicted as is:  [0.49717757 0.48586762 0.49903563 0.50495344 0.49695113 0.4988446\n",
      " 0.49399972 0.50953734 0.46826348 0.5266119  0.40379876 0.739067\n",
      " 0.99444956 0.71744156 0.13254479 0.24802169]\n",
      "  Predicted:    [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 3. 4. 3. 1. 1.]\n",
      "  Ground Truth: [3. 2. 0. 0. 2. 0. 4. 4. 3. 3. 1. 3. 4. 3. 0. 1.]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = model.predict([X_real_test, X_imag_test])\n",
    "y_pred_test_rescaled = y_pred_test * (q - 1)\n",
    "y_test_rescaled = y_test * (q - 1)\n",
    "\n",
    "print(\"Comparison of predictions and ground truth:\")\n",
    "for i in range(3):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"predicted as is: \", y_pred_test[i])\n",
    "    print(f\"  Predicted:    {np.round(y_pred_test_rescaled[i])}\")\n",
    "    print(f\"  Ground Truth: {np.round(y_test_rescaled[i])}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoded Unseen Data (Real and Imaginary Parts):\n",
      "Real Part Shape: (5, 16)\n",
      "Imaginary Part Shape: (5, 16)\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "\n",
      "Comparison of Unseen Data and Predictions:\n",
      "============================================================\n",
      "Sample 1:\n",
      "  Ground Truth: [2 3 2 1 0 0 0 4 4 1 2 4 2 2 4 4]\n",
      "  Prediction  : [2 2 2 2 2 2 2 2 2 2 2 3 2 1 4 4]\n",
      "------------------------------------------------------------\n",
      "Sample 2:\n",
      "  Ground Truth: [1 3 0 4 4 2 3 4 3 0 2 1 0 3 3 0]\n",
      "  Prediction  : [2 2 2 2 2 2 2 2 2 2 2 2 1 3 2 0]\n",
      "------------------------------------------------------------\n",
      "Sample 3:\n",
      "  Ground Truth: [3 1 1 3 3 1 3 4 2 4 3 3 2 3 2 1]\n",
      "  Prediction  : [2 2 2 2 2 2 2 2 2 2 2 3 2 3 1 1]\n",
      "------------------------------------------------------------\n",
      "Sample 4:\n",
      "  Ground Truth: [2 2 4 3 1 0 2 0 3 2 0 4 0 4 1 3]\n",
      "  Prediction  : [2 2 2 2 2 2 2 2 2 2 2 3 1 3 2 3]\n",
      "------------------------------------------------------------\n",
      "Sample 5:\n",
      "  Ground Truth: [0 2 3 0 2 2 2 2 0 1 4 4 4 1 2 2]\n",
      "  Prediction  : [2 2 2 2 2 2 2 2 2 2 2 3 3 1 1 2]\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_unseen_samples = 5\n",
    "unseen_data = np.random.randint(0, q, size=(num_unseen_samples, n))\n",
    "\n",
    "padded_unseen_data = np.hstack((unseen_data, np.zeros((num_unseen_samples, n_padded - n))))\n",
    "\n",
    "unseen_data_normalized = unseen_data.astype(np.float32) / (q - 1)\n",
    "\n",
    "# print(\"Unseen Input Data (Before Encoding):\")\n",
    "# print(unseen_data)\n",
    "\n",
    "unseen_encoded = np.array([np.dot(M_tilde, x) for x in padded_unseen_data])\n",
    "unseen_encoded[np.abs(unseen_encoded) < 1e-10] = 0\n",
    "unseen_encoded = np.round(unseen_encoded, decimals=10)\n",
    "\n",
    "X_real_unseen = np.real(unseen_encoded).astype(np.float32)\n",
    "X_imag_unseen = np.imag(unseen_encoded).astype(np.float32)\n",
    "\n",
    "print(\"\\nEncoded Unseen Data (Real and Imaginary Parts):\")\n",
    "print(\"Real Part Shape:\", X_real_unseen.shape)\n",
    "print(\"Imaginary Part Shape:\", X_imag_unseen.shape)\n",
    "\n",
    "y_pred_unseen = model.predict([X_real_unseen, X_imag_unseen])\n",
    "\n",
    "y_pred_unseen_rescaled = y_pred_unseen * (q - 1)\n",
    "\n",
    "y_pred_unseen_final = np.mod(np.round(y_pred_unseen_rescaled), q)\n",
    "\n",
    "print(\"\\nComparison of Unseen Data and Predictions:\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(num_unseen_samples):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  Ground Truth: {padded_unseen_data[i].astype(int)}\")\n",
    "    print(f\"  Prediction  : {y_pred_unseen_final[i].astype(int)}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Gradient Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real_input has no trainable variables.\n",
      "imag_input has no trainable variables.\n",
      "real_layer_1 Gradient Mean: 0.6040417\n",
      "imag_layer_1 Gradient Mean: 0.30142528\n",
      "leaky_re_lu has no trainable variables.\n",
      "leaky_re_lu_3 has no trainable variables.\n",
      "real_layer_2 Gradient Mean: 0.6628104\n",
      "imag_layer_2 Gradient Mean: 0.29396912\n",
      "leaky_re_lu_1 has no trainable variables.\n",
      "leaky_re_lu_4 has no trainable variables.\n",
      "real_layer_3 Gradient Mean: 0.62275213\n",
      "imag_layer_3 Gradient Mean: 0.45728993\n",
      "leaky_re_lu_2 has no trainable variables.\n",
      "leaky_re_lu_5 has no trainable variables.\n",
      "merge_real_imag has no trainable variables.\n",
      "diagonal_scaling_layer Gradient Mean: 0.2913307\n",
      "leaky_re_lu_6 has no trainable variables.\n",
      "output_layer Gradient Mean: 0.5089798\n",
      "activation has no trainable variables.\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    if layer.trainable_variables:\n",
    "        grads = tf.reduce_mean(tf.abs(layer.trainable_variables[0]))\n",
    "        print(layer.name, \"Gradient Mean:\", grads.numpy())\n",
    "    else:\n",
    "        print(layer.name, \"has no trainable variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Gradient OK for real_layer_1/kernel_d1:0, mean: 0.0001550926681375131\n",
      "✅ Gradient OK for real_layer_1/kernel_d2:0, mean: 6.899983418406919e-05\n",
      "✅ Gradient OK for real_layer_1/bias:0, mean: 0.0004951264709234238\n",
      "✅ Gradient OK for imag_layer_1/kernel_d1:0, mean: 0.00013981651864014566\n",
      "✅ Gradient OK for imag_layer_1/kernel_d2:0, mean: 1.5891451766947284e-05\n",
      "✅ Gradient OK for imag_layer_1/bias:0, mean: 0.00033514344249852\n",
      "✅ Gradient OK for real_layer_2/kernel_d1:0, mean: 0.0009915679693222046\n",
      "✅ Gradient OK for real_layer_2/kernel_d2:0, mean: 0.0006112392875365913\n",
      "✅ Gradient OK for real_layer_2/kernel_w:0, mean: -0.0036891985218971968\n",
      "✅ Gradient OK for real_layer_2/kernel_C_1:0, mean: -0.00041032821172848344\n",
      "✅ Gradient OK for real_layer_2/kernel_C_2:0, mean: 0.00023541174596175551\n",
      "✅ Gradient OK for real_layer_2/bias:0, mean: 0.0006862310110591352\n",
      "✅ Gradient OK for imag_layer_2/kernel_d1:0, mean: -0.0012795616639778018\n",
      "✅ Gradient OK for imag_layer_2/kernel_d2:0, mean: 0.0007310691871680319\n",
      "✅ Gradient OK for imag_layer_2/kernel_w:0, mean: 5.46859810128808e-05\n",
      "✅ Gradient OK for imag_layer_2/kernel_C_1:0, mean: 9.528837108518928e-05\n",
      "✅ Gradient OK for imag_layer_2/kernel_C_2:0, mean: 0.0019464532379060984\n",
      "✅ Gradient OK for imag_layer_2/bias:0, mean: 0.0014258868759498\n",
      "✅ Gradient OK for real_layer_3/kernel_w:0, mean: 0.00031419098377227783\n",
      "✅ Gradient OK for real_layer_3/bias:0, mean: 0.0008021187386475503\n",
      "✅ Gradient OK for imag_layer_3/kernel_w:0, mean: -0.008450101129710674\n",
      "✅ Gradient OK for imag_layer_3/bias:0, mean: 0.0027424017898738384\n",
      "✅ Gradient OK for diagonal_scaling_layer/kernel_m:0, mean: 0.0019046503584831953\n",
      "✅ Gradient OK for diagonal_scaling_layer/bias:0, mean: 0.0003974689170718193\n",
      "✅ Gradient OK for output_layer/kernel_m_1:0, mean: -0.004991681315004826\n",
      "✅ Gradient OK for output_layer/bias:0, mean: -0.006328670307993889\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4 \n",
    "X_real_dummy = np.random.rand(batch_size, X_real_train.shape[1]).astype(np.float32)\n",
    "X_imag_dummy = np.random.rand(batch_size, X_imag_train.shape[1]).astype(np.float32)\n",
    "y_dummy = np.random.rand(batch_size, y_train.shape[1]).astype(np.float32)\n",
    "\n",
    "def check_gradients(model, X_real_dummy, X_imag_dummy, y_dummy):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(model.trainable_variables)\n",
    "        predictions = model([X_real_dummy, X_imag_dummy], training=True)\n",
    "        loss = tf.keras.losses.mean_squared_error(y_dummy, predictions)\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    for i, (grad, var) in enumerate(zip(grads, model.trainable_variables)):\n",
    "        if grad is None:\n",
    "            print(f\"🚨 Gradient is None for {var.name} at index {i}\")\n",
    "        else:\n",
    "            print(f\"✅ Gradient OK for {var.name}, mean: {tf.reduce_mean(grad).numpy()}\")\n",
    "\n",
    "check_gradients(model, X_real_dummy, X_imag_dummy, y_dummy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
